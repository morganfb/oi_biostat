%!TEX root=../../main.tex

\begin{doublespace}

\chapter{Probability}
\label{probability}

% all index commands commented out for now because the syntax confuses syntax highlighting in vim.  Remove comments at final compile

%  I may have neglected to move some index commands from OI to this chapter.

\index{probability|(}  


What are the chances that a woman with an abnormal mammogram has breast cancer?  What is the likelihood that an overweight male teenager with high blood pressure will develop cardiovascular disease by the age of 50?  What is the probability that two parents who are unaffected carriers of cystic fibrosis will have a child with the disease? 

These questions use the language of probability. Like all mathematical tools, probability becomes easier to understand and work with once important concepts and terminology have been formalized. This chapter introduces that formalization, using two types of examples. One set of examples uses settings that most people have seen before -- rolling dice or picking cards from a deck. The second set of examples draws from medicine, biology, and public health, reflecting the contexts and language specific to those fields. The approaches to solving these two types of problems are surprisingly similar, and in both cases, seemingly difficult problems can be solved in a series of reliable steps.

Probability also forms the foundation for data analysis and statistical inference, since nearly every conclusion to a study should be accompanied by a measure of uncertainty. For example, the publication reporting the results of the LEAP study discussed in Chapter 1 included the probability that the observed results could have simply been due to chance variation. This aspect of probability will be discussed in later chapters.

\section{Defining probability}
\label{basicsOfProbability}

\subsection{Some examples}

The rules of probability can be easily modeled by classic scenarios, such as flipping coins or rolling dice. When a coin is flipped, there are only two possible outcomes, heads or tails. With a fair coin, each outcome is equally likely; thus, the chance of flipping either heads or tails is 1/2. The following examples deal with rolling a die or multiple dice; a die is a cube with six faces numbered \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, and \resp{6}. More outcomes are possible than in flipping a coin, but the same logic still applies.

\textit{DH: Some of these dice examples can be dropped, but leaving them for now in case they are referenced later. JV: I think these three are representative. Another option is to reformat the dice examples as coin examples (e.g. two coins, chance of two heads) to more explicitly lead in to the CF example. Alternatively, the intro to this section may make more sense if there is more of a parallel established between coins and dice. My main purpose in editing is to elaborate on the single sentences "We start with some familiar examples", etc. that didn't seem to set the right tone. I also wanted to pull as much as possible into the main body of the text, instead of keeping it in "examples", because students tend to be allergic to long examples.}

\begin{example}{What is the chance of getting \resp{1} when rolling a die?}\label{probOf1}
If the die is fair, then the chance of a \resp{1} is as good as the chance of any other number. Since there are six outcomes, the chance must be 1-in-6 or, equivalently, $1/6$.
\end{example}

\begin{comment}

\begin{example}{What is the chance of getting a \resp{1} or \resp{2} in the next roll?}\label{probOf1Or2}
\resp{1} and \resp{2} constitute two of the six equally likely possible outcomes, so the chance of getting one of these two outcomes must be $2/6 = 1/3$.
\end{example}

\begin{example}{What is the chance of getting either \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, or \resp{6} on the next roll?}\label{probOf123456}
100\%. The outcome must be one of these numbers.
\end{example}

\end{comment}

\begin{example}{What is the chance of not rolling a \resp{2}?}\label{probNot2}
Not rolling a \resp{2} is the same as getting a \resp{1}, \resp{3}, \resp{4}, \resp{5}, or \resp{6}, which makes up five of the six equally likely outcomes and has probability $5/6$.
\end{example}

\begin{example} {Consider rolling two dice. If $1/6^{th}$ of the time the first die is a \resp{1} and $1/6^{th}$ of those times the second die is a \resp{1}, what is the chance of getting two \resp{1}s?}\label{probOf2Ones}
If $16.\bar{6}$\% of the time the first die is a \resp{1} and $1/6^{th}$ of \emph{those} times the second die is also a \resp{1}, then the chance that both dice are \resp{1} is $(1/6)\times (1/6)$ or $1/36$.
\end{example}

Probability also appears in more realistic contexts, such as in predicting the inheritance of genetic disease. Cystic fibrosis (CF) is a life-threatening genetic disorder caused by mutations in the \textit{CFTR} gene located on chromosome 7. Defective copies of \textit{CFTR} can result in the reduced quantity and function of the CFTR protein, which transports sodium and chloride across cell membranes, which leads to the buildup of thick mucus in the lungs and pancreas. CF is an autosomal recessive disorder--an individual only develops CF if they have inherited two affected copies of \textit{CFTR}. Individuals with one normal (wild-type) copy and one defective (mutated) copy are known as carriers; they do not develop CF, but may pass the disease-causing mutation onto their offspring.

Although it seems more complicated to calculate probabilities for disease inheritance than for the outcome of flipping a coin or rolling a die, the previously illustrated methods still apply.

\begin{example} {Suppose that both members of a couple are CF carriers. What is the probability that a child of this couple will be affected by CF? Assume that a parent has an equal chance of passing either gene copy to a child.}\label{CFInheritanceExample}

\textit{Solution 1: Enumerate all of the possible outcomes and exploit the fact that the outcomes are equally likely, as in Example~\ref{probOf1}.}  Figure~\ref{fig:cfInheritance} shows the four possible genotypes for a child of these parents. The paternal chromosome is in blue and the maternal chromosome in green, while chromosomes with the wild-type and mutated versions of \textit{CFTR} are marked with $+$ and $-$, respectively. The child is only affected if they have genotype ($-$/$-$), with two mutated copies of \textit{CFTR}. Each of the four outcomes occurs with equal likelihood, so the child will be affected with probability 1-in-4, or $1/4$.  It is important to recognize that the child being an unaffected carrier ($+$/$-$) consists of two distinct outcomes, not one. 

\textit{JV: Perhaps it is not obvious that the outcomes are equally likely if someone is new to both genetics and probability (without calculating that each is indeed 1/4)?}

\textit{Solution 2:  Calculate the proportion of outcomes that produce an affected child, as in Example~\ref{probOf2Ones}.}  During reproduction, one parent will pass along an affected copy half of the time.  When the child receives an affected gene from one parent, half of the those times, they will also receive an affected gene from the other parent.	So the proportion of times the child will be have two affected copies is $(1/2) \times (1/2) = 1/4$.
\end{example}

\begin{figure}
	\centering
	\includegraphics[width= 0.75\textwidth]{ch_probability_oi_biostat/figures/cfInheritance/cfInheritance.png}
	\caption{Pattern of CF inheritance for a child of two unaffected carriers}
	\label{fig:cfInheritance}
\end{figure}

\begin{exercise}
Suppose the father has CF and the mother is an unaffected carrier.  What is the probability that their child will be affected by the disease?

\textit{Solution:}  Since the father has CF, he must have two affected copies; he will always pass along a defective copy of the gene.  Since the mother will pass along a defective copy half of the time, the child will be affected half of the time, or with probability $1/4$.

\end{exercise}

\subsection{Probability}

% \index{random phenomena |(}

Probability is used to assign a level of uncertainty to the outcomes of phenomena that either happen randomly (e.g. rolling dice, inheriting of disease alleles), or appear random because of a lack of understanding about exactly how the phenomenon occurs (e.g. an obese teenager with high blood pressure developing cardiovascular disease later in life). Modeling these complex phenomena as random can be useful, and in either case, the interpretation of probability is the same: the chance that some event will happen in the future.

Mathematicians and philosophers have struggled for centuries (literally) to arrive at a clear statement of how probability is defined, or what it means.  In this text we use the most common definition, which also has the clearest interpretation.

\begin{termBox}{\tBoxTitle{Probability}
The \term{probability} of an outcome is the proportion of times the outcome would occur if the random phenomenon could be observed an infinite number of times.}
\end{termBox}

\index{Law of Large Numbers |(}

\begin{figure}[bt]
	\centering
	\includegraphics[width=0.85\textwidth]{ch_probability_oi_biostat/figures/dieProp/dieProp}
	\caption{The fraction of die rolls that are \resp{1} at each stage in a simulation. The proportion tends to get closer to the probability $1/6 \approx 0.167$ as the number of rolls increases.}
	\label{fig:dieProp}
\end{figure}

This definition of probability can be illustrated by rolling a die many times. Let $\hat{p}_n$ be the proportion of outcomes that are \resp{1} after the first $n$ rolls. As the number of rolls increases, $\hat{p}_n$ will converge to the probability of rolling a \resp{1}, $p = 1/6$. Figure~\ref{fig:dieProp} shows this convergence for 100,000 die rolls. The tendency of $\hat{p}_n$ to stabilize around $p$ is described by the \term{Law of Large Numbers}. The behavior shown in Figure~\ref{fig:dieProp} matches most people's intuition about probability, but proving mathematically that the behavior is always true is surprisingly difficult and beyond the level of this text.

Occasionally the proportion will veer off from the probability and appear to defy the Law of Large Numbers, as $\hat{p}_n$ does many times in Figure~\ref{fig:dieProp}. However, these deviations become smaller as the number of rolls increases.

\begin{termBox}{\tBoxTitle{Law of Large Numbers}
As more observations are collected, the proportion $\hat{p}_n$ of occurrences with a particular outcome converges to the probability $p$ of that outcome.}
\end{termBox}

\index{Law of Large Numbers |)}

While it is easy to imagine repeatedly rolling a die to observe the law of large numbers, the interpretation of probability is more hypothetical for examples like disease inheritance, since family sizes are typically small -- if two unaffected carriers were to have many children, then approximately 25\% of their offspring would suffer from CF.

Probability is defined as a proportion, and it always takes values between 0~and~1 (inclusively). It may also be displayed as a percentage between 0\% and 100\%. The notation $p$ is the probability of rolling a \resp{1}. We can also write this probability as
\begin{eqnarray*}
P(\text{rolling a \resp{1}})
\end{eqnarray*}
\marginpar[\raggedright\vspace{-13mm}

$P(A)$\vspace{1mm}\\\footnotesize Probability of\\outcome $A$]{\raggedright\vspace{-13mm}

$P(A)$\vspace{1mm}\\\footnotesize Probability of\\outcome $A$}As we become more comfortable with this notation, we will abbreviate it further. For instance, if it is clear that the process is ``rolling a die'', we could abbreviate $P($rolling a \resp{1}$)$ as~$P($\resp{1}$)$.  We also have a notation for an event itself, so the event $A$ of rolling a 1 will be written as $A = \{\text{rolling a \resp{1}}\}$, with associated probability $P(A)$. 


\index{random phenomena|)}
\newpage

\subsection{Disjoint or mutually exclusive outcomes}

\index{disjoint|(}
\index{mutually exclusive|(}

Two outcomes are called \term{disjoint} or \term{mutually exclusive} if they cannot both happen at the same time. When rolling a die, the outcomes \resp{1} and \resp{2} are disjoint since they cannot both occur.  However, the outcomes \resp{1} and ``rolling an odd number'' are not disjoint since both occur if the outcome of the roll is a \resp{1}. Similarly, for the CF example, the outcomes of a child being affected and having one mutated copy of \textit{CFTR} are not disjoint. The terms \emph{disjoint} and \emph{mutually exclusive} are equivalent and interchangeable. 

When rolling a die, the outcomes \resp{1} and \resp{2} are disjoint. We compute the probability that one of these outcomes will occur by adding their separate probabilities:
\begin{eqnarray*}
P(\text{\resp{1} or \resp{2}}) = P(\text{\resp{1}})+P(\text{\resp{2}}) = 1/6 + 1/6 = 1/3
\end{eqnarray*}
What about  the probability of rolling a \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, or \resp{6}? Here again, all of the outcomes are disjoint so we add the probabilities:
\begin{eqnarray*}
&&P(\text{\resp{1} or \resp{2} or \resp{3} or \resp{4} or \resp{5} or \resp{6}}) \\
	&&\quad= P(\text{\resp{1}})+P(\text{\resp{2}})+P(\text{\resp{3}})+P(\text{\resp{4}})+P(\text{\resp{5}})+P(\text{\resp{6}}) \\
	&&\quad= 1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 1.
\end{eqnarray*}

For the CF example, there are two mutually exclusive outcomes for which a child of two carriers can also be a carrier -- a child can either receive an affected copy of \textit{CFTR} from the mother and a normal copy from the father, or vice versa. Thus, the probability that a child will be an unaffected carrier is $(1/2) + (1/2) = 1/4$.

The \term{Addition Rule} guarantees the accuracy of this approach when the outcomes are disjoint. 

\begin{termBox}{\tBoxTitle{Addition Rule of disjoint outcomes} If $A_1$ and $A_2$ represent two disjoint outcomes, then the probability that either one of them occurs is given by
\begin{eqnarray*}
P(A_1\text{ or } A_2) = P(A_1) + P(A_2)
\end{eqnarray*}
If there are many disjoint outcomes $A_1$, ..., $A_k$, then the probability that one of these outcomes will occur is
\begin{eqnarray}
P(A_1) + P(A_2) + \cdots + P(A_k)
\end{eqnarray}
}
\end{termBox}

\index{event|(}


Probability problems rarely consider individual outcomes and instead often deal with \indexthis{\emph{sets}}{sets} or \indexthis{\emph{collections}}{collections} of outcomes. Let $A$ represent the event in which a die roll results in \resp{1} or \resp{2} and $B$~represent the event that the die roll is a \resp{4} or a \resp{6}. We write $A$ as the set of outcomes $\{$\resp{1},~\resp{2}$\}$ and $B=\{$\resp{4}, \resp{6}$\}$. These sets are commonly called \termsub{events}{event}. Because $A$ and $B$ have no elements in common, they are disjoint events. $A$ and $B$ are represented in Figure~\ref{fig:disjointEvents}.

\begin{figure}[hhh]
\centering
\includegraphics[width=0.55\textwidth]{ch_probability_oi_biostat/figures/disjointEvents/disjointEvents.png}
\caption{Three events, $A$, $B$, and $D$, consist of outcomes from rolling a die. $A$ and $B$ are disjoint since they do not have any outcomes in common.}
\label{fig:disjointEvents}
\end{figure}

The Addition Rule applies to both disjoint outcomes and disjoint events. The probability that one of the disjoint events $A$ or $B$ occurs is the sum of the separate probabilities:
\begin{align*}
P(A\text{ or }B) = P(A) + P(B) = 1/3 + 1/3 = 2/3
\end{align*}

\begin{exercise}
(a) Verify the probability of event $A$, $P(A)$, is $1/3$ using the Addition Rule. (b) Do the same for event $B$.\footnote{(a) $P(A) = P($\resp{1} or \resp{2}$) = P($\resp{1}$) + P($\resp{2}$) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6} = \frac{1}{3}$. (b) Similarly, $P(B) = 1/3$.}
\end{exercise}

\begin{exercise} \label{exerExaminingDisjointSetsABD}
(a) Using Figure~\ref{fig:disjointEvents} as a reference, which outcomes are represented by event $D$? (b) Are events $B$ and $D$ disjoint? (c) Are events $A$ and $D$ disjoint?\footnote{(a)~Outcomes \resp{2} and \resp{3}. (b)~Yes, events $B$ and $D$ are disjoint because they share no outcomes. (c)~The events $A$ and $D$ share an outcome in common, \resp{2}, and so are not disjoint.}
\end{exercise}

\begin{exercise}
In Guided Practice~\ref{exerExaminingDisjointSetsABD}, you confirmed $B$ and $D$ from Figure~\ref{fig:disjointEvents} are disjoint. Compute the probability that event $B$ or event $D$~occurs.\footnote{Since $B$ and $D$ are disjoint events, use the Addition Rule: $P(B$ or $D) = P(B) + P(D) = \frac{1}{3} + \frac{1}{3} = \frac{2}{3}$.}
\end{exercise}

\textit{DH: Should we add more genetics problems here?  I have removed the email example because of the possible confusion between events involving sampling from a population vs a study sample.  If we think we can make that clear, we can use examples from famuss, perhaps by posing a problem of sampling members from the study participants.  Note also that this is moving more slowly than the Stat 102 notes, but we did show some of this material on the blackboard. If we use this chapter in 102, perhaps we can move quickly to more complicated examples.}

\textit{JV: Perhaps famuss examples could be added later in a more complicated context, to keep the chapter from moving too slowly at this point.}

\index{event|)}
\index{disjoint|)}
\index{mutually exclusive|)}

\subsection{Probabilities when events are not disjoint}

\term{Venn diagrams} are useful when outcomes can be categorized as ``in'' or ``out'' for two or three variables, attributes, or random processes. The Venn diagram in Figure~\ref{fig:cardsDiamondFaceVenn} uses one oval to represent diamonds and another to represent face cards (the cards labeled jacks, queens, and kings); if a card is both a diamond and a face card, it falls into the intersection of the ovals.

\begin{table}[h]
\centering
\begin{tabular}{lll lll lll lll l}
\resp{2$\clubsuit$} & \resp{3$\clubsuit$} & \resp{4$\clubsuit$} & \resp{5$\clubsuit$} & \resp{6$\clubsuit$} & \resp{7$\clubsuit$} & \resp{8$\clubsuit$} & \resp{9$\clubsuit$} & \resp{10$\clubsuit$} & \resp{J$\clubsuit$} & \resp{Q$\clubsuit$} & \resp{K$\clubsuit$} & \resp{A$\clubsuit$}  \\
\color{redcards} \resp{2$\diamondsuit$} & \color{redcards}\resp{3$\diamondsuit$} & \color{redcards}\resp{4$\diamondsuit$} & \color{redcards}\resp{5$\diamondsuit$} & \color{redcards}\resp{6$\diamondsuit$} & \color{redcards}\resp{7$\diamondsuit$} & \color{redcards}\resp{8$\diamondsuit$} & \color{redcards}\resp{9$\diamondsuit$} & \color{redcards}\resp{10$\diamondsuit$} & \color{redcards}\resp{J$\diamondsuit$} & \color{redcards}\resp{Q$\diamondsuit$} & \color{redcards}\resp{K$\diamondsuit$} & \color{redcards}\resp{A$\diamondsuit$} \\
\color{redcards}\resp{2$\heartsuit$} & \color{redcards}\resp{3$\heartsuit$} & \color{redcards}\resp{4$\heartsuit$} & \color{redcards}\resp{5$\heartsuit$} & \color{redcards}\resp{6$\heartsuit$} & \color{redcards}\resp{7$\heartsuit$} & \color{redcards}\resp{8$\heartsuit$} & \color{redcards}\resp{9$\heartsuit$} & \color{redcards}\resp{10$\heartsuit$} & \color{redcards}\resp{J$\heartsuit$} & \color{redcards}\resp{Q$\heartsuit$} & \color{redcards}\resp{K$\heartsuit$} & \color{redcards}\resp{A$\heartsuit$} \\
\resp{2$\spadesuit$} & \resp{3$\spadesuit$} & \resp{4$\spadesuit$} & \resp{5$\spadesuit$} & \resp{6$\spadesuit$} & \resp{7$\spadesuit$} & \resp{8$\spadesuit$} & \resp{9$\spadesuit$} & \resp{10$\spadesuit$} & \resp{J$\spadesuit$} & \resp{Q$\spadesuit$} & \resp{K$\spadesuit$} & \resp{A$\spadesuit$}
\end{tabular}
\caption{A \indexthis{regular deck of 52 cards}{deck of cards} is split into four \term{suits}: $\clubsuit$ (club), {\color{redcards}$\diamondsuit$} (diamond), {\color{redcards}$\heartsuit$} (heart), $\spadesuit$ (spade). Each suit has its 13 cards labeled: \resp{2}, \resp{3}, ..., \resp{10}, \resp{J} (jack), \resp{Q} (queen), \resp{K} (king), and \resp{A} (ace). Thus, each card is a unique combination of a suit and a label, e.g. {\color{redcards}\resp{4$\heartsuit$}} and \resp{J$\clubsuit$}. %The cards that are {\color{redcards}$\diamondsuit$} or {\color{redcards}$\heartsuit$} are typically colored {\color{redcards}red} while the other two suits are typically colored black.
	}
\label{deckOfCards}
\end{table}

\begin{exercise}
(a) What is the probability that a randomly selected card is a diamond? (b)~What is the probability that a randomly selected card is a face card?\footnote{(a) There are 52 cards and 13 diamonds. If the cards are thoroughly shuffled, each card has an equal chance of being drawn, so the probability that a randomly selected card is a diamond is $P({\color{redcards}\diamondsuit}) = \frac{13}{52} = 0.250$. (b)~Likewise, there are 12 face cards, so $P($face card$) = \frac{12}{52} = \frac{3}{13} = 0.231$.}
\end{exercise}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{ch_probability_oi_biostat/figures/cardsDiamondFaceVenn/cardsDiamondFaceVenn.png}
	\caption{A Venn diagram for diamonds and face cards.}
	\label{fig:cardsDiamondFaceVenn}
\end{figure}

%\begin{exercise}
%Using Figure~\ref{cardsDiamondFaceVenn}, verify $P($face card$) = 12/52=3/13$.\footnote{The Venn diagram shows face cards split up into ``face card but not {\color{redcards}$\diamondsuit$}'' and ``face card and {\color{redcards}$\diamondsuit$}''. Since these correspond to disjoint events, $P($face card$)$ is found by adding the two corresponding probabilities: $\frac{3}{52} + \frac{9}{52} = \frac{12}{52} = \frac{3}{13}$.}
%\end{exercise}

Let $A$ represent the event that a randomly selected card is a diamond and $B$ represent the event that it is a face card. Events $A$ and $B$ are not disjoint -- the cards {\color{redcards}$J\diamondsuit$}, {\color{redcards}$Q\diamondsuit$}, and {\color{redcards}$K\diamondsuit$} fall into both categories. 

As a result, adding the probabilities of the two events together is not sufficient to calculate $P(A or B)$:

\begin{eqnarray*}
	P(A) + P(B) = P({\color{redcards}\diamondsuit}) + P(\text{face card}) = 12/52 + 13/52
	\label{overCountFaceDiamond}
\end{eqnarray*}

Instead, a small modification is necessary. The three cards that are in both events were counted twice, once in each probability. To correct the double counting, subtract the probability that both events occur:

\begin{eqnarray}
P(A\text{ or } B) &=&P(\text{face card or }{\color{redcards}\diamondsuit})  \notag \\
 &=& P(\text{face card}) + P({\color{redcards}\diamondsuit}) - P(\text{face card and }{\color{redcards}\diamondsuit}) \label{diamondFace} \\
 &=& 13/52 + 12/52 - 3/52 \notag \\
 &=& 22/52 = 11/26 \notag
\end{eqnarray}
Equation~(\ref{diamondFace}) is an example of the \term{General Addition Rule}. 

\begin{termBox}{\tBoxTitle{General Addition Rule} If $A$ and $B$ are any two events, disjoint or not, then the probability that at least one of them will occur is
\begin{eqnarray}
P(A\text{ or }B) = P(A) + P(B) - P(A\text{ and }B)
\label{generalAdditionRule}
\end{eqnarray}
where $P(A$ and $B)$ is the probability that both events occur.}
\end{termBox}


\begin{tipBox}{\tipBoxTitle{``or'' is inclusive}
Note that in the language of statistics, "or" is in inclusive such that $A$ or $B$ occurs means $A$, $B$, or both $A$ and $B$ occur.}
\end{tipBox}

\begin{exercise}
(a) If $A$ and $B$ are disjoint, describe why this implies $P(A$ and $B) = 0$. (b) Using part (a), verify that the General Addition Rule simplifies to the simpler Addition Rule for disjoint events if $A$ and $B$ are disjoint.\footnote{(a) If $A$ and $B$ are disjoint, $A$ and $B$ can never occur simultaneously. (b) If $A$ and $B$ are disjoint, then the last term of Equation~(\ref{generalAdditionRule}) is 0 (see part (a)) and we are left with the Addition Rule for disjoint events.}
\end{exercise}

\begin{exercise}
Human immunodeficiency virus (HIV) and tuberculosis (TB) affect substantial proportions of the population in certain areas of the developing world. Individuals sometimes have both diseases (are co-infected); children of HIV-infected mothers may have HIV (be HIV$^+$) and TB can spread from one family member to another.  In a mother child pair, let $A = \{\text{ the mother has HIV } \}$,  $B = \{\textrm{ the mother has TB } \}$, $C = \{\text{ the child has HIV } \}$,  $D = \{\text{ the child has TB } \}$.  Write out the definitions of the events $A \text{ or } B$, $A \text{ and } B$, $A \text{ and } C$, $A \text{ or } D$. \textit{Needs solution in footnote.}

\end{exercise}

\subsection{Probability distributions}

A \term{probability distribution} is a table of all disjoint outcomes and their associated probabilities. Table~\ref{diceProb} shows the probability distribution for the sum of two dice. 

\begin{table}[h] \small
\centering
\begin{tabular}{l ccc ccc ccc cc}
  \hline
  \ \vspace{-3mm} \\
Dice sum\vspace{0.3mm} & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12  \\
Probability & $\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ & $\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ & $\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$\vspace{1mm} \\
   \hline
\end{tabular}
\caption{Probability distribution for the sum of two dice.}
\label{diceProb}
\end{table}

\begin{termBox}{\tBoxTitle{Rules for probability distributions}
A probability distribution is a list of the possible outcomes with corresponding probabilities that satisfies three rules: \vspace{-2mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item The outcomes listed must be disjoint.
\item Each probability must be between 0 and 1.
\item The probabilities must total to 1. \vspace{1mm}
\end{enumerate}}
\end{termBox}

Chapter~\ref{introductionToData} emphasized the importance of plotting data to provide quick summaries. Probability distributions can also be summarized in a bar plot. The probability distribution for the sum of two dice is shown in Table~\ref{diceProb} and plotted in Figure~\ref{diceSumDist}.

\begin{figure}
\centering
\includegraphics[width=0.73\textwidth]{ch_probability_oi_biostat/figures/diceSumDist/diceSumDist}
\caption{The probability distribution of the sum of two dice.}
\label{diceSumDist}
\end{figure}

In this bar plot, the bar heights represent the probabilities of outcomes. If the outcomes are numerical and discrete, it can be visually convenient to make a bar plot that resembles a histogram, such as in the case of the sum of two dice.  

\begin{figure}[h]
	\includegraphics[width=\textwidth]{ch_probability_oi_biostat/figures/birthwtMarginalDist/birthwtMarginalDist.pdf}
	\caption{Distribution of birth weights (in grams) of babies born in the US in 2010}
	\label{fig:birthwtMarginalDist}
\end{figure}

\textit{JV: Text in the birthweight plot is very small, in order to accommodate the number of bars... If you agree that bars can be reduced, adjust the marginal dist table and I can re-size the plot.}

A graph of probability distribution can quickly convey important information about a distribution quickly. For example, the distribution of birth weights for 3,999,386 live births in the United States in 2010 is shown in figure \ref{fig:birthwtMarginalDist}.  The data are available as part of the US CDC National Vital Statistics System.\footnote{\url{http://205.207.175.93/vitalstats/ReportFolders/reportFolders.aspx}} The graph of the distribution shows that while most babies born weighed between 2000 and 5000 grams (2 to 5 kg), there were both small (less than 1000 grams) and large (greater than 5000 grams) babies. Pediatricians consider birth weights between 2.5 and 5 kg as normal. \textit{JV: Citation?}

\subsection{Complement of an event}

Rolling a die produces a value in the set $\{$\resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, \resp{6}$\}$. This set of all possible outcomes is called the \term{sample space} ($S$)\marginpar[\raggedright\vspace{-5mm}

$S$\\\footnotesize Sample space]{\raggedright\vspace{-5mm}

$S$\\\footnotesize Sample space}\index{S@$S$} for rolling a die. 

Let $D=\{$\resp{2}, \resp{3}$\}$ represent the event that the outcome of a die roll is \resp{2} or \resp{3}. The \term{complement}\marginpar[\raggedright\vspace{0.2mm}

$A^c$\\\footnotesize Complement\\of outcome $A$]{\raggedright\vspace{0.2mm}

$A^c$\\\footnotesize Complement\\of outcome $A$}\index{Ac@$A^c$} of $D$ represents all outcomes in our sample space that are not in $D$, which is denoted by $D^c = \{$\resp{1}, \resp{4}, \resp{5}, \resp{6}$\}$. That is, $D^c$ is the set of all possible outcomes not already included in $D$. Figure~\ref{fig:complementOfD} shows the relationship between $D$, $D^c$, and the sample space $S$. 

\begin{figure}[hht]
\centering
\includegraphics[width=0.55\textwidth]{ch_probability_oi_biostat/figures/complementOfD/complementOfD}
\caption{Event $D=\{$\resp{2}, \resp{3}$\}$ and its complement, $D^c = \{$\resp{1}, \resp{4}, \resp{5}, \resp{6}$\}$. $S$~represents the sample space, which is the set of all possible events.}
\label{fig:complementOfD}
\end{figure}

\begin{exercise}
(a) Compute $P(D^c) = P($rolling a \resp{1}, \resp{4}, \resp{5}, or \resp{6}$)$. (b) What is $P(D) + P(D^c)$?\footnote{(a)~The outcomes are disjoint and each has probability $1/6$, so the total probability is $4/6=2/3$. (b)~We can also see that $P(D)=\frac{1}{6} + \frac{1}{6} = 1/3$. Since $D$ and $D^c$ are disjoint, $P(D) + P(D^c) = 1$.}
\end{exercise}

\begin{exercise}
Events $A=\{$\resp{1}, \resp{2}$\}$ and $B=\{$\resp{4}, \resp{6}$\}$ are shown in Figure~\ref{fig:disjointEvents} on page~\pageref{fig:disjointEvents}. (a) Write out what $A^c$ and $B^c$ represent. (b)~Compute $P(A^c)$ and $P(B^c)$. (c)~Compute $P(A)+P(A^c)$ and $P(B)+P(B^c)$.\footnote{Brief solutions: (a)~$A^c=\{$\resp{3}, \resp{4}, \resp{5}, \resp{6}$\}$ and $B^c=\{$\resp{1}, \resp{2}, \resp{3}, \resp{5}$\}$. (b)~Noting that each outcome is disjoint, add the individual outcome probabilities to get $P(A^c)=2/3$ and $P(B^c)=2/3$. (c)~$A$~and~$A^c$ are disjoint, and the same is true of $B$~and~$B^c$. Therefore, $P(A) + P(A^c) = 1$ and $P(B) + P(B^c) = 1$.}
\end{exercise}

A complement of an event $A$ is constructed to have two very important properties: (i) every possible outcome not in $A$ is in $A^c$, and (ii) $A$ and $A^c$ are disjoint. Property (i) implies
\begin{eqnarray}
P(A\text{ or }A^c) = 1
\label{complementSumTo1}
\end{eqnarray}
That is, if the outcome is not in $A$, it must be represented in $A^c$. We use the Addition Rule for disjoint events to apply Property (ii):
\begin{eqnarray}
P(A\text{ or }A^c) = P(A) + P(A^c)
\label{complementDisjointEquation}
\end{eqnarray}
Combining Equations~(\ref{complementSumTo1}) and~(\ref{complementDisjointEquation}) yields a useful relationship between the probability of an event and its complement.

\begin{termBox}{\tBoxTitle{Complement}
The complement of event $A$ is denoted $A^c$, and $A^c$ represents all outcomes not in~$A$. $A$ and $A^c$ are mathematically related: \vspace{-2mm}
\begin{eqnarray}\label{complement}
P(A) + P(A^c) = 1, \quad\text{i.e.}\quad P(A) = 1-P(A^c)
\end{eqnarray}\vspace{-6.5mm}}
\end{termBox}

In simple examples, computing either $A$ or $A^c$ is feasible in a few steps. However, as problems grow in complexity, using the relationship between an event and its complement can be a useful strategy.

\begin{exercise}
Let $A$ represent the event in which two dice are rolled and their total is less than \resp{12}. (a) What does the event $A^c$ represent? (b) Determine $P(A^c)$ from Table~\ref{diceProb} on page~\pageref{diceProb}. (c) Determine $P(A)$.\footnote{(a)~The complement of $A$: when the total is equal to \resp{12}. (b)~$P(A^c) = 1/36$. (c)~Use the probability of the complement from part (b), $P(A^c) = 1/36$, and Equation~(\ref{complement}): $P($less than \resp{12}$) = 1 - P($\resp{12}$) = 1 - 1/36 = 35/36$.}
\end{exercise}

\begin{exercise} Consider again the probabilities from Table~\ref{diceProb} and rolling two dice. Find the following probabilities: (a)~The sum of the dice is \emph{not} \resp{6}. (b)~The sum is at least \resp{4}. That is, determine the probability of the event $B=\{$\resp{4}, \resp{5}, ..., \resp{12}$\}$. (c) The sum is no more than \resp{10}. That is, determine the probability of the event $D=\{$\resp{2}, \resp{3}, ..., \resp{10}$\}$.\footnote{(a)~First find $P($\resp{6}$)=5/36$, then use the complement: $P($not \resp{6}$) = 1 - P($\resp{6}$) = 31/36$.

(b)~First find  the complement, which requires much less effort: $P($\resp{2} or \resp{3}$)=1/36+2/36=1/12$. Then calculate $P(B) = 1-P(B^c) = 1-1/12 = 11/12$.

(c)~As before, finding the complement is the more direct way to determine $P(D)$. First find $P(D^c) = P($\resp{11} or \resp{12}$)=2/36 + 1/36=1/12$. Then calculate $P(D) = 1 - P(D^c) = 11/12$.}
\end{exercise}

\textit{JV: I agree that this example requires too much explanation for what should be a relatively short point... Will leave as-is for now.}

Sometimes, information from a graph can be combined with using the complement of an event to calculate approximate probabilities. The gestational age of a newborn is the time between conception and birth. Because of the obvious difficulty of determining the exact date of conception, gestational ages are typically recorded in weeks. Figure \ref{fig:gestageMarginalDist} is the graphical representation of the distribution of gestational ages for the  3,999,386 babies born in 2010.  Babies born between 38 and 42 weeks of gestational age are considered normal, but the term `full term' is used for births between 39 and 40 weeks gestational age.  The graph shows that approximately 30\% of births occur at 39 weeks, and slightly less than 30\% occur at 40 weeks, so approximately 50\% of babies are considered full term.  Instead of adding up the heights of the bars for gestational ages outside the full term range, using the complement of the event of a full term birth, it is clear that approximately 50\% of births not considered full term.

\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{ch_probability_oi_biostat/figures/gestageMarginalDist/gestageMarginalDist.pdf}
  \caption{Distribution of gestational age for live births in the US in 2010, measured in weeks}
  \label{fig:gestageMarginalDist}
\end{figure}

% R script for creating figure stored with the figure; needs to be improved

The distribution of gestational age is shown in tabular form in Table \ref{gestageMarginalDistTable}.  The table shows the exact value of the proportion of babies born at 39 or 40 weeks (0.47), but when examining the important features of a distribution, approximate values are often sufficient.  In some instances, the graph is all that will be available.  Since small probabilities are difficult to read accurately from the graph of a distribution, they are best read from the table.  Pre-term babies are those born at less than 37 weeks gestational age.  Table \ref{gestageMarginalDistTable} shows that the probability of this event is $0.01 + 0.01 + 0.02 + 0.08 = 0.12$.  Of course, even the table shows approximate values, since the small proportion of very premature babies born at less than 20 weeks is rounded to zero.

\textit{DH: two problems with the example: it is too clumsy for what it accomplishes, and I have  included the not stated category in the calculations of the proportions.  This is negligible, but wrong.}

\begin{table}[ht]
\label{gestageMarginalDistTable}
\centering
\begin{tabular}{rr}
  \hline
 & x \\ 
  \hline
Under.wk.20 & 0.00 \\ 
  wk.20.27 & 0.01 \\ 
  wk.28.31 & 0.01 \\ 
  wk.32.33 & 0.02 \\ 
  wk.34.36 & 0.08 \\ 
  wk.37.38 & 0.27 \\ 
  wk39 & 0.28 \\ 
  wk.40 & 0.19 \\ 
  wk.41 & 0.08 \\ 
  wk.42.and.over & 0.05 \\ 
  Not.stated & 0.00 \\ 
   \hline
\end{tabular}
\end{table}

\textit{DH: value labels should be modified}

\textit{DH: not satisfied with the way this example worked out; it should be improved or changed}

\subsection{Independence}
\label{probabilityIndependence}

Just as variables and observations can be independent, random phenomena can also be independent. Two processes are \term{independent} if knowing the outcome of one provides no information about the outcome of the other. For instance, flipping a coin and rolling a die are two independent processes -- knowing that the coin lands heads up does not help determine the outcome of a die roll. On the other hand, stock prices usually move up or down together, so they are not independent. \textit{JV: Perhaps these examples could be replaced... Ex. dependence of height and weight? independence of other two traits, maybe height and eye color?}

Independence was used implicitly on page~\pageref{CFInheritanceExample} in the second solution to the probability that two carriers will have an affected child with cystic fibrosis. Genes are typically passed along from the mother and father independently, allowing for the assumption that half of the offspring who have received a mutated CF gene from the mother will receive a mutated gene from the father.

Example~\ref{probOf2Ones} provides a basic example of two independent processes: rolling two dice. What is the probability that both will be \resp{1}? Suppose one of the dice is blue and the other green. If the outcome of the blue die is a \resp{1}, it provides no information about the outcome of the green die. This question was first encountered in Example~\ref{probOf2Ones} (page~\pageref{probOf2Ones}), with the solution using the following logic: $1/6^{th}$ of the time the blue die is a \resp{1}, and $1/6^{th}$ of \emph{those} times the green die will also be \resp{1}. This is illustrated in Figure~\ref{fig:indepForRollingTwo1s}. Because the rolls are independent, the probabilities of the corresponding outcomes can be multiplied to obtain the final answer: $(1/6)\times(1/6)=1/36$. This can be generalized to many independent processes. 

\begin{figure}[hht]
\centering
\includegraphics[width=0.5\textwidth]{ch_probability_oi_biostat/figures/indepForRollingTwo1s/indepForRollingTwo1s.png}
\caption{$1/6^{th}$ of the time, the first roll is a \resp{1}. Then $1/6^{th}$ of \emph{those} times, the second roll will also be a \resp{1}.}
\label{fig:indepForRollingTwo1s}
\end{figure}

\begin{exercise}{What if there was also a red die independent of the other two? What is the probability of rolling the three dice and getting all \resp{1}s?}\label{threeDice}
\footnote{The same logic applies from Example~\ref{probOf2Ones}. If $1/36^{th}$ of the time the blue and green dice are both \resp{1}, then $1/6^{th}$ of \emph{those} times the red die will also be \resp{1}, so multiply:
{\begin{align*}
P(blue=\text{\small\resp{1} and } green=\text{\small\resp{1} and } red=\text{\small\resp{1}})
	&= P(blue=\text{\small\resp{1}})\times P(green=\text{\small\resp{1}})\times P(red=\text{\small\resp{1}}) \\
	&= (1/6)\times (1/6)\times (1/6)
	= 1/216
\end{align*}} \vspace{-7mm}
}
\end{exercise}

Guided Practice~\ref{threeDice} illustrates the Multiplication Rule for independent processes. 

\begin{termBox}{\tBoxTitle{\term{Multiplication Rule} for independent processes}
If $A$ and $B$ represent events from two different and independent processes, then the probability that both $A$ and $B$ occur can be calculated as the product of their separate probabilities: \vspace{-1.5mm}
\begin{eqnarray}\label{eqForIndependentEvents}
P(A \text{ and }B) = P(A) \times  P(B)
\end{eqnarray}
Similarly, if there are $k$ events $A_1$, ..., $A_k$ from $k$ independent processes, then the probability they all occur is\vspace{-1.5mm}
\begin{eqnarray*}
P(A_1) \times  P(A_2)\times  \cdots \times  P(A_k)
\end{eqnarray*}\vspace{-6mm}}
\end{termBox}

In applications to biology or medicine, complicated probability problems are often solved with the simple ideas used in the dice examples. \textit{JV: I think the handedness problems can be put in end-of-chapter exercises. Given this kind of lead-in, we should focus on demonstrating some realistic problems.}

\begin{comment}
\begin{exercise} \label{ex2Handedness}
About 9\% of people are left-handed. Suppose 2 people are selected at random from the U.S. population. Because the sample size of 2 is very small relative to the population, it is reasonable to assume these two people are independent. (a)~What is the probability that both are left-handed? (b)~What is the probability that both are right-handed?\footnote{(a) The probability the first person is left-handed is $0.09$, which is the same for the second person. We apply the Multiplication Rule for independent processes to determine the probability that both will be left-handed: $0.09\times 0.09 = 0.0081$.

(b) It is reasonable to assume the proportion of people who are ambidextrous (both right and left handed) is nearly 0, which results in $P($right-handed$)=1-0.09=0.91$. Using the same reasoning as in part~(a), the probability that both will be right-handed is $0.91\times 0.91 = 0.8281$.}
\end{exercise}

\begin{exercise} \label{ex5Handedness}
Suppose 5 people are selected at random.\footnote{(a)~The abbreviations \resp{RH} and \resp{LH} are used for right-handed and left-handed, respectively. Since each are independent, we apply the Multiplication Rule for independent processes:
\begin{align*}
P(\text{all five are \resp{RH}})
&= P(\text{first = \resp{RH}, second = \resp{RH}, ..., fifth = \resp{RH}}) \\
&= P(\text{first = \resp{RH}})\times P(\text{second = \resp{RH}})\times  \dots \times P(\text{fifth = \resp{RH}}) \\
&= 0.91\times 0.91\times 0.91\times 0.91\times 0.91 = 0.624
\end{align*}

(b)~Using the same reasoning as in~(a), $0.09\times 0.09\times 0.09\times 0.09\times 0.09 = 0.0000059$

(c)~Use the complement, $P($all five are \resp{RH}$)$, to answer this question:
\begin{align*}
P(\text{not all \resp{RH}})
	= 1 - P(\text{all \resp{RH}})
	= 1 - 0.624 = 0.376
\end{align*}} \vspace{-1.5mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item[(a)] What is the probability that all are right-handed?
\item[(b)] What is the probability that all are left-handed?
\item[(c)] What is the probability that not all of the people are right-handed?
\end{enumerate}
\end{exercise}

Suppose the variables \var{handedness} and \var{gender} are independent, i.e. knowing someone's \var{gender} provides no useful information about their \var{handedness} and vice-versa. Then we can compute whether a randomly selected person is right-handed and female\footnote{The actual proportion of the U.S. population that is \resp{female} is about 50\%, and so we use 0.5 for the probability of sampling a woman. However, this probability does differ in other countries.} using the Multiplication Rule:
\begin{eqnarray*}
P(\text{right-handed and female}) &=& P(\text{right-handed}) \times  P(\text{female}) \\
&=& 0.91 \times  0.50 = 0.455
\end{eqnarray*}


\begin{exercise}
Three people are selected at random.\footnote{Brief answers are provided. (a)~This can be written in probability notation as $P($a randomly selected person is male and right-handed$)=0.455$. (b) 0.207. (c) 0.045. (d) 0.0093.} \vspace{-1.5mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item[(a)] What is the probability that the first person is male and right-handed?
\item[(b)] What is the probability that the first two people are male and right-handed?.
\item[(c)] What is the probability that the third person is female and left-handed?
\item[(d)] What is the probability that the first two people are male and right-handed and the third person is female and left-handed?
\end{enumerate}
\end{exercise}

\end{comment}

\begin{example}{\textbf{Mandatory drug testing.} Mandatory drug testing in the workplace is common practice for certain professions, such as air traffic controllers and transportation workers.  A false positive in a drug screening test occurs when the test incorrectly indicates that a screened person is an illegal drug user. Suppose a mandatory drug test has a false-positive rate of 1.2\% (i.e., has probability  0.012 of indicating that an employee is using illegal drugs when that is not the case).  Given 150 employees who are in reality drug free, what is the probability that at least one will (falsely) test positive? Assume that the outcome of one drug test has no affect on the others.}

The solution first defines the event $P(\text{At least 1 "+"})$ in terms of its complement, then uses the multiplication rule to calculate the probability of the complement $P(\text{150 "-"})$.

   \begin{align*} 
   P(\text{At least 1 "+"}) &= P(\text{1 or 2 or 3 \ldots or 150 are "+"}) \\
           &= 1 - P(\text{None are "+"}) \\
           &= 1 - P(\text{150 "-"}) \\
 P(\text{150 are "-"}) &= P(\text{1 is "-"})^{150} \\
           &= (0.988)^{150} = 0.16.
    \end{align*}
   So $P(\text{At least 1 is "+"})  = 1 - P(\text{150 are "-"}) = 0.84.$
 
   \textit{DH: should we be more formal here in defining events? Also, this is the example that we also solved in R, two different ways.  Those solutions are candidates for the R supplement JV: No, I think it may actually be more intuitive to see events written this way.}

Even when using a test with a small probability of a false positive, the company is more than 80\% likely to incorrectly claim at least one employee is an illegal drug user!

\end{example}

\begin{exercise}
Because of the high likelihood of at least one false positive in company wide drug screening programs, an individual with a positive test is almost always re-tested with a different screening test, one that is more expensive than the first but with a lower false positive probability.  Suppose the second test has a false positive rate of 0.8\%.  What is the probability that an employee who is not using illegal drugs will test positive on both tests?

\textit{DH: solution to be added if we keep the problem JV: I like this one.}

\end{exercise}

\begin{figure}[h]
	\centering
	\includegraphics[height= 0.65\textwidth]{ch_probability_oi_biostat/figures/aboInheritance/aboInheritance.png}
	\caption{Inheritance of ABO blood groups.}
	\label{fig:aboInheritance}
\end{figure}

\begin{example}{\textbf{ABO blood groups.} There are four different common blood types (A, B, AB, and O), which are determined by the presence of certain antigens located on cell surfaces. Antigens are substances used by the immune system to recognize self versus non-self; if the immune system encounters antigens not normally found on the body's own cells, it will attack the foreign cells. When patients receive blood transfusions, it is critical that the antigens of transfused cells match those of the patient's, or else an immune system response will be triggered.
		
The ABO blood group system consists of four different blood groups, which describe whether an individual's red blood cells carry the A antigen, B antigen, both, or neither. The ABO gene has three alleles: ${I}^{A}$, ${I}^{B}$, and \textit{i}. The \textit{i} allele is recessive to both ${I}^{A}$ and ${I}^{B}$, and does not produce antigens; thus, an individual with genotype ${I}^{A}i$ is blood group A and an individual with genotype ${I}^{B}i$ is blood group B. The ${I}^{A}$ and ${I}^{B}$ are codominant, such that individuals of ${I}^{A}$${I}^{B}$ genotype are AB. Individuals homozygous for the \textit{i} allele are known as blood group O, with neither A nor B antigens.}

Suppose that both members of a couple have Group AB blood.	
\begin{enumerate}[a)]
	\item What is the probability that a child of this couple will have Group A blood?
	\item What is the probability that they have two children with Group A blood?
\end{enumerate}	

\end{example}

\textit{DH: solutions to be added if we keep the exercise. JV: Changed this to an example, could add an exercise w/some of the numbers from the donor data? }

The previous examples in this section have used independence to solve probability problems. The definition of independence can also be used to check whether two events are independent -- two events $A$ and $B$ are independent if they satisfy Equation~\eqref{eqForIndependentEvents}.

\begin{example}{Is the event of drawing a heart from a deck of cards independent of drawing an ace?}

The probability the card is a heart is $1/4$ ($13/52=1/4$) and the probability that it is an ace is $1/13$ ($4/52=1/13$). The probability that the card is the ace of hearts is $1/52$. Check whether Equation~\ref{eqForIndependentEvents} is satisfied:

\begin{align*}
P({\color{redcards}\heartsuit})\times P(\text{ace}) = \frac{1}{4}\times \frac{1}{13} = \frac{1}{52} 
= P({\color{redcards}\heartsuit}\text{ and ace})
\end{align*}
Because the equation holds, the event that the card is a heart and the event that the card is an ace are independent events.
	
\end{example}

\begin{example}
 {In the general population, about 15\% of adults between 25 and 40  years of age are hypertensive.  Suppose that among males of this age, hypertension occurs about 18\% of the time.  Is hypertension independent of sex?} 

 \textit{DH: solution to be filled in if we keep it.  Emphasize in the solution how the wording here is more realistic than the playing card/dice examples.}
\end{example}

%_________________


\section{Conditional probability}
\label{conditionalProbabilitySection}

While precise estimates are difficult to come by, the US CDC estimated that in 2012, approximately 29.1 million Americans had type 2 diabetes - about 9.3\% of the population.\footnote{21 million of these cases are diagnosed, while the CDC predicts that 8.1 million cases are undiagnosed; that is, approximately 8.1 million people are living with diabetes, but they (and their physicians) are unaware that they have the condition.} A health care practitioner seeing a new patient should expect a 9.3\% chance that the patient might have diabetes. 

However, this is only the case if nothing is known about the patient. The prevalence of type 2 diabetes varies with age. Between the ages of 20 and 44, approximately 4\% of the population have diabetes, but almost 27\% of people age 65 and older have diabetes. Knowing the age of a patient provides information about the chance of diabetes; age and diabetes status are not independent. While the probability of diabetes in a randomly chosen member of the population is 0.093, the \textit{conditional} probability of diabetes in a person known to be 65 or older is 0.27.

Conditional probability is used to  characterize how the probability of an outcome varies with the knowledge of another factor or condition, and is closely related to the concepts of marginal and joint probabilities.


\subsection{Marginal and joint probabilities}
\label{marginalAndJointProbabilities}

\index{marginal probability|(}
\index{joint probability|(}

%  DH: OI has a nice venn diagram for joint distributions, but the diabetes/age example is probably too complicated for the diagram.

Tables~\ref{DiabetesAgeContTable} and \ref{DiabetesAgeProbTable} provide additional information about the relationship between diabetes prevalence and age.  \footnote{Because the CDC provides only approximate numbers for diabetes prevalence, the numbers in the table are approximations of actual population counts.} Table~\ref{DiabetesAgeContTable} is a contingency table like those discussed in Chapter 1, but for the entire US population in 2012; the values in the table are in thousands, to make the table more readable.  

In the first row, for instance, the table shows that in the entire population of approximately 313,320,000 people, approximately 200,000 individuals were in the less than 20 years age group and suffered from type 2 diagnosis -- about 0.1\%. The table also indicates that among the approximately 86,864,000 individuals less than 20 years of age, only 200,000 suffered from type 2 diabetes, approximately 0.2\%. 

The distinction between the two statements is small but important. The first provides information about the size of the type 2 diabetes population that is less than 20 years of age, relative to the entire population. In contrast, the second statement is about the size of the diabetes population in the less than 20 years of age group, relative to the size of that age group. 

% latex table generated in R 3.0.1 by xtable 1.7-1 package
% Thu Sep 10 11:36:48 2015
\begin{table}[ht]
	\centering
	\begin{tabular}{rrrr}
		\hline
		& Diabetes & No Diabetes & Sum \\ 
		\hline
		Less than 20 years & 200 & 86,664 & 86,864 \\ 
		20 to 44 years & 4,300 & 98,724 & 103,024 \\ 
		45 to 64 years & 13,400 & 68,526 & 81,926 \\ 
		Greater than 64 years & 11,200 & 30,306 & 41,506 \\ 
		Sum & 29,100 & 284,220 & 313,320 \\ 
		\hline
	\end{tabular}
	\caption{Contingency table showing type 2 diabetes status and age group, in thousands}
	\label{DiabetesAgeContTable}
\end{table}
% can xtable insert comma separators automatically?  If not, I will add them by hand. JV: apparently R can do this, but I didn't manage to get it working http://stackoverflow.com/questions/1581232/adding-commas-into-number-for-output

\begin{exercise} \label{DiabetesAge20to44}

What fraction of the US population are 45 to 64 years of age and have diabetes?  What fraction of the population age 45 to 64 have diabetes?

\end{exercise}

The counts in Table~\ref{DiabetesAgeContTable} have been converted to proportions by dividing each value in the cells of the contingency table by the total population size: 313,320,000. The entries in this table show the proportions of the population in each of the 8 categories defined by diabetes status and age. 

If these proportions are interpreted as probabilities for randomly chosen individuals from the population, 0.014 in the first column of row 2 implies that the probability of selecting someone at random who has diabetes and whose age is between 20 and 44 is 0.014, or 1.4\%. The entries in the 8 main table cells (excluding the values in the margins) are called \term{joint probabilities} since they specify the probability of two events happening at the same time -- diabetes and a particular age group. In probability notation, $0.014 = P(\text{diabetes and age 20 to 44})$. It is common to also write this as $P(\text{diabetes, age 20 to 44})$, with a comma replacing ``and''.

The values in the last row and column of the table are the sums of the corresponding rows or columns. Since 0.329 is the sum of the of the probabilities of the disjoint events (diabetes and age 20 to 44) and (no diabetes and age 20 to 44), it is the probability of being in the age group 20 to 44. The row and column sums are called \term{marginal probabilities}; they are probabilities about only one type of event, such as age in the case of 0.0329. The sum of the first column (0.093) is the marginal probability of a member of the population having diabetes.

%  xtable(diabetes.age.table.thousands, digits = 0)
%  this table uses census counts; still not perfect, but closer

% latex table generated in R 3.0.1 by xtable 1.7-1 package
% Thu Sep 10 11:44:44 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \hline
 & Diabetes & No Diabetes & Sum \\ 
  \hline
Less than 20 years & 0.001 & 0.277 & 0.277 \\ 
  20 to 44 years & 0.014 & 0.315 & 0.329 \\ 
  45 to 64 years & 0.043 & 0.219 & 0.261 \\ 
  Greater than 64 years & 0.036 & 0.097 & 0.132 \\ 
  Sum & 0.093 & 0.907 & 1.000 \\ 
   \hline
\end{tabular}
\caption{Probability table summarizing diabetes status and age group}
\label{DiabetesAgeProbTable}
\end{table}
%   xtable(diabetes.age.table.prop, digits = 3)


\begin{termBox}{\tBoxTitle{Marginal and joint probabilities}
If a probability is based on a single variable, it is a \emph{\hiddenterm{marginal probability}}. The probability of outcomes for two or more variables or processes is called a \emph{\hiddenterm{joint probability}}.}
\end{termBox}

\begin{exercise} 
  {What is the interpretation of the value 0.907 in the last row of the table?  Of the value 1.000 in the bottom right corner? }
\label{MarginalJointProbDiabetes}
\end{exercise}


\subsection{Defining conditional probability}

\index{conditional probability|(}

The probability that a randomly selected individual from the US has diabetes is 0.093, the sum of the first column in Table~\ref{DiabetesAgeProbTable}.  How does that probability change if it is known that the individual's age is 65 or greater?  Table~\ref{DiabetesAgeContTable} shows that 11,200,000 of the 41,506,000 people in that age group have diabetes, so the likelihood that someone from that age group has diabetes is:
\[  
     \frac{11,200,000}{41,506,000} = 0.27,
\]
or 27\%.  The additional information allows for a better estimate of the probability of diabetes; the conditional probability of diabetes, given the information that an individual is older than 65, is 0.27.

The conditional probability of diabetes given age 65 or greater is simply the ratio of the proportion of the population with diabetes and age 65 or greater divided by the proportion greater than age 65:
 
\begin{align*}
    \frac{\text{prop. of population with diabetes, age 65 or greater}}{\text{prop. of population greater than age 65}} &= \frac{11,200,000/313,320,000}{41,506,000/313,320,000}\\
			&=\frac{11,200,000}{41,506,000} \\
	                               &= \frac{0.036}{0.132} \\
								   &= 0.270,
\end{align*}

This leads to the mathematical definition of conditional probability.

\begin{termBox}{\tBoxTitle{Conditional probability}
The conditional probability of the outcome of interest $A$ given condition $B$ is computed as the following:
\begin{align}
P(A | B) = \frac{P(A\text{ and }B)}{P(B)}
\label{condProbEq}
\end{align}}
\end{termBox}

\begin{exercise}\label{familyCollegeProbOfParentsEqualNotGivenTeen}
(a) Write out the following statement in conditional probability notation: ``\emph{The probability a randomly selected person has diabetes, given that his or her age is between 45 and 64 }''. Note that the condition is now based on the {teenager}, not the {parent}. \\[1mm]
(b) Calculate the conditional probability in part (a). \\[1mm]
(c) Write out the following statement in conditional probability notation: ``\emph{The probability a randomly selected person is between 45 and 64 years old, given that the person has diabetes}''. Note that the condition is now based on the {diabetes}, not the {age}. \\[1mm]
(d) Calculate the probability in part (c).
\end{exercise}

%\subsection{Smallpox in Boston, 1721}

\index{data!smallpox|(}

\textit{DH: I am not fond of small pox example, since it is a biased sample, but leaving it for now. JV: If another example is added, it should become part of 2.2.2 instead of having its own subsection.}

The \data{smallpox} data set provides a sample of 6,224 individuals from the year 1721 who were exposed to smallpox in Boston.\footnote{Fenner F. 1988. \emph{Smallpox and Its Eradication (History of International Public Health, No. 6)}. Geneva: World Health Organization. ISBN 92-4-156110-6.} Doctors at the time believed that inoculation, which involves exposing a person to the disease in a controlled form, could reduce the likelihood of death.

Each case represents one person with two variables: \var{inoculated} and \var{result}. The variable \var{inoculated} takes two levels: \resp{yes} or \resp{no}, indicating whether the person was inoculated or not. The variable \var{result} has outcomes \resp{lived} or \resp{died}. These data are summarized in Tables~\ref{smallpoxContingencyTable} and~\ref{smallpoxProbabilityTable}.

\begin{table}[h]
\centering
\begin{tabular}{ll rr r}
& & \multicolumn{2}{c}{inoculated} & \\
\cline{3-4}
& & \resp{yes} & \resp{no} & Total  \\
\cline{2-5}
		& \resp{lived}     & 238 & 5136 & 5374 \\
\raisebox{1.5ex}[0pt]{\var{result}} &  \resp{died} \hspace{0.5cm} & 6 & 844 & 850  \\
\cline{2-5}
	& Total & 244 & 5980 & 6224 \\
\end{tabular}
\caption{Contingency table for the \data{smallpox} data set.}
\label{smallpoxContingencyTable}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ll rr r}
& & \multicolumn{2}{c}{inoculated} & \\
\cline{3-4}
& & \resp{yes} & \resp{no} & Total  \\
   \cline{2-5}
 & \resp{lived}     & 0.0382 & 0.8252 & 0.8634 \\
\raisebox{1.5ex}[0pt]{\var{result}} & \resp{died} \hspace{0.5cm} & 0.0010 & 0.1356  & 0.1366  \\
   \cline{2-5}
& Total & 0.0392 & 0.9608 & 1.0000 \\
\end{tabular}
\caption{Table proportions for the \data{smallpox} data, computed by dividing each count by the table total, 6224.\textC{\vspace{-2mm}}}
\label{smallpoxProbabilityTable}
\end{table}

%\textC{\newpage}

\begin{exercise} \label{probDiedIfNotInoculated}
Write out, in formal notation, the probability a randomly selected person who was not inoculated died from smallpox, and find this \mbox{probability.}\footnote{$P($\var{result} = \resp{died} $|$ \var{inoculated} = \resp{no}$) = \frac{P(\text{\var{result} = \resp{died} and \var{inoculated} = \resp{no}})}{P(\text{\var{inoculated} = \resp{no}})} = \frac{0.1356}{0.9608} = 0.1411$.}
\end{exercise}

\begin{exercise}
Determine the probability that an inoculated person died from smallpox. How does this result compare with the result of Guided Practice~\ref{probDiedIfNotInoculated}?\footnote{$P($\var{result} = \resp{died} $|$ \var{inoculated} = \resp{yes}$) = \frac{P(\text{\var{result} = \resp{died} and \var{inoculated} = \resp{yes}})}{P(\text{\var{inoculated} = \resp{yes}})} = \frac{0.0010}{0.0392} = 0.0255$. The death rate for individuals who were inoculated is only about 1~in~40 while the death rate is about 1~in~7 for those who were not inoculated.}
\end{exercise}

\begin{exercise}\label{SmallpoxInoculationObsExpExercise}
The people of Boston self-selected whether or not to be inoculated. (a) Is this study observational or was this an experiment? (b) Can we infer any causal connection using these data? (c) What are some potential confounding variables that might influence whether someone \resp{lived} or \resp{died} and also affect whether that person was inoculated?\footnote{Brief answers: (a)~Observational. (b)~No, we cannot infer causation from this observational study. (c)~Accessibility to the latest and best medical care. There are other valid answers for part~(c).}
\end{exercise}

\subsection{General multiplication rule}

Section~\ref{probabilityIndependence} introduced the Multiplication Rule for independent processes. Here, the \term{General Multiplication Rule} is introduced for events that might not be independent.

\begin{termBox}{\tBoxTitle{General Multiplication Rule}
If $A$ and $B$ represent two outcomes or events, then \vspace{-1.5mm}
\begin{eqnarray*}
P(A\text{ and }B) = P(A | B)\times P(B)
\end{eqnarray*} \vspace{-6.5mm} \par
It is useful to think of $A$ as the outcome of interest and $B$ as the condition.}
\end{termBox}
This General Multiplication Rule is simply a rearrangement of the definition for conditional probability in Equation~(\ref{condProbEq}) on page~\pageref{condProbEq}.

\begin{example}{Consider the \data{smallpox} dataset. Suppose we are given only two pieces of information: 96.08\% of residents were not inoculated, and 85.88\% of the residents who were not inoculated ended up surviving. How could we compute the probability that a resident was not inoculated and lived? \textit{JV: This will also need to change if smallpox example is eliminated.}}
We will compute our answer using the General Multiplication Rule and then verify it using Table~\ref{smallpoxProbabilityTable}. We want to determine
\begin{eqnarray*}
P(\text{\var{result} = \resp{lived} and \var{inoculated} = \resp{no}})
\end{eqnarray*}
and we are given that
\begin{eqnarray*}
P(\text{\var{result} = \resp{lived} }|\text{ \var{inoculated} = \resp{no}})=0.8588 \\
P(\text{\var{inoculated} = \resp{no}})=0.9608
\end{eqnarray*}
Among the 96.08\% of people who were not inoculated, 85.88\% survived:
\begin{eqnarray*}
P(\text{\var{result} = \resp{lived} and \var{inoculated} = \resp{no}}) = 0.8588\times 0.9608 = 0.8251
\end{eqnarray*}
This is equivalent to the General Multiplication Rule. We can confirm this probability in Table~\ref{smallpoxProbabilityTable} at the intersection of \resp{no} and \resp{lived} (with a small rounding error).
\end{example}

\begin{exercise}
Use $P($\var{inoculated} = \resp{yes}$) = 0.0392$ and $P($\var{result} = \resp{lived} $|$ \var{inoculated} = \resp{yes}$) = 0.9754$ to determine the probability that a person was both inoculated and lived.\footnote{The answer is 0.0382, which can be verified using Table~\ref{smallpoxProbabilityTable}.}
\end{exercise}

\begin{exercise}
If 97.45\% of the people who were inoculated lived, what proportion of inoculated people must have died?\footnote{There were only two possible outcomes: \resp{lived} or \resp{died}. This means that 100\% - 97.45\% = 2.55\% of the people who were inoculated died.}
\end{exercise}

\textit{JV: I am not clear on why this tipbox appears in this subsection. Would it be better at the end of the previous subsection?}

\begin{termBox}{\tBoxTitle{Sum of conditional probabilities}
Let $A_1$, ..., $A_k$ represent all the disjoint outcomes for a variable or process. Then if $B$ is an event, possibly for another variable or process, we have: \vspace{-1mm}
\begin{eqnarray*}
P(A_1|B)+\cdots+P(A_k|B) = 1
\end{eqnarray*}\vspace{-5.5mm} \par
The rule for complements also holds when an event and its complement are conditioned on the same information: \vspace{-1.5mm}
\begin{eqnarray*}
P(A | B) = 1 - P(A^c | B)
\end{eqnarray*}}
\end{termBox}

\begin{exercise}
Based on the probabilities computed above, does it appear that inoculation is effective at reducing the risk of death from smallpox?\footnote{The samples are large relative to the difference in death rates for the ``inoculated'' and ``not inoculated'' groups, so it seems there is an association between \var{inoculated} and \var{outcome}. However, as noted in the solution to Guided Practice~\ref{SmallpoxInoculationObsExpExercise}, this is an observational study and we cannot be sure if there is a causal connection. (Further research has shown that inoculation is effective at reducing death rates.)}
\end{exercise}


\subsection{Independence and conditional probability}

If two events are independent, knowing the outcome of one should provide no information about the other.  That intuitively clear statement can be shown mathematically. 

\begin{exercise} \label{condProbOfRollingA1AfterOne1}
Let $X$ and $Y$ represent the outcomes of rolling two dice.\footnote{Brief solutions: (a) $1/6$. (b) $1/36$. (c)~$\frac{P(Y = \text{ \resp{1} and }X=\text{ \resp{1}})}{P(X=\text{ \resp{1}})} = \frac{1/36}{1/6} = 1/6$. (d)~The probability is the same as in part~(c): $P(Y=1)=1/6$. The probability that $Y=1$ was unchanged by knowledge about $X$, which makes sense as $X$ and $Y$ are independent.}
\begin{enumerate}[(a)]
\item What is the probability that the first die, $X$, is \resp{1}?
\item What is the probability that both $X$ and $Y$ are \resp{1}?
\item Use the formula for conditional probability to compute $P(Y =$ \resp{1}$\ |\ X = $ \resp{1}$)$.
\item What is $P(Y=1)$? Is this different from the answer from part (c)? Explain.
\end{enumerate}
\end{exercise}

\textC{\newpage}

It is not difficult to show in Guided Practice~\ref{condProbOfRollingA1AfterOne1}(c) that the conditioning information has no influence by using the Multiplication Rule for independence events:
\begin{eqnarray*}
P(Y=\text{\resp{1}}\ |\ X=\text{\resp{1}})
	&=& \frac{P(Y=\text{\resp{1} and }X=\text{\resp{1}})}{P(X=\text{\resp{1}})} \\
	&=& \frac{P(Y=\text{\resp{1}})\times \color{oiGB}P(X=\text{\resp{1}})}{\color{oiGB}P(X=\text{\resp{1}})} \\
	&=& P(Y=\text{\resp{1}}) \\
\end{eqnarray*}



\begin{exercise}
Casinos often rely on gamblers not understanding independence.  Suppose the last five outcomes on a roulette table were \resp{black}.  What is wrong with the reasoning that the next outcome is highly likely to be red? \textit{JV: Needs footnote solution.}
\end{exercise}

There is a subtle but important point behind the last example.  The probability of the next outcome being black is different than the probability that the sixth outcome is black when a gambler has seen the last five outcomes and knows that they are black.  This is an example of an unconditional versus a conditional probability.

\textit{JV: This section definitely needs some re-working -- comes across as rather lifeless right now. Could add an example in which independence is illustrated using the conditional probability formula.}

\subsection{Tree diagrams}

\index{data!smallpox|)}
\index{tree diagram|(}

\termsub{Tree diagrams}{tree diagram} are a tool to organize outcomes and probabilities around the structure of the data. They are most useful when two or more processes occur in a sequence and each process is conditioned on its predecessors.

The \data{smallpox} data fit this description. The population is split by \var{inoculation}: \resp{yes} and \resp{no}. Following this split, survival rates were observed for each group. This structure is reflected in the \term{tree diagram} shown in Figure~\ref{smallpoxTreeDiagram}. The first branch for \var{inoculation} is said to be the \term{primary} branch while the other branches are \term{secondary}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.93\textwidth]{ch_probability_oi_biostat/figures/smallpoxTreeDiagram/smallpoxTreeDiagram}
\caption{A tree diagram of the \data{smallpox} data set.}
\label{smallpoxTreeDiagram}
\end{figure}

Tree diagrams are annotated with marginal and conditional probabilities, as shown in Figure~\ref{smallpoxTreeDiagram}. This tree diagram splits the smallpox data by \var{inoculation} into the \resp{yes} and \resp{no} groups with respective marginal probabilities 0.0392 and 0.9608. The secondary branches are conditioned on the first, so we assign conditional probabilities to these branches. For example, the top branch in Figure~\ref{smallpoxTreeDiagram} is the probability that \var{result} = \resp{lived} conditioned on the information that \var{inoculated} = \resp{yes}. Joint probabilities can be constructed at the end of each branch in the tree by multiplying the numbers from left to right. These joint probabilities are computed using the General Multiplication Rule:
\begin{eqnarray*}
&& P(\text{\var{inoculated} = \resp{yes} and \var{result} = \resp{lived}}) \\
	&&\quad = P(\text{\var{inoculated} = \resp{yes}})\times P(\text{\var{result} = \resp{lived}}|\text{\var{inoculated} = \resp{yes}}) \\
	&&\quad = 0.0392\times 0.9754=0.0382
\end{eqnarray*}


In addition to being a graphical representation of how to compute a probability, tree diagrams are a useful way to organize the information in a probability problem and can often reduce a seemingly difficult problem to a series of simple steps

\begin{example}  

In the general population, about 1 in 28 individuals (approximately 3.6\%) is an unaffected carrier of a mutation in the cystic fibrosis gene, \textit{CFTR}, discussed in example~\ref{CFInheritanceExample}.  Most unaffected carriers are unaware that they harbor the mutation.  Suppose that people with cystic fibrosis do not live long enough to reproduce with a partner.  In the absence of any testing information, what is the probability that a child of two parent will have CF?
	
	\textit{DH: solution not given for now, since I did not want to take the time to draw the tree.  This will be a good example to also solve algebraically, to show that the tree diagram is a representation of the theorem of total prob. Useful, since it will come up with Bayes Thm. Borrow some of the wording from the solution to the grade problem in OI that I removed here.  Below has a start of the solution}
	
	
\textit{solution}
The probabilities calculated in example~\ref{CFInheritanceExample} were conditional probabilities given the carrier status of the parents, though that term was not used in the example.  This exercise asks for an unconditional probability about the disease status of the child, with no additional information about the parents.  The solution uses methods already discussed, but combined is a series of steps.  

The main steps are to first enumerate all the possibilities for carrier status of the parents, calculate the probabilities of each of those probabilities, then calculate the conditional probability of an affected child, given each of the possible outcomes of the parents, and finally use a tree to calculate the probability of a child being affected. Since the problem assumes that affected parents do not reproduce, the pair of parents must satisfy one of the events $A$ = (neither parent is a carrier), $B$ (the mother is a carrier, the father is not), $C$ = (the father is a carrier, the mother is not), $D$ = (both parents are carriers). Since the carrier status of the mother is independent of that of the father, the probabilities of these events are:

\begin{itemize}
	\item $P(A) = (1 - 0.036)(1 - 0.036) = 0.929$
	
	\item $P(B) = P(C) = (1 - 0.036)(0.036) = 0.035$
	
	\item $P(D) = (0.036)(0.036) = 0.001$
\end{itemize}
	
These probabilities do not sum to one because the events where one or both parents are homozygous for the mutation, that is, are affected.

\textit{DH: solution continues now with the conditional probabilities and the tree.  We should think about whether this example is too big of a step from the others, and perhaps add an intermediate example}

\textit{JV: I agree that this might be too much at once... }

\end{example}

\subsection{Bayes' Theorem}
\label{bayesTheoremSubsection}

\index{Bayes' Theorem|(}

This chapter began with a straightforward question -- what are the chances that a woman with an abnormal (i.e., positive) mammogram has breast cancer?  For a clinician, this question can be rephrased as the conditional probability  that a woman has breast cancer, given that her mammogram is abnormal. This conditional probability is called the \term{positive predictive value} of a mammogram.  The characteristics of a mammogram (and diagnostic tests in general) are given with the reverse conditional probabilities --  the probability that if a woman fact has breast cancer, a mammogram will detect it.  More concisely, if $A = \text{(a mammogram is  positive)}$ and $B = \text{(a woman has breast cancer)}$, the first question is answered by finding $P(A|B)$, while the the known characteristics of a mammogram specify $P(B|A)$.

Many problems provide information about  
\begin{align*}
P(\text{statement about variable 1} | \text{ statement about variable 2})
\end{align*}
but ask for the reverse conditional probability:
\begin{align*}
P(\text{statement about variable 2} | \text{ statement about variable 1})
\end{align*}

The problem arises so often in medicine, and is so important, that is useful to have several strategies to approach the problem -- 1) constructing tree diagrams, 2) using a purely algebraic approach using Bayes' Theorem, and 3) the construction of a large, hypothetical population that allows conditional probabilities to be calculated from contingency tables.  The first solution uses tree diagrams and more specific information about mammograms and breast cancer.

\textit{JV: What do you think about condensing the tree diagram subsection (mainly the introduction section) and adding that material here?}

\begin{example}{In Canada, about 0.35\% of women over 40 will develop breast cancer in any given year. A common screening test for cancer is the mammogram, but it is not perfect. In about 11\% of patients with breast cancer, the test gives a \term{false negative}: it indicates a woman does not have breast cancer when she does have breast cancer. Similarly, the test gives a \term{false positive} in 7\% of patients who do not have breast cancer: it indicates these patients have breast cancer when they actually do not.\footnote{The probabilities reported here were obtained using studies reported at \oiRedirect{textbook-breastCancerDotOrg_20090831b}{www.breastcancer.org} and \oiRedirect{textbook-ncbi_nih_breast_cancer}{www.ncbi.nlm.nih.gov/pmc/articles/PMC1173421}.} If a random selected woman over 40 is tested for breast cancer using a mammogram and the test is positive -- that is, the test suggests the woman has cancer -- what is the probability she has breast cancer?} 

\label{probabilityOfBreastCancerGivenPositiveTestExample}

\begin{figure}[h]
\centering
\includegraphics[width=0.93\textwidth]{ch_probability_oi_biostat/figures/BreastCancerTreeDiagram/BreastCancerTreeDiagram}
\caption{Tree diagram for Example~\ref{probabilityOfBreastCancerGivenPositiveTestExample}, computing the probability a random patient who tests positive on a mammogram has breast cancer.}
\label{BreastCancerTreeDiagram}
\end{figure}

The problem provides enough information to compute the probability of testing positive if a woman has breast cancer ($1.00-0.11=0.89$), but not the reverse conditional probability of cancer given a positive test result.  This reverse conditional probability may be broken into two pieces:
\begin{align*}
P(\text{has BC } | \text{ mammogram$^+$}) = \frac{P(\text{has BC and mammogram$^+$})}{P(\text{mammogram$^+$})}
\end{align*}
where ``has BC'' is an abbreviation for breast cancer and ``mammogram$^+$'' means the mammogram screening was positive. A tree diagram can be used to identify each probability and is shown in Figure~\ref{BreastCancerTreeDiagram}. The probability the woman has breast cancer and the mammogram is positive is
\begin{align*}
P(\text{has BC and mammogram$^+$}) &= P(\text{mammogram$^+$ } | \text{ has BC})P(\text{has BC}) \\
	&= 0.89\times 0.0035 = 0.00312
\end{align*}
The probability of a positive test result is the sum of the two corresponding scenarios:
\begin{align*}
P(\text{\underline{\color{black}mammogram$^+$}}) &= P(\text{\underline{\color{black}mammogram$^+$} and has BC}) + P(\text{\underline{\color{black}mammogram$^+$} and no BC}) \\
	&= P(\text{has BC})P(\text{mammogram$^+$ } | \text{ has BC}) \\
	&\qquad\qquad	+ P(\text{no BC})P(\text{mammogram$^+$ } | \text{ no BC}) \\
	&= 0.0035\times 0.89 + 0.9965\times 0.07 = 0.07288
\end{align*}
Then if the mammogram screening is positive for a patient, the probability the patient has breast cancer is
\begin{align*}
P(\text{has BC } | \text{ mammogram$^+$})
	&= \frac{P(\text{has BC and mammogram$^+$})}{P(\text{mammogram$^+$})}\\
	&= \frac{0.00312}{0.07288} \approx 0.0428
\end{align*}
Even with a positive mammogram, there is still only a~4\%~chance of breast cancer.  Most people find this a surprising result.  \textit{DH: return to this to finish why low prevalence leads to low ppv. that is the important concept for pre-meds and others}
\end{example}


Consider again the last equation of Example~\ref{probabilityOfBreastCancerGivenPositiveTestExample}.
Using the tree diagram, we can see that the numerator (the top of the fraction) is equal to the following product:
\begin{align*}
P(\text{has BC and mammogram$^+$}) = P(\text{mammogram$^+$ } | \text{ has BC})P(\text{has BC})
\end{align*}
The denominator -- the probability the screening was positive -- is equal to the sum of probabilities for each positive screening scenario:
\begin{align*}
P(\text{\underline{\color{black}mammogram$^+$}})
	&= P(\text{\underline{\color{black}mammogram$^+$} and no BC})
		+ P(\text{\underline{\color{black}mammogram$^+$} and has BC})
\end{align*}
In the example, each of the probabilities on the right side was broken down into a product of a conditional probability and marginal probability using the tree diagram.
\begin{align*}
P(\text{mammogram$^+$})
	&= P(\text{mammogram$^+$ and no BC}) + P(\text{mammogram$^+$ and has BC}) \\
	&= P(\text{mammogram$^+$ } | \text{ no BC})P(\text{no BC}) \\
			   &\qquad\qquad + P(\text{mammogram$^+$ } | \text{ has BC})P(\text{has BC})
\end{align*}
We can see an application of Bayes' Theorem by substituting the resulting probability expressions into the numerator and denominator of the original conditional probability.
\begin{align*}
& P(\text{has BC } | \text{ mammogram$^+$})  \\
& \qquad= \frac{P(\text{mammogram$^+$ } | \text{ has BC})P(\text{has BC})}
	{P(\text{mammogram$^+$ } | \text{ no BC})P(\text{no BC}) + P(\text{mammogram$^+$ } | \text{ has BC})P(\text{has BC})}
\end{align*}

\begin{termBox}{\tBoxTitle{Bayes' Theorem}
Consider the following conditional probability for variable 1 and variable 2:\vspace{-1.5mm}
\begin{align*}
P(\text{outcome $A_1$ of variable 1 } | \text{ outcome $B$ of variable 2})
\end{align*}
Bayes' Theorem states that this conditional probability can be identified as the following fraction:\vspace{-1.5mm}
\begin{align}
\frac{P(B | A_1) P(A_1)}
	{P(B | A_1) P(A_1) + P(B | A_2) P(A_2) + \cdots + P(B | A_k) P(A_k)}
	\label{equationOfBayesTheorem}
\end{align}
where $A_2$, $A_3$, ..., and $A_k$ represent all other possible outcomes of the first variable.}\index{Bayes' Theorem|textbf}
\end{termBox}

Bayes' Theorem is also called Bayes' rule.

Bayes' Theorem is just a generalization of what we have done using tree diagrams. The numerator identifies the probability of getting both $A_1$ and $B$. The denominator is the marginal probability of getting $B$. This bottom component of the fraction appears long and complicated since we have to add up probabilities from all of the different ways to get $B$. We always completed this step when using tree diagrams. However, we usually did it in a separate step so it didn't seem as complex.

To apply Bayes' Theorem correctly, there are two preparatory steps:
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item[(1)] First identify the marginal probabilities of each possible outcome of the first variable: $P(A_1)$, $P(A_2)$, ..., $P(A_k)$.
\item[(2)] Then identify the probability of the outcome $B$, conditioned on each possible scenario for the first variable: $P(B | A_1)$, $P(B | A_2)$, ..., $P(B | A_k)$.
\end{enumerate}
Once each of these probabilities are identified, they can be applied directly within the formula.

\textit{JV: Agree that Bayes' Rule section still needs some work. I am not sure how to make the flow smoother, yet. Table method needs to be added, as well.}

%%%%%
%
% revisions to here 14sep2015 11:36.  Not satisfied with the treatment of BR here, and we need more examples


% index commands below should be inserted just before section on sampling from a small population

\index{Bayes' Theorem|)}
\index{tree diagram|)}
\index{conditional probability|)}
\index{probability|)}


\textC{\newpage}



%_________________
\section{Random variables}
\label{randomVariablesSection}

\index{random variable|(}

Some forms of cancer, such as advanced lung cancer, unfortunately have treatments which are effective in a small percentage of patients, typically 20\% or less. In planning studies in clinical research, investigators try to anticipate the results they might see, at least under certain hypotheses. In planning a study of a new drug in advanced cancer, it is useful to ask what the expected outcome might be if, for instance, only 20\% of patients would have a good response (another unsuccessful treatment), or if, perhaps, 50\% of patients would have a good response (a potentially useful drug). In a study with 20 participants, one would expect 4 of the 20 participants to have a good response if the drug does not look promising, and 10 if the drug was promising. Other numbers of good responses would be expected for treatments that were either more or less successful than the ones in this example.

The anticipated outcome in this hypothetical study is an example of a \term{random variable}, something that measures the outcome numerically, but is not observed before the study is conducted and so appears random. Random variables provide mathematical models for data before it has been observed, and are useful in many applications of statistics in addition to study planning. Random variables can be used to model more than just the expected outcome of a study. Under the working assumption that 20\% of patients would respond to a treatment is true, investigators would expect 4 responses, but would not be surprised to see 3 or 5. But they might be surprised to see 12 or more responses; random variables provide a tool for calculating precisely how surprising that might be.

This section outlines some general concepts used with random variables; chapter 3 explores specific and particularly useful types of random variables.

\subsection{Distributions of Random Variables}

Formally, a random variable assigns numerical values to the outcome of a random phenomenon, and is usually written with a capital letter such as $X$, $Y$, or $Z$.  Rolling dice or tossing coins provide simple but instructive examples of random variables.  If a coin is tossed three times, the outcome is the sequence of observed heads and tails, and might be (TTH), tails on the first two tosses, heads on the third.  If the random variable $X$ is the number of heads on the three tosses, $X = 1$. If $Y$ is the number of tails, $Y = 2$.  For the sequence of outcomes (THT), $X$ and $Y$ have the same values 1 and 2. But for the sequence (HHH), $X = 3$ and $Y = 0$.  Even in this simple setting, it is possible to define other random variables.  If $Z$ is the toss when the first heads occurs, then $Z=3$ for the first set of 3 tosses and 1 for the second set.

If probabilities can be assigned to the outcomes in a random phenomenon or study, those can be used to assign probabilities to values of a random variable.  Using independence, $P(\text{HHH}) = (1/2)^3 = 1/8$.  Since $X$ in the above example can only be three if the three tosses are all heads, $P(X=3) = 1/8$.  The distribution of a random variable is the collection of probabilities for all of the variables unique values.  When a coin is tossed 3 times, the 8 possible outcomes are (TTT, HTT, THT, TTH, HHT, HTH, THH, HHH). The variable $X = 0$ for the first of these tosses, $X = 1$ for the next 3 sequences, $X = 2$ for next 3, and $X = 3$ for the last sequence (HHH).  Using independence again, each of the 8 outcomes have probability 1/8, so $P(X = 0) = P(X = 3) = 1/8$ and $P(X = 1) = P(X = 2) = 3/8$. For random variables with a small number of outcomes, it is often easier and more instructive to display its distribution in a table. Table~\ref{distCoinTossing} shows the probability distribution for $X$.  Probability distributions for random variables follow the rules for probability, so, for instance, the sum of the probabilities must be 1.00.  The possible outcomes of $X$ are labeled with a corresponding lower case letter $x$ and subscripts.  The values of X are $x_1=0$, $x_2=1$,  $x_3 = 2$, and $x_4 = 3$; these occur with probabilities $1/8$, $3/8$, $3/8$ and $1/8$.

\begin{table}[h]
\centering 
\begin{tabular}{l rrrr l}
	\hline 
	$i$ & 1 & 2 & 3 & 4 & Total\\
	\hline
	$x_i$ & 0 & 1 & 2 & 3 & --\\
	 $P(X = x_i)$ & 1/8 & 3/8 & 3/8 & 1/8 & 8/8 = 1.00\\
\end{tabular}
\caption{Tabular form of the distribution of the number of Heads in 3 coin tosses}
\label{distCoinTossing}
\end{table}

Bar graphs can be used to show the distribution of a random variable, just as histograms or bar graphs provide a visual summary of the distribution of a dataset. Instead of showing the frequency of observations in a dataset, the heights of the bars in the graph of a probability distribution show the probabilities of possible values of a random variable.  Figure~\ref{barPlotCoinTossing} is a bar graph of the distribution of $X$ in the coin tossing example.

\begin{figure}[h]
\centering
\includegraphics[width=0.60\textwidth]
{ch_probability_oi_biostat/figures/barPlotCoinTossing/barPlotCoinTossing.pdf}
\caption{Bar plot of the distribution of the number of heads in 3 coin tosses }
\label{barPlotCoinTossing}
\end{figure}

$X$ is an example of a \term{discrete random variable} since it takes on a finite number of values\footnote{Some discrete random variables have an infinite number of possible values, such as all the non-negative integers.}  Because $X$ arises in a simple experiment (3 tosses of a coin) where probabilities are easy to calculate, calculating the probability distribution of $X$ is straightforward and the bar plot is not particularly informative.  

The hypothetical clinical study at the beginning of this section raised the question of how surprising it would it be to observe 12 or more responses in a setting where approximately 20\% of patients respond to a treatment.  Suppose $X$ is a random variable that will denote the number of responding participants in a clinical study being designed.  $X$ will have the same probability distribution as the number of heads in 20 tosses of a weighted coin, where the probability of landing heads is 0.2 instead of $1/2 = 0.5$.  The graph of the probability distribution for $X$ in figure~\ref{distRespClinStudy} can be used to approximate this probability\footnote{This and other useful distribution will be examined in detail  in Chapter 3}. The nine values (12, 13,\ldots, 20) make up the event 12 or more, and the graph shows that each of these probabilities is smaller than 0.005, so the chance of 12 or more responses is less than 0.01, or clearly less than 1\%.  Formulas in Chapter 3 can be used to show that the exact probability is slightly larger than 0.0001.

\begin{figure}[h]
\centering
\includegraphics[width=0.60\textwidth]
{ch_probability_oi_biostat/figures/distRespClinStudy/distRespClinStudy.pdf}
\caption{Bar plot of the distribution of the number of responses in a study with 20 participants and response probability 0.20}
\label{disRespClinStudy}
\end{figure}

 \textit{this example can be improved, and the graph could be improved as well.}


\subsection{Expectation}

\index{expectation|(}

Just like distributions of data, distributions of random variables have means, variances, standard deviations, medians, etc., but these characteristics of random variables are computed differently.  To calculate the mean of a random variable, add each multiply each possible value by its corresponding probability and add these products.  The mean of a random variable is called its \term{expected value} and written $E(X)$. the term is well-chosen -- it is the value one would expect to see if the variable were to be observed. For the variable $X$ in the last section,

\begin{align*}
E(X) &= (0)(P(X=0)) + (1)(P(X=1)) + (2)(P(X=2)) + (3)(P(X = 3)) \\
	&= (0))(1/8) + (1)(3/8) + (2)(3/8) + (3)(1/8) = 12/8 = 1.5
\end{align*}

\begin{termBox}{\tBoxTitle{Expected value of a Discrete Random Variable}
If $X$ takes outcomes $x_1$, ..., $x_k$ with probabilities $P(X=x_1)$, ..., $P(X=x_k)$, the expected value of $X$ is the sum of each outcome multiplied by its corresponding probability:
\begin{align}
E(X) 	&= x_1\times P(X=x_1) + \cdots + x_k\times P(X=x_k) \notag \\
	&= \sum_{i=1}^{k}x_iP(X=x_i)
\end{align}
The Greek letter $\mu$\index{Greek!mu ($\mu$)} may be used in place of the notation $E(X)$.}
\end{termBox}
The expected value for a random variable represents the average outcome. For example, $E(X)=1.5$ represents the average number of heads in three tosses of a coin, if the three tosses were repeated many times; this is also written $\mu=1.5$.  It often happens with discrete random variables that the expected value is not one of possible outcomes of the variable.

The expected value of a random variable has the same interpretation as a mean in a dataset.  The distribution can be represented by a series of weights at each outcome, and the mean represents the balancing point.

\textit{I have omitted the material on continuous distribuions and may re-insert it later.  I also did not include figure 2.22 from OI, which I like.  Maybe we should put that back as well }

\index{expectation|)}

\subsection{Variability in random variables}

How variable will the outcomes be when tossing a coin three times?  The \indexthis{variance}{variance} and \indexthis{standard deviation}{standard deviation} can be used to describe the variability of a random variable. Section~\ref{variability}
introduced a method for finding the variance and standard deviation for a data set. We first computed deviations from the mean ($x_i - \mu$), squared those deviations, and took an average to get the variance. In the case of a random variable, we again compute squared deviations. However, we take their sum weighted by their corresponding probabilities, just like we did for the expectation. This weighted sum of squared deviations equals the variance, and we calculate the standard deviation by taking the square root of the variance, just as we did in Section~\ref{variability}.

\begin{termBox}{\tBoxTitle{General variance formula}
If $X$ takes outcomes $x_1$, ..., $x_k$ with probabilities $P(X=x_1)$, ..., $P(X=x_k)$ and expected value $\mu=E(X)$, then the variance of $X$, denoted by $\text{Var}(X)$ or the symbol $\sigma^2$, is
\begin{align}
\sigma^2 &= (x_1-\mu)^2\times P(X=x_1) + \cdots \notag \\
	& \qquad\quad\cdots+ (x_k-\mu)^2\times P(X=x_k) \notag \\
	&= \sum_{j=1}^{k} (x_j - \mu)^2 P(X=x_j)
\end{align}
The standard deviation of $X$, labeled $\sigma$\index{Greek!sigma ($\sigma$)}, is the square root of the variance.}
\end{termBox}
\marginpar[\raggedright\vspace{-47mm}

$\text{Var}(X)$\vspace{1mm}\\\footnotesize Variance\\of $X$]{\raggedright\vspace{-47mm}

$\text{Var}(X)$\vspace{1mm}\\\footnotesize Variance\\of $X$}


%%%%%% ended here, 8 Oct 2015, 17:00

\begin{example}{Compute the variance and standard deviation of $X$, the number of heads in three tosses of a coin.}
%% do this directly, skipping the table.  After this example, do a more difficult one, perhaps profitability of health care or grades in Stat 102 last year.  Hospital CEO may construct a building.	
	
It is helpful to construct a table that holds computations for each outcome separately, then add up the results.  Since $E(X)$ is used the calculation of the variance, that calculation is added to the table, using decimals instead of fractions
\begin{center}
\begin{tabular}{l rrrr r}
\hline
$i$ & 1 & 2 & 3 & 4 & Total \\
\hline
$x_i$ & \$0 & 1 & 2 & 3 &  \\
$P(X=x_i)$ & 0.20 & 0.55 & 0.25 &  \\
$x_i \times  P(X=x_i)$ & 0 & 75.35 & 42.50 & 117.85 \\
\hline
\end{tabular}
\end{center}
Thus, the expected value is $\mu=117.85$, which we computed earlier. The variance can be constructed by extending this table:
\begin{center}
\begin{tabular}{l rrr r}
\hline
$i$ & 1 & 2 & 3 & Total \\
\hline
$x_i$ & \$0 & \$137 & \$170 &  \\
$P(X=x_i)$ & 0.20 & 0.55 & 0.25 &  \\
$x_i \times  P(X=x_i)$ & 0 & 75.35 & 42.50 & 117.85 \\
$x_i - \mu$ & -117.85 & 19.15 & 52.15 &  \\
$(x_i-\mu)^2$ & 13888.62 &  366.72 & 2719.62 &  \\
$(x_i-\mu)^2\times P(X=x_i)$ & 2777.7 & 201.7 & 679.9 & 3659.3 \\
\hline
\end{tabular}
\end{center}
The variance of $X$ is $\sigma^2 = 3659.3$, which means the standard deviation is $\sigma = \sqrt{3659.3} = \$60.49$.
\end{example}

\end{doublespace}