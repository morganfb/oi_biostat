\chapter{Foundations for inference} 
\label{foundationsForInference}

The United States Center for Disease Control and Prevention (CDC) focuses ``on policy and environmental strategies to make healthy eating and active living accessible and affordable for everyone.'' \footnote{\url{http://www.cdc.gov/obesity/}} One of the CDC's top priorities is understanding national obesity. A Body mass index (BMI) captures a person's height and weight within one measurement -- a high BMI indicates high body fat. The World Health Organization (WHO) classifies adults as underweight, healthy weight, overweight, and obese according to BMI. Below is a categorization provided by the WHO. \footnote{\url{http://apps.who.int/bmi/index.jsp?introPage=intro_3.html}}

\begin{center}
\begin{tabular}{|c|c|}
\hline 
Category & BMI range\tabularnewline
\hline 
\hline 
Underweight & $<18.50$\tabularnewline
\hline 
Normal (healthy weight) & 18.5-24.99\tabularnewline
\hline 
Overweight & $\geq 25$\tabularnewline
\hline 
Obese & $\geq30$\tabularnewline
\hline 
\end{tabular}
\end{center}

These policy makers are interested in the following questions: What is the average population BMI for adults in the United States? Instead of providing a single number, what is a reasonable range for the average U.S. adult BMI? Finally, is the average adult BMI in 2000 different than the average BMI 20 years ago?

These questions encompass the general ideas for statistical inference covered in Chapter \ref{foundationsForInference}. Inference is a set of tools used to estimate properties of a population, also known as parameters, after observing a sample from the population. Inference allows for different levels of confidence for these property estimates. Once they estimate the average adult BMI in the United States from a sample, scientists can ask how confident are they that this estimate is representative of the U.S. adult population. A classic inferential question is, ``How likely is it that the estimated mean, $\overline{x}$, is near the population mean, $\mu$?'' Statistical inference includes asking these questions but also determining which estimates to use.

Chapter \ref {foundationsForInference} provides the groundwork for inference on a population from observing one sample. Later chapters will cover inference comparing two or more distinct populations. The equations and details change depending on the setting, but the foundations and general procedures are the same throughout statistical inference. Understanding how to make inferences using one sample in this chapter will provide familiarity for more sophisticated methods in upcoming chapters. 

%__________________
\section{BRFSS data}
\label{brfssData}
The Behavioral Risk Factor Surveillance System (BRFSS), organized by the CDC and started in 1984, is the world's largest on-going telephone health survey system. This survey is nationwide and aims to ``monitor state-level prevalence of major behavioral risks among adults associated with premature morbidity and mortality.'' \footnote{\url{http://www.cdc.gov/brfss/about/about_brfss.htm}} Questionnaire topics include smoking, alcohol use, diet and exercise. The annual survey data from 2000, \data{BRFSS}, could be used to estimate the average adult BMI of the U.S. population. The four variables\footnote{There are a total of 289 variables in the dataset.} that the CDC is interested in are listed in Table~\ref{brfssBMIVariables}. 


\begin{comment} http://www.cdc.gov/brfss/annual_data/annual_2000.htm#information\end{comment}
\begin{table}[h]
\centering\small
\begin{tabular}{l p{65mm}}
\hline
{\bf variable} & {\bf description} \\
\hline
\var{sex} & Male or Female where 1 is Male and 2 is Female\\
\var{age} & In years \\
\var{height} & In feet and inches where, for example, 5' 5" is listed as 505 \\
\var{weight} & In pounds\\
\end{tabular}
\caption{Variables of interest and their descriptions  from the \data{BRFSS} data set.}
\label{brfssBMIVariables}
\end{table}

BMI is not one of the variables listed, but BMI can be calculated from a person's height and weight. The calculation of BMI using both Metric and Imperial is \[BMI=\frac{\mathrm{weight_{kg}}}{\mathrm{height_{m}}^2}=\frac{\mathrm{weight_{lb}}}{\mathrm{height_{in}}^2}\cdot 703\]
where $\mathrm{weight_{kg}}$ and $\mathrm{height_{m}}$ are the weight and height measured in kilograms and meters respectively, and $\mathrm{weight_{lb}}$ and $\mathrm{height_{in}}$ are the weight and height in pounds and inches respectively. 

The CDC infers the average BMI of adults in the United States, the target population. A target population is the group that the statistician hopes to understand. Through inference, the statistician draws conclusions about the target population. 

The \data{BRFSS} dataset contains170,000 observations.  A simple random sample of 40 adults from the \data{BRFSS} data is taken and used by the CDC as the observed sample. \footnote{The CDC, after collecting the data, edited, processed, and weighted the raw data to make the values appear like they were drawn from a simple random sample of the U.S. adult population. Noncoverage and nonresponse become equal among all groupings of the population. The \data{BRFSS} data are this post-processed data. A simple random sample from \data{BRFSS} allows all observations an equal chance of being selected. The weighting formulae can be found at \url{http://www.cdc.gov/brfss/annual_data/2010/pdf/overview_10.pdf} under the Data Processing section.} This simple random sample of 40 adults from \data{BRFSS} (\data{BRFSS BMI}) will be used to draw conclusions about the target population, U.S. adults. Part of this dataset, including the BMI calculation, is shown in Table~\ref{brfssBMIData}. 

% latex table generated in R 3.1.1 by xtable 1.7-4 package
% Fri Oct  9 05:04:04 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & sex & age & height & weight & BMI \\ 
  \hline
1 &   2 &  60 & 508 & 200 & 30.41 \\ 
  2 &   2 &  25 & 506 & 145 & 23.40 \\ 
  3 &   1 &  40 & 511 & 180 & 25.10 \\ 
  4 &   1 &  53 & 511 & 210 & 29.29 \\ 
  5 &   2 &  80 & 504 & 170 & 29.18 \\ 
  6 &   2 &  71 & 501 & 108 & 20.40 \\ 
   \hline
\end{tabular}
\caption{Six observations from the \data{BRFSS BMI} dataset} 
\label{brfssBMIData}
\end{table}

Strictly speaking, \data{BRFSS} is the target population when using the \data{BRFSS BMI} sample because the 40 observations were not a simple random sample from the U.S. adult population but a simple random sample from \data{BRFSS} instead. However if the \data{BRFSS} dataset is a reasonable surrogate for the U.S. population, \footnote{We believe so.} then drawing samples from \data{BRFSS}'s 170,000 observations is equivalent to directly drawing from the U.S. adult population. This assumption is made in throughout the chapter going forward. In practice, the CDC would simply use all 170,000 observations as a single sample to make inference on the U.S. adult population.

Observing a random sample and drawing conclusions about the target population is the practice of statistical inference in the broadest sense, but the data should be explored graphically using the tools from Chapter~\ref{introductionToData} first. The histogram and box plot in Figure~\ref{exploreBMI} show that the sample is skewed right. The mean BMI of the sample is higher than the median of the sample. Foundations and methods of statistical inference follow naturally after the data have been examined and explored. 

\begin{figure}
\centering
\includegraphics[width =  \textwidth]{ch_inference_foundations_oi_biostat/figures/brfssBMIsampHistograms/brfssBMIsampHistograms}
\caption{Histogram and boxplot of \var{BMI} for the \data{BRFSS BMI} data. The data is skewed right.}
\label{exploreBMI}
\end{figure}

%__________________
\section{Mechanics of inference}
\label{mechanicsofInference}

Chapter~\ref{foundationsForInference} introduces two forms of estimation for the population average: a point estimate and a confidence interval. A point estimate is a single number that best estimates the average adult BMI in the United States. A confidence interval is a range of values that best captures the average U.S. adult BMI at some confidence level. The confidence interval does not guarantee the inclusion of the population parameter. 

\data{BRFSS BMI}, a sample of 40 observations, has a sample mean of 26.53, and a sample standard deviation of 5.84. The point estimate for the population average is 26.53, the sample mean, $\overline{\mathrm{BMI}}$. With 95\% confidence, the CDC calculates the confidence interval as
\begin{eqnarray}
\text{point estimate}\ \pm\ 2 \times SE \\
\overline{\mathrm{BMI}}\ \pm\ 2 \times \frac{s}{\sqrt{n}}\\
26.53\ \pm\ 2 \times \frac{5.84}{\sqrt{40}}\\
(24.68, 28.38)
\label{95PercentConfidenceIntervalFormula}
\end{eqnarray}

where $s$ is the sample standard deviation, $n$ is the number of observations within the sample and $\overline{\mathrm{BMI}}$ is the point estimate. 

The calculation of these estimates is straightforward and mechanical. The steps involved in hypothesis testing are equivalently formulaic. Understanding what these estimates represent, why they are calculated as they are, and how they relate to one another is even more foundational. 

Section~\ref{variabilityInEstimates} introduces the point estimate and its particular flaw: ignoring point estimate variability. Sampling variability is difficult to observe from a single observation, but Section~\ref{pointEstimates} and Section~\ref{sampDist} illustrate through \textsf{R} and repeated sampling that such variation exists. 

Section~\ref{confidenceIntervals} presents the confidence interval to incorporate variability and dissects the formula \[\text{point estimate}\ \pm\ 2 \times SE\] to fully understand each of its terms. These opening sections will explore the theory and motivate point estimates and confidence intervals empirically through \textsf{R}. These estimation techniques are fundamental in mastering hypothesis testing, presented in Section~\ref{hypothesisTesting}.

%__________________
\section{Variability in estimates}
\label{variabilityInEstimates}

\index{point estimate|(}

If the CDC, after observing the \data{BRFSS BMI} sample, were asked to give its best guess for the average adult BMI in the U.S., what would it be? The CDC would provide a point estimate. A \term{point estimate} is a single value derived from sample data that serves as the ``best guess'' for that population parameter. Section~\ref{variabilityInEstimates} looks at point estimates and the variability inherent in using a single number as the best guess.\footnote{This chapter begins with inference on the population mean but questions regarding variation are often just as important. For instance, potential action regarding obesity could change if the standard deviation of the average adult BMI were 5 versus if it were 15.} 

\subsection{Point estimates}
\label{pointEstimates}

A likely choice to estimate the \term{population mean} is to take the \term{sample mean}. That is, use the average BMI in the \data{BRFSS BMI} sample as the estimate for the BMI among U.S. adults. 

For notation, use $\mathrm{BMI}_1, \mathrm{BMI}_2, \ldots, \mathrm{BMI}_{40}$  to represent the BMI for each survey respondent in the \data{BRFSS BMI} sample. The sample mean of \data{BRFSS BMI} is 
\begin{eqnarray*}
\overline{\mathrm{BMI}} = \frac{30.41 + 23.40 + 25.10 + \dots}{40} = 26.53
\end{eqnarray*}
\index{point estimate!single mean|(}
$\overline{\mathrm{BMI}}=26.53$ serves as the \term{point estimate} of the population mean. \footnote{If \var{weight} is the variable of interest instead, the sample mean of observations denoted $w_1,\ldots w_{40}$ would be $\overline{w}$}

What about generating point estimates of other \term{population parameters}, such as the population median or population standard deviation? Sample statistics can estimate these parameters as well. The population standard deviation for adult BMIs can be estimated using the sample standard deviation. The population median is estimated by the sample median. Table~\ref{BMIEstimates} provides the point estimates to other BMI population parameters using the \data{BRFSS BMI} sample.

% latex table generated in R 3.1.1 by xtable 1.7-4 package
% Fri Oct  9 05:32:46 2015
\begin{table}[ht]
\centering
\begin{tabular}{lr}
  \hline
BMI & estimates \\ 
  \hline
mean & 26.53 \\ 
  median & 25.93 \\ 
  std. dev. & 5.84 \\ 
   \hline
\end{tabular}
\caption{Point estimates for the \var{BMI} variable} 
\label{BMIEstimates}
\end{table}

\begin{exercise} \label{pointEstimateOfDesiredWeights}
The CDC wants to estimate the difference in the average age for U.S. adult men and women. If $\overline{\mathrm{age}}_{\mathrm{women}} = 50.22 $ years and $\overline{\mathrm{age}} _ {\mathrm{men}} = 47 $ years, what would be a reasonable point estimate for the age difference in the population? \footnote{Take the difference of the two sample means: $\overline{\mathrm{age}}_{\mathrm{women}} - \overline{\mathrm{age}} _ {\mathrm{men}} = 50.22 - 47 =  3.22$. Women are, on average, estimated to be older than men by 3.22 years.}
\end{exercise}
%men = brfss.sample[which(brfss.sample$sex == 1),]
%women = brfss.sample[which(brfss.sample$sex == 2),]
%mean(women$age) - mean(men$age)

\begin{exercise}
Provide a point estimate for the population IQR given the \data{BRFSS BMI} sample. \footnote{Use the IQR of a random sample from the population as the point estimate for the population IQR.}
\index{point estimate!single mean|)}

\end{exercise}

Suppose a different sample of 40 people is taken from \data{BRFSS} and the mean calculated; the answer will likely not be the same as $26.53$, the \data{BRFSS BMI} sample mean. There exists \term{sampling variation} within a point estimate. Estimates generally vary from one sample to another even among samples of the same size. The sample mean is the best guess for the population average, but it is most likely not equal to the population parameter. Low sampling variation can suggest that the estimate observed may be close to the population parameter, but as the population parameter is rarely ever known, there is no guarantee that the estimate is close. A larger sample size can, with high probability, produce a closer estimate to the population parameter than one taken from a smaller sized sample. 

\textsf{R} can be used to demonstrate sampling variation. Another simple random sample from the \data{BRFSS} data of 40 is taken. The new sample mean for the BMI could be 28.17. Doing this again, the average BMI could be 24.89. Estimates differ across samples through sampling variation, but in practice, it is extremely rare to observe more than one sample from a population.

A \term{running mean} -- a sequence of means where each mean uses one more observation in its calculation than the mean directly before it in the sequence -- demonstrates sampling variation and increasing precision for larger sample sizes. With \data{BRFSS BMI}, the second mean is the average of the first two observations, $\mathrm{BMI}_1, \mathrm{BMI}_2$. The third number in the sequence is the average of $\mathrm{BMI}_1, \mathrm{BMI}_2,$ and $\mathrm{BMI}_3$. 

The running mean from the \data{BRFSS BMI} sample is shown in Figure~\ref{BMIRunningMean}. As more observations are included, the running mean converges closer to 26.53, the sample mean. Similarly if the sample size increases from 40 to 100 observations, the sample mean from the larger sample should be closer to the average U.S. adult BMI than the sample mean with 40 observations. 

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/brfssBMIRunningMean/brfssBMIRunningMean}
   \caption{The running mean calculated from the \var{BRFSS BMI} sample of 40 observations. The mean stabilizes and approaches $\overline{\mathrm{BMI}} = 26.53$ as the number of observations increases to 40.}
      \label{BMIRunningMean}
\end{figure}

Sampling variation is across samples of the same size. Figure~\ref{runningSamplingVariation} displays the running means of two samples randomly drawn from \data{BRFSS} of size 40. There exists a substantial amount of sampling variation; the path of the running means are not the same at a small sample size.

Sampling variation decreases as the number of observations increases. Figure~\ref{runningSamplingVariation20} illustrates this more clearly by calculating the running means of 20 independent samples of 40 observations. The sampling variation is large with small sample sizes $n$, but as the number of observations increases toward 40, the variation among the running means decreases. 

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/brfssBMISampVar/brfssBMISampVar}
   \caption{The running means of two samples drawn from \data{BRFSS}. The gray line is the running mean from \data{BRFSS BMI}. The blue line is the running mean from a different sample drawn randomly from \data{BRFSS}. The exists high sampling variation at the beginning but as the sample size increases, the running means converge.}
	\label{runningSamplingVariation}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/brfssBMISampVar/brfssBMISampVar20}
   \caption{The sampling variation among the 20 running means decreases as the number of observations gets larger.}
	\label{runningSamplingVariation20}
\end{figure}

Sampling variation at different sample sizes also serves as evidence. Figure~\ref{sampleMeanPrecision} shows two histograms of sample means for samples taken from \data{BRFSS}. The sample means in the left histogram and right histogram have sample sizes of five and 40 respectively. The histogram with $n=40$ has noticeably smaller variance than the histogram with $n=5$. Section~\ref{seOfTheMean} demonstrates how to quantify and measure sampling variation as more data becomes available. 

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/brfssBMISampleMeanPrecision/brfssBMISampleMeanPrecision}
   \caption{Sample means of size $n=5$ and $n=40$. The histogram with $n=40$ has a smaller variance.}
   \label{sampleMeanPrecision}
\end{figure}

Beyond spread, the histogram with $n=5$ is also slightly skewed, but both appear to be centered approximately around the same value. Another characteristic of the sample mean is that it does not contain any systematic error or bias. The center of the running means in Figure~\ref{runningSamplingVariation20} and the histograms in Figure~\ref{sampleMeanPrecision} are, on average, the population average BMI. Section~\ref{aFrameworkForInference} discusses other unbiased point estimates beyond the sample mean used for statistical inference.
 
\subsection{Sampling variability and sampling distribution}
\label{sampDist}

\data{BRFSS BMI} is one random sample from \data{BRFSS} with one sample mean, $26.53$. Another random sample of 40 was taken from \data{BRFSS} in Section~\ref{pointEstimates} and its mean calculated, $28.17$. This happens again (24.89) and again (26.34), and continue to do this many many times. \footnote{These values were calculated from resampling the \data{BRFSS} data in \textsf{R}. These particular numbers are some of many possible values a sample mean can take on, depending on the sample itself.} Repeated sampling is only possible with access to  the\data{BRFSS} data. This procedure generates an approximation to the \term{sampling distribution} for the sample mean of sample size 40, shown in Figure~\ref{brfssBMISamplingDistribution}.\footnote{The sampling distribution is constructed by repeated sampling from the target population. While \data{BRFSS} is not quite the target population of U.S. adults, the 170,000 observations are a large enough sample to be a representative substitute for the target population.} 

\begin{termBox}{\tBoxTitle{Sampling distribution}
The \term{sampling distribution} is the distribution of a point estimate based on samples of a fixed size from a certain population. A point estimator has an associated unique sampling distribution. Understanding the concept of a sampling distribution is central to understanding variability and statistical inference.}
\end{termBox}

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/brfssBMISamplingDistribution/brfssBMISamplingDistribution}
   \caption{A histogram of an approximation to the sampling distribution. Here 100,000 sample means for BMI were calculated, where the samples are of size $n=40$. }
      \label{brfssBMISamplingDistribution}
\end{figure}

Figure~\ref{brfssBMISamplingDistribution} is an approximation of the sampling distribution. A point estimate from a particular sample of said size is one observation in the sampling distribution. In almost all cases, the sample distribution is unobservable beyond the single point estimate from the sample the scientists observe. \footnote{To get an exact sampling distribution, the sample means from every possible unique combination of 40 respondents in the U.S. adult population (and not from the \data{BRFSS} dataset) needs to be calculated. The CDC would never create a sampling distribution and instead would use all 170,000 observations to calculate one sample mean.}The sampling distribution, however, is a fundamental concept that illustrates sampling variability. 
 
An approximation of the sampling distribution of the sampling mean can be created using \data{BRFSS} and sampling variation illustrated with the following pseudocode: 

\begin{verbatim}
(1) Create a vector to store the sample mean values once calculated
(2) Take a sample of 40 from BRFSS
(3) Calculate the sample mean of BMI values and store the sample mean in (1)
(4) Repeat (2) and (3) many many times 
(5) Plot the vector of sample means as a histogram 
\end{verbatim}
  
The sampling distribution for the sample mean is unimodal and symmetric, centered at the target population's mean. The sample mean is, intuitively, the best guess for the population mean. The sample means should tend to ``fall around'' the mean of the target population.

\subsection{Standard error of the mean}
\label{seOfTheMean}

Variability exists among point estimates. The average adult BMI ranged from 24 to 29 in Figure~\ref{brfssBMISamplingDistribution} for samples of size 40. Variability decreases for larger sample sizes. Section~\ref{pointEstimates} suggests that there should exist some metric, dependent on sample size, to quantify the variability of a point estimate.

\begin{tipBox}{\tipBoxTitle{More data means less variability}
In sampling, the larger the sample size the better. The precision of the sample mean increases as more data is observed within a sample.}
\end{tipBox}

Standard deviation is an obvious method to quantify variability. The standard deviation of the sample mean indicates how far the typical estimate is away from the population mean. IStandard deviation is a very good metric to size the typical \term{error} of the point estimate, and for this reason, an estimate's standard deviation is called the \term{standard error (SE)} \index{SE}\marginpar[\raggedright\vspace{-4mm} $SE$\\\footnotesize standard\\error]{\raggedright\vspace{-4mm} $SE$\\\footnotesize standard\\error} of the estimate. 

\begin{termBox}{\tBoxTitle{Standard error of an estimate}
The standard deviation associated with an estimate is called the \emph{standard error}. It describes the typical error or uncertainty associated with the estimate.}
\end{termBox}

The standard error of the estimate is a measure of spread among the possible estimates, which are drawn from the sampling distribution. The standard deviation of the sampling distribution, denoted  $\sigma_{\overline{x}}$, serves as a reasonable measure of a point estimate's variability.

\begin{tipBox}{\tipBoxTitle{"Standard Deviation" $\neq$ "Standard Error"}
Caution: The standard deviation of the sample mean is not equivalent to the estimate for the population standard deviation. The term "standard error" is not interchangeable with "standard deviation." Standard deviation describes the spread of values within one sample. The standard error of the sample mean describes how accurate the sample mean is to the population mean. It represents the spread among all sample means. These two terms are measuring two separate quantities.}
\end{tipBox}

\begin{exercise}
(a) Which one achieves a "better" estimate of a parameter: using a small sample or a large sample? Why? (b) Using the same reasoning from (a), is a point estimate based on a small sample expected to have smaller or larger standard error than a point estimate based on a larger sample?

\footnote{(a) Prefer a large sample. Consider two random samples: one of size 10 and one of size 1000. Individual observations in the small sample are highly influential on the estimate while in larger samples these individual observations would more often "average each other out." The larger sample would tend to provide a more precise estimate of the parameter. (b) A "better" estimate typically means it has less error. Based on (a), intuition suggests that a larger sample size corresponds to a smaller standard error. In general the standard error, or variability, for an estimate gets smaller as the samples get larger.}
\end{exercise}

The standard deviation of the sampling distribution is difficult to compute since the sampling distribution is unobservable. Through repeated sampling, an approximation of the sampling distribution serves as a substitute. The standard deviation of the calculated sample means from Section~\ref{sampDist} pseudocode serves as the estimate for the standard error. The algorithm in \textsf{R} is below:\footnote{This computing experience samples without replacement to simulate sampling in the real world (researchers would never sample the same person twice), however standard errors would never be calculated through simulation. Theory, instead, requires individual BMI values to be independent. A reliable method to ensure independence is to use the 10\% rule of thumb: observations from simple random samples with a size less than 10\% of the population are considered independent. Sampling without replacement within a finite population still results in a reasonable estimate for the standard error.}

\begin{verbatim}
N <-10000 #the number of iterations to do
sample.means<- array(data=NA,dim = N) #to store the sample means
for(i in 1:N){
  #take a sample of size 40 from the BRFSS dataset without replacement
  #BRFSS is the dataset and $BMI only calls the "BMI" column from the BRFSS dataset
  sample<-sample(x=BRFSS$BMI, size=40, replace=FALSE) 
  
  #calculate the mean of the sample and store it 
  sample.means[i]<-mean(sample)
}

#standard deviation of the approximation to the sampling distribution
#serves as the estimate of the standard error
sd(sample.means) 
\end{verbatim}

The estimated standard error, \data{sd(sample.means)}, of a simulation with 10,000 sample means is $0.83$. This method, however, has one underlying problem: most often, scientists observe only one sample mean, $\overline{x}$. They cannot repeatedly sample. What do they do? The standard error of the sample mean can be calculated from a single sample with the following equation:

\begin{termBox}{\tBoxTitle{Calculating SE for the sample mean}
Given $n$ independent observations from a population with standard deviation $\sigma$, the standard error of the sample mean is equal to \vspace{-1mm}
\begin{eqnarray}
SE_{\text{ sample mean}} = \frac{\sigma}{\sqrt{n}}
\label{seOfXBar}
\end{eqnarray}\vspace{-3mm}%

A reliable method to ensure sample observations are independent is to guarantee that the sample from the population is a simple random sample with a size that is less than 10\% of the population size.\index{standard error!single mean}
}
\end{termBox}

There is one subtle issue of Equation~(\ref{seOfXBar}): the population standard deviation is typically unknown. Section \ref{pointEstimates} suggests that the``best guess'' for the population standard deviation, the sample standard deviation, can be used as a substitute. This estimate tends to be sufficiently good when the sample size is at least 30 and the population distribution is not strongly skewed. Practitioners then replace $s$ for $\sigma$ in Equation~\ref{seOfXBar}. When the sample size is smaller than 30 or the skew condition is not met, a larger sample or other methods need to be used. These topics are further discussed in Section~\ref{cltSection}. 

The standard error of the sample mean from observing \data{BRFSS BMI} is 

\begin{eqnarray*}
SE_{\overline{x}} = \frac{s}{\sqrt{n}} = \frac{5.84}{\sqrt{40}} =  0.92
\end{eqnarray*}
where $s$ is the standard deviation of the sample and $n$ is the number of observations in the sample. The calculated standard error  $(0.92)$ is similar to the empirical standard deviation that was calculated from the sampling distribution $(0.83)$ \footnote{For context, the sample taken in Section~\ref{pointEstimates} with sample mean 28.17 results in a standard error of $8.85/\sqrt{40} =1.40$. Variability also exists within sample standard deviations.}. 

\begin{exercise}
In another sample of 40 U.S. adults, the standard deviation of the sample is $s_\mathrm{BMI} = 4.81$. The sample is a simple random sample of less than 10\% of the United States population. The observations are independent. (a)~What is the standard error of the sample mean, $\overline{BMI}$? (b)~Is it surprising if the CDC published a report with the average BMI of all U.S. adults as 30 after observing a sample mean of 26.36? How about 25?

\footnote{(a) Use Equation~(\ref{seOfXBar}) with the sample standard deviation to compute the standard error: $SE_{\overline{\mathrm{BMI}}} = 4.81/\sqrt{40} =  0.76$. (b) It is surprising if the average adult BMI of the U.S. population was 30. A BMI of 30 is many many standard deviations away ($SE=0.76$) from the sample mean of 26.36. 
A BMI of 25 is less than one standard deviation away from the estimated population mean. The report would be less surprising. }
\end{exercise}

\begin{exercise}
(a) Are the results of a sample of 100 or 400 observations more trusting? (b) If the standard deviation of the individual observations is 10, what is the estimate of the standard error when the sample size is 100? What about when it is 400? (c) Explain how your answer to (b) mathematically justifies your intuition in part~(a).

\footnote{(a) Look back to Section~\ref{pointEstimates} on sampling variation and more data. Extra observations are helpful in understanding the population, so a point estimate with 400 observations seems more trustworthy. (b) The standard error when the sample size is 100 is given by $SE_{\bar{x}_100} = 10/\sqrt{100} = 1$. For 400: $SE_{\bar{x}_400} = 10/\sqrt{400} = 0.5$. The larger sample has a smaller standard error if the sample standard deviations were the same. Estimates tend to be more precise when the sample size is larger. (c) The standard error, the typical error between the estimate and the population parameter, of a sample with 400 observations is lower than for a sample with 100 observations with equal sample standard deviations. The math in parts (b) shows that estimates from a larger sample tend to have smaller standard errors, though it does not guarantee that every large sample will provide a better estimate than a particular small sample.} 
\end{exercise}

\subsection{Basic properties of point estimates}

This section achieved three goals. First, point estimates from a sample are single numbers used to estimate population parameters. Furthermore, there exists sampling variation for point estimates. Because researchers only have the capacity to observe a single sample, it is not obvious that point estimates contain variability. The sampling distribution, a theoretical and fundamental concept, justifies the existence of sampling variation through its many possible sample estimates. Lastly, standard error quantifies sampling variation, which can be mathematically calculated from Equation~\eqref{seOfXBar}. Point estimates and their standard errors can be quantified for other parameters  -- the median, standard deviation, or any other number of statistics. These extensions will be postponed until later chapters and courses.

\index{point estimate)}

%__________________
\section{Confidence intervals}
\label{confidenceIntervals}

\index{confidence interval|(}

\subsection{Capturing the population parameter}

Using a point estimate is like fishing with a spear in a murky lake. Fishermen with spears will likely miss. Using a net in the murky lake is like using a confidence interval; fishermen are more likely to catch a fish with a net. 

A point estimate, a single value, rarely estimates the population parameter exactly; usually there is error in the estimate. The standard error conveys the magnitude of the sampling variation. A single point estimate alone does not. Is there an estimation technique that comprises a point estimate and its sampling variation?

Reporting a range of plausible values  -- \term{a confidence interval} -- increases the likelihood of capturing or containing the population parameter. The width of the interval incorporates sampling variation. A larger interval generally indicates a large sampling variation and standard error \footnote{It could also indicate the higher confidence level. This will be introduced later}. As with fishing, the goal of the confidence interval is to catch the population parameter within the confidence interval range.

\begin{exercise}
Should a wider or narrower interval be used to be certain that the population parameter is captured? \footnote{A fisherman uses wider net to be more certain of capturing fish. Likewise, a researcher use a wider confidence interval to be more certain of capturing a population parameter. Containing the population parameter in the interval, however, is not the only goal when constructing a confidence interval. The widest interval $(-\infty,\infty)$ guarantees that the parameter is included, but this range does not provide any insight into the value of the parameter.} 
\label{CIwidth}
\end{exercise}
\begin{exercise}
Researchers are 50\% confidence that a range 10 units wide encompasses the population parameter. Another interval, centered at the same value, was five units wide instead. Are researchers more or less than 50\% confidence that this new confidence interval will include the population parameter? \footnote{They are less than 50\% confidence that the smaller interval will include the population parameter.}
\end{exercise}

\subsection{Confidence levels}
\label{confidenceLevels}

Exercise~\ref{CIwidth} demonstrates that the size of a confidence interval varies with levels of certainty of capturing the parameter. If researchers want to guarantee the parameter is included, the confidence interval is $(-\infty,\infty)$. If researchers are not very certain that the confidence interval contains the parameter, the interval is very narrow. In these cases, ``certainty'' is defined by a confidence level.

Before constructing a confidence interval, scientists must choose a confidence level. In many instances, a 95\% confidence level is used \footnote{Section~\ref{significanceLevel} demonstrates that any confidence level can be used.} but what does ``95\% confident'' mean? 

Suppose many samples were independently drawn and 95\% confidence intervals were built around each sample. At a 95\% confidence level, approximately 95\% of those intervals would contain the population parameter, $\mu$. If the CDC were table to take 100 independent samples and built 100 confidence intervals at the 95\% confidence level for adult U.S. BMI, 95 of these intervals would contain the average population BMI. Five of these would not.

Figure~\ref{95PercentConfidenceInterval} shows that among 25 samples randomly sampled, 24 of the resulting 95\% confidence intervals contain the average U.S. population BMI and one does not. In most instances, researchers only observe a single sample and confidence interval. They do not know where $\mu$ lies, and they do not know whether their confidence interval contains $\mu$ or not. 

\begin{figure}[hht]
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/95PercentConfidenceInterval/95PercentConfidenceInterval}
   \caption{For each of the 25 samples, a 95\% confidence interval was created. Only~1 of these~25 intervals did not capture the average adult BMI for the U.S. population.}
   \label{95PercentConfidenceInterval}
\end{figure}

\begin{example}{Consider extreme confidence levels. What are the implications of a 100\% confidence interval? How about a 0.001\% confidence level?} \label{extremeConfidenceLevels}
The 100\% confidence interval created will \emph{always} capture the population parameter. If Figure~\ref{95PercentConfidenceInterval} were recreated for 100\% confidence intervals, all intervals would be colored in blue. To guarantee this, the confidence interval will be $(-\infty, \infty)$. Confidence intervals at an extremely low confidence level (0.001\%) will, many times, not include the population parameter and be highlighted red. These intervals are expected to be extremely narrow.
\end{example}

\subsection{An approximate 95\% confidence interval through computation} \textbf{ahhhhhhhhhh}
\label{95confidence}

A confidence interval can be completely defined by two attributes: its center and its width. The interval is centered at the point estimate -- the best guess for where the population parameter is. Its width depends on the confidence level and the sampling variation of the point estimate. 

Scientists use Equation~\ref{95PercentConfidenceIntervalFormula}, introduced in Section~\ref{mechanicsofInference}, to be roughly \term{95\% confident} that the confidence interval has captured the population parameter:
\begin{eqnarray}
\text{point estimate}\ \pm\ 2 \times SE 
\label{95PercentConfidenceIntervalFormula}
\end{eqnarray}

The equation's components can be broken down into three parts: point estimate, standard error, and a confidence level multiplier, the ``2'' in ``$2\times SE$'' -- the confidence interval width. Repeated sampling in \textsf{R} provides insight into how this equation arises. 

 
The confidence interval  "there is a 95\% probability that the population mean is between these two values" -- which is not exactly true 

The sampling distribution provided in 

 Figure~\ref{brfssBMISamplingDistribution} 
 
 should be inspiration on how to find a range of values that intends to capture the population parameter. 
 
 %ahhhhhh
 a population mean in this range could reasonable produce the observed data so that's why we take the middle 95\% stuff
 Example: Landwehr, Swift, and Watkins [14] introduce the idea of confidence intervals without resorting to formulas. Using a fixed sample size, they ask groups of students to draw samples using a specific population proportion, varying this value among the groups. Each group then produces a "90\% boxplot" containing the middle 90\% of their generated proportions. Students then construct a chart which displays the 90\% boxplots for the different values of the population proportion. Given a new sample proportion, students see which of these boxplots overlap with this value; these boxplots indicate which population proportions could reasonably have produced the new sample proportion. Students thus interpret a confidence interval as a set of plausible values for the population parameter based on the observed sample statistic

As the mean of the sampling distribution is the mean of the population, building a 95\% confidence interval around an approximation of the sampling distribution is extremely reasonable. The sampling distribution represents all the possible values for an observed sample mean, so using the middle 95\% of the sampling distribution serves as a reasonable estimation for a 95\% confidence interval. 

Below is the pseudocode that implements this procedure using samples of size 40 \footnote{This highly resembles the pseudocode for approximating a sampling distribution from Section~\ref{sampDist}}
\begin{verbatim}
(1) Create a vector to store the sample mean values once calculated 
(2) Take a sample of 40 from the BRFSS dataset
(3) Calculate the sample mean and store the value in (1)
(4) Repeat (2) and (3) many many times 
(5) Use the middle 95% of values as the 95% confidence interval. 
\end{verbatim}

The 95\% confidence interval through computation is the BMI value  $\mathrm{bmi}_1$ such that 2.5\% of the  values in the vector from Step 1 \footnote{denoted \var{sample.means} from Section~\ref{seOfTheMean}} is below $\mathrm{bmi}_1$ and another BMI value $\mathrm{bmi}_2$ such that 2.5\% of the distribution is greater than $\mathrm{bmi}_2$

Recall from the distributions unit~\ref{distributions}, to find these values, use the \var{quantile()} function in \textsf{R}. Particularly use the \var{quantile()} function on \var{sample.means}, the vector that that stores the sample means. 
\begin{verbatim}
confidence.interval<-quantile(x=sample.means,c(0.025,0.975))
> confidence.interval
    2.5%    97.5% 
24.79 	28.08
\end{verbatim}
The interval (24.79,28.08) is an estimation of a 95\% confidence interval using the sampling distribution of sample means. Scientists at the CDC are 95\% confident that the adult population mean BMI is between 24.79 and 28.08. Similarly if they calculated many confidence intervals from many different observed samples, 95\% of the confidence intervals that were calculated capture the population mean. 

\begin{exercise}
The CDC is interested in creating a 90\% confidence interval and a 50\% confidence interval. (a) how do the widths of the confidence intervals compare? (b) How would the \var{quantile()} function be used to find the 90\% and 50\% confidence intervals from using the vector \var{sample.means}?
 \footnote{(a) The 50\% confidence interval has the smaller width. In general the more confidence people want, the larger the confidence interval width will be.(b) Remember to grab the middle percent of observed sample means. The \textsf{R} code for calculating a 90\% confidence interval is \var{quantile(x=sample.means,c(0.05,0.95))}. For a 50\% confidence interval, the code would be \var{quantile(x=sample.means,c(0.25,0.75))}}
\end{exercise}

However, the CDC would not calculate a 95\% confidence interval through repeated sampling in \textsf{R}. The CDC does not have access to the entire U.S. population, and it cannot resample the U.S. population independently 100,000 times. Even more, it can never observe the sampling distribution. Yet, this procedure is useful to capture the essence of confidence intervals: incorporating variability. 

%#################

\subsection{Calculating an approximate 95\% confidence interval}
\label{calculate95confidence}

The CDC cannot estimate a 95\% confidence interval using the method given in Section~\ref{95confidence}. It only observes a single sample, \data{BRFSS BMI} and calculates only one sample mean. The CDC uses Equation~\ref{95PercentConfidenceIntervalFormula} instead. 

Equation~\ref{95PercentConfidenceIntervalFormula} is constructed such that its center is the sample mean. Its width allows for the confidence interval to incorporate sampling variation, a feature that a point estimate cannot offer. Section~\ref{95confidence} demonstrates that a sampling distribution with high variance results in a confidence interval with a large width. A confidence interval derived from a sampling distribution with small variability has a narrow width. The confidence interval width encompasses randomness associated with sampling variability. Recall from Section~\ref{seOfTheMean} the definition standard error. Standard error is a natural measurement of uncertainty among sample means, and is one of two factors that determines the confidence interval width. 

The confidence level is the other factor that defines a confidence interval. Roughly 95\% of the time, the point estimate is within approximately two standard errors\footnote{1.96 to be more precise if the sampling distribution resembles a Normal Distribution. Details coming up in Section~\ref{sampdistmean}} of the population mean. A 95\% confidence level produces a confidence interval that is two standard errors wide on each side of the point estimate. 

These three parts -- point estimate, confidence level and standard error --  allow scientists to incorporate sampling variability in their estimates. Using Equation~\ref{95PercentConfidenceIntervalFormula} and the observed sample, scientists are roughly \term{95\% confident} that the confidence interval has captured the population parameter.

The 95\% confidence interval is calculated from the \data{BRFSS BMI} sample. It has a sample mean of 26.53 and sample standard deviation of 5.84.
\begin{align*}
\text{point estimate}\ &\pm\ 2 \times SE\\
26.53 &\pm 2\times \frac{5.84}{\sqrt{40}}\\
(24.68 &, 28.38)
\end{align*}

Simulations in \textsf{R} generates a confidence interval similar to the theoretical calculation of a confidence interval above. Differences are due to variability in samples. \data{BRFSS BMI} produces a point estimate and standard error different than in \textsf{R}. 

\begin{exercise}
How do the center of the confidence interval from \data{BRFSS BMI} and the center of the confidence interval computed in Section~\ref{95confidence} compare? \footnote{The sampling distribution from \textsf{R} is extremely similar to the true sampling distribution of the sample mean. The center of the confidence interval in \textsf{R} is extremely close to the population mean. The center of the \data{BRFSS BMI} confidence interval is what the CDC believes to be the best guess for the population mean. This center, the sample mean of \data{BRFSS BMI} is most likely not equal to the population parameter due to sampling variation.}
\end{exercise}

\begin{exercise}
Imagine if the CDC observed only the confidence interval in Figure~\ref{95PercentConfidenceInterval}, highlighted in red. Does this imply that the average population BMI cannot be $\mu$? \footnote{No. Some observations occur more than two standard deviations from the mean. Some point estimates will be more than two standard errors from the parameter $\mu$. A confidence interval only provides a plausible range of values for a parameter up to some confidence level. Observing the red confidence interval suggests that the average BMI is implausibly $\mu$ but this does not mean it is absolutely impossible. In fact, if $\mu$ was known, as it is in Figure~\ref{95PercentConfidenceInterval}, the red confidence interval was the "unlucky" one to not contain the average BMI $\mu$.}
\end{exercise}

The statement "about 95\% of observations are within two standard deviations of the mean" is only approximately true. This rule of thumb holds very well if the observations are distributed normally. As Section~\ref{cltSection} soon shows, the observations -- the sample means -- tend to be normally distributed when the sample size is sufficiently large. 

\begin{example}{The CDC is interested in how the average heights of men and women compare. The researchers create separate 
95\% confidence intervals for the average male height and the average female height. With 40 men and 40 women sampled from \data{BRFSS}, the average male height is 67.05 inches and the average female height is 66.50 inches. The sample standard deviations for males and females heights are 3.82 and 4.14 respectively. What are the 95\% confidence intervals for the average male and female height in the U.S.? }
\label{CIforGenderHeight}
Both 95\% confidence intervals are calculated using the formula \[\text{point estimate}\ \pm\ 2 \times SE\] and the information given above: 
\begin{align*}
\text{men: }\overline{\mathrm{height}_\mathrm{men}} &\pm\ 2 \times SE\\
67.05 &\pm 2\times \frac{3.82}{\sqrt{40}}\\
(65.84 &, 68.26)
\end{align*}
\begin{align*}
\text{women: }\overline{\mathrm{height}_\mathrm{men}}\ &\pm\ 2 \times SE\\
66.50 &\pm 2\times \frac{4.14}{\sqrt{40}}\\
(65.19 &, 67.81)
\end{align*}
The confidence intervals suggest that the average heights by gender are slightly different, but the CDC cannot formally conclude this. Chapter~\ref{inferenceForNumericalData} will introduce how to compare the average heights of men and female directly. 
\end{example}

\begin{exercise} \label{95CIExerciseForBRFSSAge}
The average age of adults in \data{BRFSS BMI} is 48.85 years with a standard error of 2.95 years (estimated using the sample standard deviation, 18.69). What is an approximate 95\% confidence interval for the average age of U.S. adults?\footnote{Apply Equation~(\ref{95PercentConfidenceIntervalFormula}): $48.85 \ \pm \ 2\times 2.95 \rightarrow (42.95, 54.75)$. The interpretation of the interval is, "We are about 95\% confident the average age of U.S. adults is between 42.95 and 54.75 years." The average age of all adults in \data{BRFSS} is 46.72 which is indeed within the confidence interval just calculated (Normally scientists do not have this luxury!).}
\end{exercise}

\subsection{The sample size for a sampling distribution}
\label{sampdistmean}

The sampling distribution of $\overline{\mathrm{BMI}}$ for $n=5$ in Figure~\ref{sampleMeanPrecision} is slightly skewed. The sampling distribution is more symmetric for $n=40$. Figure~\ref{sampDistNormal} is the sampling distribution of size $n=40$ from Figure~\ref{sampleMeanPrecision} but with a normal probability plot of those sample means. 

\begin{figure}[hht]
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/sampDistNormal/sampDistNormal}
   \caption{The left panel shows the histogram of the sample means for 100,000 different random samples of size $n=40$. The right panel shows a normal probability plot of those sample means.}
   \label{sampDistNormal}
\end{figure}

Does this sampling distribution resemble a familiar probability distribution (think back to Chapter~\ref{modeling})? Hopefully so! The sampling distribution of sample means closely resembles the normal distribution (see Section~\ref{normalDist}). The right panel of  Figure~\ref{sampDistNormal}, a normal probability plot, evaluates if the sample means are approximately normally distributed. The points closely fall along a straight line, indicating the distribution of sample means is nearly normal. The Central Limit Theorem explains this result. \footnote{A more formal definition coming soon in~\ref{cltSection}}

\begin{termBox}{\tBoxTitle{Central Limit Theorem, informal description}
If a sample consists of at least 30 independent observations and the data are not strongly skewed, then the distribution of the sample mean is well approximated by a~normal model.\index{Central Limit Theorem}}
\end{termBox}


\subsubsection{Why 30?}
\label{why30}

This text offers 30 as the sample size for the Central Limit Theorem to apply but this rule of thumb varies from book to book. Can could this rule of thumb be tested using the \data{BRFSS} data in \textsf{R}? Is 30 a sufficient sample size to approximate the sampling distribution of the sample mean to a normal model? 

Values below 30 should be evaluated as alternative sample sizes. The sampling distribution should be compared to a normal distribution for fit. Overlaying a normal approximation onto the sampling distribution histogram in \textsf{R} is an effective way to make such a comparison.  \footnote{Reuse the code that creates a sampling distribution but vary the sample size. TO overlay a normal distribution, use the code:\\
\texttt{hist(sample.means, freq=FALSE ) \#histogram of the sampling distribution \\
curve(dnorm(x,mean=mean(sample.means), sd=sqrt(var(sample.means))), add = TRUE) \# overlay a normal distribution curve}\\
The function \texttt{curve()} draws a curve on an existing plot (\texttt{hist(sample.means)}) if the parameter \texttt{add} is \texttt{TRUE}. The parameters of the normal distribution are \texttt{mean(sample.means)} and \texttt{var(sample.means)} for the mean and the variance respectively.}

Figure~\ref{cltThirty} displays the sampling distributions of the sample mean for sample sizes of 5, 15, and 30. The overlaying curve on each histogram is distributed $\mathcal{N}(\mu, \sigma)$ where $\mu$ is the mean of the sample means and $\sigma$ is the standard deviation of the sample means. \footnote{The center of the normal curve is shifted slightly to align with the center of each histogram bar.}

\begin{figure}[hht] 
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/clt30/clt30}
   \caption{A normal density curve is superimposed on sampling distributions with different sample sizes $n=5, 15, 30$. It begins to be a fitting approximation as $n$ increases towards $30$, confirming the Central Limit Theorem rule of thumb.}
      \label{cltThirty}
\end{figure}

Under the normal model, with a sufficient number of samples ($n\geq 30$), Equation~(\ref{95PercentConfidenceIntervalFormula}) is more precise replacing 1.96 for 2 as the multiplier indicating 95\% confidence.
\begin{eqnarray}
\text{point estimate}\ \pm\ 1.96\times SE
\label{95PercentCIWhenUsingNormalModel}
\end{eqnarray}
If the distribution of the point estimate, such as $\overline{x}$, follows a normal model with standard error $SE$, use Equation~\ref{95PercentCIWhenUsingNormalModel} to create a 95\% confidence interval. 

\subsection{Changing the confidence level}
\label{changingTheConfidenceLevelSection}

\index{confidence interval!confidence level|(}

The confidence level may change beyond 95\% depending on the context and application. Estimating the average BMI for U.S. adults is fairly low risk if the population parameter were evaluated incorrectly. The CDC might opt for a lower confidence level. Consider the U.S. Food and Drug Administration (FDA) studying a drug's effect on children. The FDA might prefer inference at the 99\% confidence level because of the specific context and risks for incorrect estimations. Parameters of more critical problems can call for higher confidence levels when calculating confidence intervals and performing hypothesis tests \footnote{In many cases, these are not the only factors that determine a confidence level. Funding, logistics, and the number of observations that will need to be sampled are all considered when choosing a confidence level}.

Think back to the analogy of catching a fish: if fishermen want to be more sure of catching fish, they use wider nets. To be more than 95\% confident the population parameter is within the confidence interval, the interval must have larger widths. A confidence interval at a lower confidence level would be slimmer if the same sample was used to calculate all of these confidence intervals. 

How can the width change if the sample stays the same? Looking back to the structure of the 95\% confidence interval in Equation~\ref{95PercentCIWhenUsingNormalModel}, the width is defined as the product of a multiplier and the standard error -- $1.96 \times SE$. Figure~\ref{choosingZForCI} demonstrates that 95\% of the area underneath a normal curve lies within 1.96 standard deviations from the center. Because the standard error remains constant under the same sample, the confidence interval width changes by varying the multiplier $1.96$. 

\begin{exercise} \label{leadInForMakingA99PercentCIExercise}
If $X$ is a normally distributed random variable, how often will $X$ be within 2.58 standard deviations of the mean?\footnote{How often is the Z-score larger than -2.58 but less than 2.58 (See Figure~\ref{choosingZForCI} for a visualization.). Look up -2.58 and 2.58 in the normal probability table (0.0049 and 0.9951). There is a $0.9951-0.0049 \approx 0.99$ probability that the unobserved random variable $X$ will be within 2.58 standard deviations of the mean, $\mu$.}
\end{exercise}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/choosingZForCI/choosingZForCI}
\caption{If the confidence level is 99\%, choose $z^{\star}$ such that 99\% of the normal curve is between -$z^{\star}$ and $z^{\star}$, which corresponds to 0.5\% in the lower tail and 0.5\% in the upper tail: $z^{\star}=2.58$.}
\label{choosingZForCI}
\index{confidence interval!confidence level|)}
\end{figure}

To calculate a 99\% confidence interval, the multiplier should instead be $2.58$, corresponding to the Z-score established in Exercise~\ref{leadInForMakingA99PercentCIExercise}. 

This approach -- use the Z-score in the normal model as the multiplier for confidence levels -- is appropriate when $\overline{x}$ is distributed normally with mean $\mu$ and standard deviation $SE_{\overline{x}}$. The formula for a 99\% confidence interval to estimate the population mean is
\begin{eqnarray}
\overline{x}\ \pm\ 2.58\times SE_{\overline{x}}
\label{99PercCIForMean}
\end{eqnarray}

Equation~\ref{99PercCIForMean} does not need to be population mean specific. In fact, it can be generalized for many population parameters \[\text{point estimate} \pm 2.58\times SE\]

The normal approximation is crucial for the precision of these confidence intervals. Section~\ref{cltSection} provides a more detailed discussion for when the normal model can safely be applied under the Central Limit Theorem. When the normal model is not a good fit for the sampling distribution, alternative distributions can be used. Below is a good checklist to determine whether or not the Central Limit Theorem can be informally applied to the distribution of sample mean, and when the Z-score can be used as a multiplier for calculating a confidence interval width.

\begin{termBox}{\tBoxTitle{Conditions for $\overline{x}$ being nearly normal and $SE$ being accurate\label{terBoxOfCondForXBarBeingNearlyNormalAndSEBeingAccurate}}
Important conditions to help ensure the sampling distribution of $\overline{x}$ is nearly normal and the estimate of SE sufficiently accurate:
\begin{itemize}
\setlength{\itemsep}{0mm}
\item The sample observations are independent.
\item The sample size is large: $n\geq30$ is a good rule of thumb.
\item The population distribution is not strongly skewed. (Check this using the distribution of the sample as an estimate of the population distribution.)
\end{itemize}
Additionally, the larger the sample size, the more lenient scientists are with the sample's skew.}
\end{termBox}

These three conditions help ensure that $\overline{x}$ is distributed normally and the standard error is accurate. If the distribution of $\overline{x}$ is nearly normal, choosing a precise multiplier becomes much easier for calculating confidence intervals. More importantly, the representativeness of the sample is imperative in the ability to make inferences on the target population. Randomness, independence and a large sample size safeguard against extreme observations skewing the sample mean. These conditions ensure the ability to accurately infer and generalize to the population of interest.

Verifying independence is often the most difficult of the conditions to check, and the way to check for independence varies from one situation to another. However, randomness is almost always necessary for the independence assumption. 

\begin{tipBox}{\tipBoxTitle{How to verify sample observations are independent}
Observations in a simple random sample consisting of less than 10\% of the population can be considered independent.}
\end{tipBox}

\begin{caution}
{Independence for random processes and experiments}
{If a sample is from a random process or experiment, it is important to verify the observations from the process or subjects in the experiment are nearly independent and maintain their independence throughout the process or experiment. Usually subjects are considered independent if they undergo random assignment in an experiment or are selected randomly for some process.}
\end{caution}

\begin{exercise} \label{find99CIForBRFSSWeightExercise}
Verify assumptions, and create a 99\% confidence interval for the average U.S. male adult weight using the sample of 40 males from Example~\ref{CIforGenderHeight} The point estimate is $\overline{w_{\mathrm{men}}} = 172.65$ pounds and the standard error is $SE = 6.50$ pounds. Refer to Figure~\ref{brfssMenWeight} for guidance on skewness.\footnote{The observations are independent (simple random sample, $<10\%$ of the population), the sample size is at least 30 ($n=40$), and the distribution is slightly skewed (Figure~\ref{brfssMenWeight}); the skewness assumption is weakly satisfied. Conditions are verified, and the normal approximation and estimate of SE should be reasonable. Apply the 99\% confidence interval formula: $\overline{w_{\mathrm{men}}}\ \pm\ 2.58 \times  SE_{\overline{w_{\mathrm{men}}}} \rightarrow (155.87, 189.43)$. There is 99\% confidence that the average weight of all U.S. adult males is between 155.87 and 189.43 pounds.}
\end{exercise}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/brfssMenWeight/brfssMenWeight.pdf}
\caption{A histogram of weights for 40 men sampled from \data{BRFSS}. The weights are slightly skewed, but with a sufficiently large sample size taken from a random sample, the sample mean can still be considered nearly normal.}
\label{brfssMenWeight}
\index{confidence interval!confidence level|)}
\end{figure}

Confidence levels vary by statistician, context, and inference goal. The equation of a confidence interval at any confidence level follows naturally. 

\begin{termBox}{\tBoxTitle{Confidence interval for any confidence level (nearly normal model)}
If the point estimate follows the normal model with standard error $SE$, then a confidence interval to estimate the population parameter is
\begin{eqnarray} 
\text{point estimate}\ \pm\ z^{\star} SE
\end{eqnarray} 
where the value of $z^{\star}$ is the Z-score that corresponds to the confidence level selected. The coefficient on the standard error, $z^{\star}$, is also known as the critical value. Only used $z^{\star}$ when the point estimate resembles a normal model. \footnote{$z^{\star}$ is also used when the population standard deviation is known. This is rarely ever the case in practice, and as such, this text disregards this situation completely.}}
\end{termBox}

\begin{termBox}{\tBoxTitle{Margin of error}
\label{marginOfErrorTermBox}In a confidence interval, $z^{\star}\times SE$ is called the \term{margin of error} and is half the width of the confidence interval.}
\end{termBox}

Figure~\ref{choosingZForCI} demonstrates how to choose $z^{\star}$ for a given confidence level. Select $z^{\star}$ such that the area between -$z^{\star}$ and $z^{\star}$ in a standard normal model, $\mathcal{N}(0,1)$, corresponds to the confidence level. Scientists either use \textsf{R} software or a Z-table \footnote{also known as a Normal table} to find the critical value, $z^{\star}$. 

The \var{qnorm()} function in \textsf{R} takes probability $p$ as an input and outputs the quantile value $z$ such that $P(Z\leq z)=p$. Use $p=0.025$ for a 95\% confidence interval, grabbing the \emph{middle} 95\% of values. To find $z^{\star}$ in \textsf{R} for a 95\% confidence level \begin{verbatim}
> qnorm(0.025)
[1] -1.959964
\end{verbatim}
$z^{\star}=1.96$ is the critical value for 95\%. \footnote{It does not matter whether $z^{\star}$ is positive or negative since the half widths are on both sides of the point estimate, but scientists generally let $z^{\star}$ be positive.} 
\begin{exercise} 
What is the critical value associated with a (a) 90\%, (b) 75\% and (c) 50\% confidence interval? \footnote{For any confidence level $C$, \var{qnorm($0.5\cdot (1-C)$)} outputs a $z^{\star}$ to contain the \emph{middle} values in a standard normal.(a) \var{qnorm(0.05)= -1.644854}. $z^{\star}=1.65$ for a 90\% confidence level (b) 1.15 (c) 0.67}
\end{exercise}

\begin{exercise} \label{find90CIForBRFSSWeightExercise}
Use the data in Exercise~\ref{find99CIForBRFSSWeightExercise} to create a 90\% confidence interval for the average weight of men in the United States.\footnote{Find$z^{\star}$ such that 90\% of the standard normal distribution falls between -$z^{\star}$ and $z^{\star}$: $z^{\star}=1.65$. The 90\% confidence interval is $\overline{w_\mathrm{men}}\ \pm\ 1.65\times SE_{\overline{w_\mathrm{men}}} \to (178.32, 199.78)$.  (The were already verified in Exercise~\ref{find99CIForBRFSSWeightExercise}.) That is, we are 90\% confident the average weight of males is between 178.32 and 199.78 pounds. Note that the width of this confidence interval is smaller than the 95\% confidence interval calculated in Exercise~\ref{find99CIForBRFSSWeightExercise}.}
\end{exercise}

\subsection{Interpreting confidence intervals}
\label{interpretingCIs}

\index{confidence interval!interpretation|(}

The confidence interval conclusion contains some fairly awkward language. Correct interpretation: 
\begin{quote}
We are XX\% confident that the population parameter is between...
\end{quote}

Section~\ref{confidenceLevels} defined a 95\% confidence level through the process of repeated sampling. Approximately 95\% of confidence intervals created from independent random samples would contain the population parameter. 

Researchers would almost never be able to resample and generate confidence intervals 100 times. More importantly, they would not know which intervals contain the population parameter since the parameter is unknown! The meaning of being "95\% confident" is one grounded in theory and less in practice. "Confidence" relates more to the reliability of the process that creates such a range and less in the probability that the value is captured within the range. 

\emph{Incorrect} language can describe a confidence interval as capturing the population parameter with a certain probability. This is one of the most common errors: while it might be useful to think of it as a probability, the confidence level only quantifies how plausible the parameter is in the interval. 

Another especially important consideration is that confidence intervals try to capture the \emph{population parameter}. Intervals say nothing about capturing individual observations, a proportion of the observations, a percent of all the data or just the sampled data. A confidence interval also deals nothing with capturing the sample mean or other point estimates. A confidence interval is always centered at the observed point estimate. Confidence intervals only attempt to capture and and estimate plausible values that the population parameter can take on. 

Some incorrect interpretations of a 95\% confidence interval include: 
\begin{quote}
95\% of the observed data is between ...\\
95\% of the population distribution is contained in the confidence interval.\\
\end{quote}

The differences in correct and incorrect interpretations are extremely nuanced. The technical and precise wording of a confidence interval interpretation has little bearing in practice. The goal of this book is to, instead, provide a meaningful understanding of confidence intervals and the tools to calculate a confidence interval from data. 

\index{confidence interval!interpretation|)}
\index{confidence interval|)}

\subsection[Nearly normal population with known SD (special topic)]{Nearly normal population with known SD (special topic)}
\label{nearlyNormalPopWithKnownSD}

\index{Central Limit Theorem!normal data|(}

In rare circumstances, important characteristics of a population are already known. Scientists might know certain parameter values, but a random sample will still be drawn to study other characteristics of the population. Consider the assumptions required to model the sample mean after a normal distribution: 
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item[(1)] The observations are independent.
\item[(2)] The sample size $n$ is at least 30.
\item[(3)] The data distribution is not strongly skewed.
\end{enumerate}

These conditions are required to ensure the distribution of sample means is nearly normal. If the population is already known to be nearly normal, scientists do not need to rely on a rule of thumb of $n\geq 30$. The sample mean is always distributed normally (this is a special case of the Central Limit Theorem). If the population standard deviation is also known, then conditions (2) and (3) are not necessary for the data.

\begin{example}{The heights of male seniors in high school closely follow a normal distribution $\mathcal{N}(\mu=70.43, \sigma=2.73)$, where the units are inches.\footnote{These values were computed using the USDA Food Commodity Intake Database. \url{http://www.ars.usda.gov/News/docs.htm?docid=14514}} If the heights of five male seniors are randomly sampled, what distribution should the sample mean follow?}\label{simpleSampleOfFiveMaleSeniors}
The heights of the population, high school male seniors, is nearly normal, the population standard deviation, $\sigma$, is known, and the random sample is from a much larger population. These observations are considered independent. Therefore the sample mean is distributed normally with mean $\mu=70.43$ inches and standard error $SE=\sigma/\sqrt{n} = 2.73/\sqrt{5}=1.22$ inches.
\end{example}

\begin{termBox}{\tBoxTitle{Alternative conditions for applying the normal distribution to model the sample mean}
If the population is known to be nearly normal and the population standard deviation, $\sigma$, is known, the sample mean $\overline{x}$ follows a nearly normal distribution, $\mathcal{N}(\mu, \sigma/\sqrt{n})$, if the sampled observations are independent. There is no uncertainty surrounding the standard deviation of population. The standard error, instead, uses the known population standard deviation: $SE = \sigma/\sqrt{n}$}
\end{termBox}

\begin{tipBox}{\tipBoxTitle{Relaxing the nearly normal condition}
As the sample size becomes larger, it is reasonable to \emph{slowly} relax the nearly normal assumption on the data. By the time the sample size reaches 30, the data must show strong skew to have concerns about the normality of the sample mean.}
\index{Central Limit Theorem!normal data|)}
\end{tipBox}

In practice, the population standard deviation is rarely know, but the Central Limit Theorem describes the distribution of the sample mean more specifically with samples of large sample sizes.

%__________________
\section{Hypothesis testing}
\label{hypothesisTesting}

\index{hypothesis testing|(}

The CDC first provided a single number to estimate the average adult BMI in the U.S. in 2000. Then it used a 95\% confidence interval as a plausible range of values for the population parameter to take on. Now, the CDC investigates if the average U.S. adult BMI in 2000 has changed from 20 years ago ($\overline{\mathrm{BMI}}_{1980} =25.3$). \footnote{Flegal, KM, MD Carroll, RJ Kuczmarski, and CL Johnson. "Overweight and Obesity in the United States: Prevalence and Trends, 1960?1994." International Journal of Obesity (1998): 39-47. Web. \url{<http://www.ncbi.nlm.nih.gov/pubmed/9481598>}.}

The National Institute for Occupational Safety and Health (NIOSH) is a branch of the CDC responsible for work-related injury and illness. It is concerned with the respiratory health of U.S. coal miners. High levels of dust exposure in coal mines increases the prevalence of coal workers' pneumoconiosis (black lung) and progressive massive fibrosis. In the 1992 \emph{American Industrial Hygiene Association Journal}, scientists collected dust exposure data in coal mines. \footnote{Attfield, M.d., and K. Morring. ``An Investigation into the Relationship Between Coal Workers' Pneumoconiosis and Dust Exposure in U.S. Coal Miners.'' American Industrial Hygiene Association Journal 53.8 (1992): 486-92. Web. \url{<http://www.cdc.gov/NIOSH/NAS/RDRP/appendices/chapter3/a3-6.pdf}>.} The Mine Safety and Health Administration (MSHA) permissible dust exposure limit was 2 $mg/m^3$ in 1992. Were coal mines adhering to this limit at the time? 

Many questions like these, given the correct data, can be answered through hypothesis testing. \term{Hypothesis testing} is a statistical method that evaluates whether or not a population parameter takes on some hypothesized value with an associated probability of error. This procedure determines the probability that a given hypothesis is true.

Hypotheses are often simple questions that have a yes or no answer. Consider some hypotheses below: \begin{quote}
Is the mean body temperature really $98.6^\circ$ F? \\
Has consumption of soda changed across the U.S. overtime? \\
Do MCAT classes improve MCAT scores? 
\end{quote}

The hypothesis testing process consists of five steps. Stepping through the \term{hypothesis testing framework} in Section~\ref{hypothesisTesting} allows scientists to answer these yes/no questions with a certain degree of confidence after observing a single sample from the target population.  Hypothesis testing and confidence intervals can be used to make inferences on two or more populations, but Chapter~\ref{foundationsForInference} focuses on estimating the population mean from a single sample. Chapter~\ref{inferenceForNumericalData} expands to two or more populations and other population parameters.

\subsection{Hypothesis testing framework}
\label{hypothesisFramework}

The sample mean of \data{BRFSS BMI} is 26.53. The CDC wonders if this sample provides enough evidence that adults are, on average, as healthy as they were 20 years ago versus the alternative, they are not. This question can be simplified into two \term{hypotheses}: 
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[$H_0$:] U.S. adults in 2000 are, on average, as healthy as they were in 1980 when the average BMI was 25.3. 
\item[$H_A$:] The average U.S. adult BMI in 2000 is not the same as the average U.S. adult BMI in 1980. 
\end{itemize}

\subsubsection{Step 1: Formulating hypotheses}

The first step within the hypothesis testing framework is defining the hypotheses. There are  generally two hypotheses, a null and an alternative. $H_0$ is called\marginpar[\raggedright\vspace{6mm}

$H_0$\\\footnotesize null hypothesis\vspace{3mm}\\\normalsize $H_A$\\\footnotesize alternative\\ hypothesis]{\raggedright\vspace{6mm}

$H_0$\\\footnotesize null hypothesis\vspace{3mm}\\\normalsize $H_A$\\\footnotesize alternative\\ hypothesis} the null hypothesis, and $H_A$ is the alternative hypothesis.

\begin{termBox}{\tBoxTitle{Null and alternative hypotheses}
{\small The \term{null hypothesis ($H_0$)} often represents either a skeptical perspective or a claim to be tested. The \term{alternative hypothesis ($H_A$)} represents an alternative claim under consideration and is often represented by a range of possible parameter values.}}
\end{termBox}

The null hypothesis is often a common view on something. The null hypothesis is generally denoted as "no difference" or what one would observe if there is no change.  The alternative hypothesis often represents a new perspective, the possibility that there has been a change or a new discovery. If the null hypothesis is true, any difference between the observed sample and the null hypothesis is due only to chance variation. 

\begin{tipBox}{\tipBoxTitle{Hypothesis testing framework}
The logic of hypothesis testing is that the null hypothesis ($H_0$) will not be rejected, unless the evidence in favor of the alternative hypothesis ($H_A$) is so strong that $H_0$ must be rejected to favor $H_A$.}
\end{tipBox}

The framework's first step describes the very nature of the scientific method. The hypotheses are stated and evidence is observed. The null hypothesis $H_0$ is generally assumed to be true unless there is sufficient observed evidence that does not support the claim. Then the null hypothesis is rejected in favor of the alternative. 

\begin{exercise} \label{hypTestStudyExample}
A new study would like to be published in a scientific journal. The referee that determines the validity of the study considers two possible claims about this study: either the study is valid or it is pseudoscience. Using the hypothesis framework, which of these claims would be the null hypothesis and which the alternative? \footnote{ The referee assumes that past literature is true and is somewhat skeptical of the discovery. If the study is legitimate after evaluating the evidence (methodology, results and reproducibility), the referee rejects the null hypothesis (The study is pseudoscience. There is no change.) and concludes the study is valid and should be published (the alternative hypothesis).}
\end{exercise}

Even if the referee of the journal leaves unconvinced that the study is publishable, the study is not necessarily a complete fabrication. Hypothesis testing is the same: \emph{even if the null hypothesis is not rejected, it cannot be accepted}. Failing to find strong evidence for the alternative hypothesis is not equivalent to accepting the null hypothesis. 

\begin{tipBox}{\tipBoxTitle{Double negatives are used in statistics}
Many times in statistics, double negatives are used. For instance, scientists might say that the null hypothesis is \emph{not implausible}, or they \emph{failed to reject} the null hypothesis. Double negatives are used to communicate that while a position is not being rejected, it is also not accepted as truth.}
\end{tipBox}

The null hypothesis is the average U.S. adult BMI in 2000 equals 25.3, the average BMI in 1980. The alternative hypothesis represents something new or more interesting: the average U.S. adult BMI has changed since 1980. The average U.S. adult BMI in 2000 is not equal to 25.3. These hypotheses can be written in mathematical notation, defining $\mu_{\mathrm{BMI}}$ as the average BMI for U.S. adults in 2000.
\[H_0:\mu_{\mathrm{BMI}} = 25.3 \hspace{0.5in} H_A: \mu_{\mathrm{BMIi}} \neq 25.3\]

Within hypotheses, 25.32 is referred to as the \term{null value}, denoted $\mu_0$. It represents the value of the parameter if the null hypothesis is true. The \data{BRFSS BMI} sample will be used to evaluate these hypotheses. 

It is important to note that the CDC is not testing whether or not the average BMI in \data{BRFFS BMI} is 25.3. The CDC can simply calculate the average BMI in \data{BRFSS BMI} and see. Rather these hypotheses are testing if the \emph{population parameter}, the average BMI of all U.S. adults, is 25.3. 

\begin{tipBox}{\tipBoxTitle{Null and Alternative Hypothesis Setup}
The null hypothesis is generally written as $H_0: \mu=\mu_0$. $\mu$ is the population mean, and $\mu_0$ is the null value. \\ \\
The alternative hypothesis can take on many forms. If the researchers are interested in showing any difference-- an increase or decrease-- then the safest alternative hypothesis choice would be $\mu\neq \mu_0$, a two-sided alternative. If there exist a prior belief of how $\mu$ and $\mu_0$ compare or researchers are interested in showing only an increase or decrease, not both, a one-sided alternative, $\mu >  \mu_0$ or $\mu < \mu_0$, should be used. Section~\ref{pValue} goes into more detail on one-side versus two sided alternative hypotheses.}
\end{tipBox}

\subsubsection{Step 2: Specifying a significance level $\alpha$}
After researchers state a null and alternative hypothesis, they specify a \term{significance level}. The significance level, $\alpha$, is the acceptable error probability of the test. The error probability is the probability of incorrectly concluding the alternative hypothesis is true when it is, in fact, not true. This error is called a Type 1 error, and $\alpha$ is the probability of making a Type 1 error. Section \ref{DecisionErrors} goes into more detail on error types. 

Typically, $\alpha$ is taken to be 0.05, 0.01, or some other small value, and is a measure of uncertainty. If $\alpha=0.05$, the hypothesis test will be performed at a 95\% confidence level. Section~\ref{utilizingOurCI} offers a connection between hypothesis testing and confidence intervals.  

\subsubsection{Step 3: Calculating the test statistic}
The third step is to calculate a test statistic from the observed data. The test statistic measures the difference between the observed point estimate and what is expected if the null hypothesis were true. It answers the question: "How many standard deviations from the null value is the observed sample mean?" The test statistic is used in the follow steps to arrive at a conclusion. 

The test statistic follows a similar construction as standardizing a normal distribution in Section~\ref{normalDist}. The test statistic used for inference on a population mean when the population standard deviation is unknown will always be
\begin{eqnarray}T=\frac{\overline{x}-\mu_0}{s/\sqrt{n}}\end{eqnarray} 
where $\overline{x}$ is the sample mean, $s$ is the sample standard deviation and $n$ is the number of observations in the sample. 

\emph{Note:} In general, test statistics follow the form $\frac{\mathrm{observed-hypothesized}}{\text{standard error}}$ to calculate the number of standard deviations the point estimate is from the null value. 

\begin{termBox}{\tBoxTitle{Test statistic}
A \emph{test statistic} is a special summary statistic that is particularly useful for evaluating hypothesis tests. The test statistic summarizes how many standard deviations the point estimate is from the null value and follows a $t$-distribution with $n-1$ degrees of freedom. The $t$-distribution will be covered in Section~\ref{tdistribution}.}
\index{hypothesis testing!using normal model|)}
\end{termBox}

\subsubsection{Step 4: Calculating the p-value}
Consider a T-statistic of 10. The observed sample mean is 10 standard deviations away from the hypothesized value if the null hypothesis were true. Observing this sample mean is such a rare event. It is so far away from the hypothesized value, but how rare is observing a value 10 standard deviations away from the mean? 10\% likely? 1\% likely? 0.001\%?

The \term{p-value} is the probability of observing a point estimate or a more extreme point estimate assuming the null hypothesis is true. Formally the p-value is a conditional probability. The p-value allows the researchers to make conclusions from the T-statistic. 

\begin{termBox}{\tBoxTitle{p-value}
The \term{p-value}\index{hypothesis testing!p-value|textbf} is the probability of observing data at least as favorable as the current sample to reject the null hypothesis, if the null hypothesis is true. A summary statistic of the data, the T-statistic, is typically used to help compute the p-value and evaluate the hypotheses.}
\end{termBox}

How is this probability calculated? Section~\ref{pValue} provides the details to calculate the p-value using \textsf{R}, and the Z and $t$-tables. 

\subsubsection{Step 5: Making the conclusion}
The final step within the hypothesis testing framework is to make a conclusion using the T-statistic and p-value. 

A large T-statistic indicates an extreme observation, and the p-value, the probability associated with observing an extreme point estimate, will be small. A low p-value from Step 4 provides evidence that the null value is unlikely true. The smaller the p-value, the stronger the evidence reject the null hypothesis. 

How small is small? The significance level, $\alpha$ from Step 2 determines the cutoff and whether or not scientists can reject the null hypothesis. If the p-value is smaller than $\alpha$ (usually 0.01 or 0.05), the null hypothesis is rejected. If the p-value is $\alpha$ or greater, there is not enough evidence to reject the null hypothesis. 

Failure to reject $H_0$ is not equivalent to acceptance of $H_0$ (refer to Example \ref{hypTestStudyExample} for clarity) theoretically. In practice, however, failure to reject $H_0$ is often the same as accepting $H_0$, and rejecting $H_0$ is equivalent to accepting $H_A$. Most importantly, researchers must state the conclusion in the context of the original problem, using the language and units of the inference question. Many times this is forgotten, but it is absolutely necessary in both theory and practice. 

\subsection{Calculating p-values}
\label{pValue}

\index{hypothesis testing!p-value|(}

Calculating p-values can be the most difficult part of hypothesis testing. The p-value depends on many moving parts: the T-statistic, the sample size and the alternative hypothesis, but always remember, if the p-value is smaller than $\alpha$, the sample indicates that something rare was observed. The null hypothesis should be rejected as true. 

\begin{figure}[ht]
   \centering
   \includegraphics[width=0.9\textwidth]{ch_inference_foundations_oi_biostat/figures/pValueOneSidedSleepStudyExplained/pValueOneSidedSleepStudyExplained}
   \caption{The p-value is defined as the probability of observing the $\overline{x}$ or a sample mean even more extreme under the null hypothesis. For a one sided alternative $\mu > \mu_0$, the p-value is the shaded area right of $\overline{x}$ in the upper tail.} 
   \label{pValueOneSidedSleepStudyExplained}
\end{figure}

The shaded area in Figure~\ref{pValueOneSidedSleepStudyExplained} is the p-value for a one sided alternative $\mu > \mu_0$. If the alternative hypothesis has the form $\mu > \mu_0$, the p-value is the area shaded in the upper tail. If the alternative is one sided but has the form $\mu < \mu_0$, then the p-value would be the shaded area, left of the observed $\overline{x}$. In Figure~\ref{pValueOneSidedSleepStudyExplained}, if the alternative were $\mu < \mu_0$, the shaded area would be the majority of the distribution. With a two-sided alternative hypothesis, \emph{two tails are shaded} since evidence in either direction is contradicts $H_0$ (Figure~\ref{2ndSchSleepHTExample}). 

\begin{figure}
   \centering
   \includegraphics[width=0.9\textwidth]{ch_inference_foundations_oi_biostat/figures/2ndSchSleepHTExample/2ndSchSleepHTExample}
   \caption{$H_A$ is two-sided, so \emph{both} tails must be counted for the p-value. Observing a significance change -- increase or a decrease --  would be favorable to $H_A$.}
   \label{2ndSchSleepHTExample}
\end{figure}

How do scientists quantify this shaded area? The process of using a T-statistic to calculate the p-value is analogous to using a Z-score to determine shaded area underneath a normal curve. Recall the example where the CDC compares adult health in 2000 to adult health in 1980 using the following hypotheses: \[H_0:\mu_{\mathrm{BMI}} = 25.3 \hspace{0.5in} H_A: \mu_{\mathrm{BMI}} \neq 25.3\]

Consider a sample of 50 people with a sample mean of 24. The sample standard deviation is 5. \footnote{calculating the T-statistic using the sample mean from the \data{BRFSS BMI} dataset is an exercise in the book} The T-statistic is therefore \[T=\frac{24-25.3}{5/\sqrt{50}}= -1.84\] The T-statistic indicates how many standard deviations the observed sample mean is from the null value. This standardization becomes a great way to unify all the moving parts in order to calculate the p-value. Step 3 concludes with the T-statistic as -1.84. 

Calculating the p-value in Step 4 begins with identifying the distribution of the T-statistic. Recall from Section~\ref{hypothesisFramework}, the T-statistic follows a $t$-distribution with $n-1$ degrees of freedom. If $n\geq 30$, the T-statistic beings to be distributed normally, shown in Section~\ref{tdistribution}. Scientists prefer to approximate the distribution of the T-statistic to a normal because of the attractive qualities introduced in Chapter~\ref{modeling} that a normal distribution offers. 

Researchers can either use a Z or $t$-table to calculate the p-value.When $n<30$, scientists use a $t$-table like the one in Appendix~\ref{tDistributionTable} on page~\pageref{tDistributionTable}. If $n\geq 30$, Z-tables are used, found in Appendix~\ref{normalProbabilityTable} on page~\pageref{normalProbabilityTable}. The $t$-table lists the approximate area for one or two tailed alternative hypotheses whereas the Z-table only indicates the area left of the T-statistic, coinciding with a one-sided alternative. Because normal distributions are symmetric, finding the p-value for a two sided alternative is just the value from a Z-table times two!

The T-statistic of -1.84 was calculated from a sample of 50 people. The Z-table indicates that the area to the left of -1.84 is 0.0329. The alternative hypothesis is two-sided, $\mu_{\mathrm{bmi}} \neq 25.3$, so both tails are shaded in Figure~\ref{2ndSchSleepHTExample}. The p-value, $p$, is 
\begin{align*}
p &=Pr(T\leq -1.84) + Pr(T\geq 1.84)\\
&= Pr(|T| \leq -1.84)\\
&= 2Pr(T\leq -1.84)\\
&= 2 \cdot 0.0329\\
&= 0.0658
\end{align*}

If the CDC did not use the rule of thumb, $n\geq 30$, they would use the $t$-table to determine the p-value. A T-statistic of -1.84 and 49, ($n-1$), degrees of freedom results in a p-value between 0.1 and 0.05 for a two-sided alternative. The $t$-table lists fewer values than the Z-table. The resulting p-value is imprecise. 

 In most cases, researchers use \textsf{R} once the T-statistic and its distribution is known. Use the \var{pt()} or \var{pnorm()} function to calculate the area left of the T-statistic. However from Figure~\ref{pValueOneSidedSleepStudyExplained}, be mindful that the area left of the T-statistic is not necessarily the p-value. \footnote{Figure~\ref{pValueOneSidedSleepStudyExplained} is the distribution of the sample mean, not the distribution of the T-statistic. The distribution of the T-statistic is very similar because the T-statistic is the sample mean normalized. The normalized distribution is centered at 0 (the null value in the sample distribution), and $\overline{x}$ is analogous to the T-statistic. The shaded area, therefore, remains the same.} The complement of the \textsf{R} output may be used to isolate the appropriate shaded area of the distribution. 

The $n\geq 30$ threshold does not need to be adopted when using \textsf{R}. The \var{pt()} function is just as easy and more accurate than approximating the distribution to a normal model with the \var{pnorm()} function. However as $n$ increases, the normal and $t$-distributions almost become equivalent, evidenced by Section ~ref{distribution}. 

Use both \var{pnorm()} and \var{pt()} to calculate the p-value. \begin{verbatim}
> 2*pnorm(-1.84)
[1] 0.06576824
> 2*pt(-1.84,49)
[1] 0.07182936
\end{verbatim}

Both functions produce extremely similar outputs for a p-value around 0.07.

The CDC proceeds to Step 5: form a conclusion in the context of the question. The p-value (0.07) is greater than the significance level, $\alpha=0.05$, established in Step 2. $H_0$ can not be rejected. In context, after observing a sample mean of 24 for the average BMI in the U.S. in 2000, a p-value of 0.07 was observed. This p-value is greater than 0.05. Therefore the average BMI from the observed sample suggests that the average adult BMI in the U.S. is 25.3, the average in 1980. There does not exist a noticeable difference between the health of adults from 1980 to 2000. 

The p-value of 0.07 is extremely close to $\alpha=0.05$. A p-value of 0.99 would achieve a similar conclusion-- failure to reject the null hypothesis-- but these p-values convey very different information. In practice when scientists observe p-values very close to $\alpha$, they can either weakly reject or fail to reject the null hypothesis or they can run the study again. Regardless of what they do, these scientists must reveal their p-value in the conclusion. 

\begin{termBox}{\tBoxTitle{p-value as a tool in hypothesis testing}
The p-value quantifies how strongly the data favor $H_A$ over $H_0$. A small p-value (usually $<0.05$) corresponds to sufficient evidence to reject $H_0$ in favor of $H_A$.}
\index{hypothesis testing!p-value|)}
\end{termBox}

\begin{exercise}
If the null hypothesis is true, how often should the p-value be less than 0.05?\footnote{About 5\% of the time. If the null hypothesis is true, then the data only has a 5\% chance of being in the 5\% of data contradictory to $H_0$ and most favorable to $H_A$.}
\index{data!school sleep|)}
\end{exercise}

\begin{tipBox}{\tipBoxTitle{Concluding from Critical Values}
Conclusions are made from the relationship between the p-value and the significance level. If $\alpha$ is a common value like 0.05, the critical value can offer a quick shortcut to the conclusion. \\

Recall that critical value is the coefficient of the standard error in the confidence interval formula. The critical value is also the Z-score of a normalized sampling distribution. For a two-sided alternative, if the absolute value of the T-statistic is greater than the critical value, the p-value is less than $\alpha$ and the null hypothesis can be rejected. If the alternative hypothesis is one-sided, the critical value for comparison is if the confidence level were $(1-2*\alpha)$. For example under a one sided alternative at a 95\% confidence level, the T-statistic is compared to 1.65, the critical value for a 90\% confidence interval.}
\end{tipBox}

\begin{caution}{Critical value $\neq$ T-statistic}
{The critical value and the T-statistic are often confused for each other. The critical value is associated with the value of $\alpha$ and does not change. For a specific $\alpha$, there is only one critical value. The T-statistic varies with the sample observed. When forming a conclusion, the T-statistic is compared to the critical value using the critical value as a benchmark.}
\end{caution}

\subsubsection{One sided coal example}
\label{onesidedCoalExample}

In 1992, NIOSH published a paper in the \emph{American Industrial Hygiene Association Journal}.  \footnote{Attfield, M.d., and K. Morring. "An Investigation into the Relationship Between Coal Workers? Pneumoconiosis and Dust Exposure in U.S. Coal Miners." American Industrial Hygiene Association Journal 53.8 (1992): 486-92. Web. \url{<http://www.cdc.gov/NIOSH/NAS/RDRP/appendices/chapter3/a3-6.pdf}>.}  In this paper, Attfield and Morring studied the prevalence of coal workers' pneumoconiosis (CWP) and its relation to indexes of dust exposure. High levels of dust exposure for miners were linked to prevalences of both simple CWP and progressive massive fibrosis. With this clear relationship between high dust exposure and illness, were mines respecting the federal compliance level of 2 $\mathrm{mg}/\mathrm{m}^3$ in 1992?

The \data{COAL} dataset used in the paper also allows NIOSH to test if the average dust exposure in U.S. mines were under the legal limit. The \data{COAL} data is a sample of 8936 coal miners across the United States. Their personal dust exposures, \var{dust}, were estimated from intensive dust sampling from 1968 to 1969 from 17 out of 31 underground mines and serves as an excellent sample to infer on U.S. mines in 1992. \footnote{While the sampling mechanism is not clear, the coal miners were not chosen from a simple random sample of all coal miners in the 31 underground mines with equal probability. A simple random sample is rare in many cross-national studies but assumptions of independence and randomness are loosened due to the size of the sample. With 8936 coal miners in the sample, the sample mean is extremely likely to be normally distributed.}

\begin{example}{
NIOSH hopes that U.S. mines in 1992 are complying with federal regulation but have a prior belief that they are lax in occupational safety. How would NIOSH define the null and alternative hypotheses?}
\label{coalHypotheses}

The dust exposures for coal miners across the U.S. are assumed to be nearly normal, and the observations in \data{COAL} are independent.  NIOSH defines the null and alternative hypotheses: 
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[$H_0$:] The dust levels in U.S. mines during 1992 were less than 2 $\mathrm{mg}/\mathrm{m}^3$. 
\item[$H_A$:] The U.S. mines during 1992 were breaking the law. That is, the dust levels were higher than 2 $\mathrm{mg}/\mathrm{m}^3$. 
\end{itemize}
The alternative is one-sided because NIOSH wants to test if the U.S. mines are breaking the law. The choice of one-sided versus two-sided alternative is entirely based on the interests of the researchers. A two-sided alternative would indicate that NIOSH is interested in any difference -- the U.S. mines could have lower or higher dust exposure levels than 2 $\mathrm{mg}/\mathrm{m}^3$. This is not the scope of this hypothesis test.  
\end{example}

\begin{exercise} \label{coalHypMath}
Let $\mu_\mathrm{dust}$ denote the average dust exposure in U.S. mines during 1992. What would the hypotheses be in mathematical notation?
\footnote{$H_0: \mu_\mathrm{dust}\leq2 \mathrm{mg}/\mathrm{m}^3$ and $H_A: \mu_\mathrm{dust}> 2 \mathrm{mg}/\mathrm{m}^3$}
\end{exercise}

\index{data!school sleep|(}

\emph{Note}: Always use a two-sided test unless it was made clear prior to data collection that the test should be one-sided. Switching a two-sided test to a one-sided test after observing the data is dangerous because it can inflate the chance of an incorrect conclusion. Section~\ref{twoSidedTestsWithPValues} explores the consequences of switching between different alternative hypotheses. 

\begin{tipBox}{\tipBoxTitle{One-sided and two-sided tests}
If the researchers are only interested in showing an increase or a decrease, but not both, use a one-sided test. If the researchers would be interested in any difference from the null value -- an increase or decrease -- then the test should be two-sided.\vspace{0.5mm}}
\end{tipBox}

\begin{tipBox}{\tipBoxTitle{Write the null hypothesis as an equality}
Writing the null hypothesis as an equality (e.g. $\mu = 2$) makes hypothesis testing easier. The alternative then should either be written with an unequal or inequality sign (e.g. $\mu\neq2$, $\mu >2$, or $\mu <2$).\footnote{Guided Practice~\ref{coalHypMath} does not follow this tip. NIOSH does not want to test if $\mu_\mathrm{dust}$ is exactly equal to two, but rather if the mines are complying. Tips are useful guidelines to follow, but thinking about the context of the problem is more important.}}
\end{tipBox}

The \data{COAL} dataset has a sample mean of 2.20 $\mathrm{mg}/\mathrm{m}^3$.  Among 8936 coal miners, the standard deviation of dust exposure was 1.23 $\mathrm{mg}/\mathrm{m}^3$. A histogram of the sample is shown in Figure~\ref{histOfDustExposure}.

\begin{figure}
\centering
\includegraphics[width = \textwidth]{ch_inference_foundations_oi_biostat/figures/histOfDustExposure/histOfDustExposure}
\caption{Distribution of the dust exposure among 8936 coal miners. The box plot shows that these data are highly skewed. The distribution has an extremely long right tail. However, there are only approximately 70 outliers out of 8936 coal miners. Even with extreme skew $\overline{\mathrm{dust}}$ will be distributed normally because of the size of the sample.}
\index{skew!example: moderate}
\label{histOfDustExposure}
\end{figure}

Before moving onto Step 2 in the hypothesis testing framework, conditions for normality must be verified. (1)~There is no obvious evidence in the paper that the authors did not include randomness in sampling. Observations can be considered independent. (2) The sample size is sufficiently large as it is greater than 30. (3) The data show extreme skew in Figure~\ref{histOfDustExposure} with the presence of many outliers. The extreme skewness does not prevent $\overline{\mathrm{dust}}$ from being normally distributed due to the sample size of $n=8936$ \footnote{The outliers can be high levels of dust exposure or misinformation. Data is almost never collected perfectly. There generally exists error be it sampling error or measurement error within the data. Having a larger sample size mitigates large effects on inference.}. With these conditions verified, the normal model can be safely applied to $\overline{\mathrm{dust}}$ and the estimated standard error will be accurate.

\begin{exercise} \label{findSEOfFirstSleepStudyCheckingGreaterThan7Hours}
Estimate the standard error of $\overline{\mathrm{dust}}$.\footnote{The standard error can be estimated from the sample standard deviation and the sample size: $SE_{\overline{\mathrm{dust}}} = \frac{s_\mathrm{dust}}{\sqrt{n}} = \frac{1.23}{\sqrt{8936}} = 0.013$.}
\end{exercise}

NIOSH evaluates the hypothesis test using the standard significance level $\alpha = 0.05$. The sampling distribution assumes that the null hypothesis is true. In this case, the sample mean, 2.20 hours, was drawn from a distribution that is nearly normal with mean 2 and standard deviation of 0.013. Such a distribution is shown in Figure~\ref{pValueOneSidedCoalStudy}. 

\begin{figure}[hht]
   \centering
   \includegraphics[width=0.73\textwidth]{ch_inference_foundations_oi_biostat/figures/pValueOneSidedCoalStudy/pValueOneSidedCoalStudy}
  \caption{Assuming the null hypothesis is true, the sample mean $\overline{\mathrm{dust}}$ is drawn from this nearly normal distribution. The right tail describes the probability of observing an even larger mean if the null hypothesis were true. The p-value is the shaded blue area of values more extreme than $2.20$ and provides evidence more favorable to the alternative hypothesis than to the null hypothesis.}
\label{pValueOneSidedCoalStudy}
\end{figure}

Step 3 is the calculation of the T-statistic for observed sample mean $\overline{\mathrm{dust}} = 2.20$. 
\[ T = \frac{\overline{\mathrm{dust}} - \text{null value}}{SE_{\overline{\mathrm{dust}}}} = \frac{2.20 -2}{0.013} = 15.35\]

The T-statistic is 15.35 and can be approximated to the normal distribution as $n=8936$. The normal table on Page~\pageref{normalProbabilityTable} does not list Z-scores larger than 3.4 in absolute value. NIOSH turns to \textsf{R}. 
 \begin{verbatim} 
 > 1-pnorm(15.35)
[1] 0 
\end{verbatim}
Chapter~\ref{modeling} explains that the area to the right of 15.35 cannot be exactly 0, but the area is so microscopic that \textsf{R} approximates the p-value to 0. 

{\em If the null hypothesis is true, the probability of observing such a large sample mean of 2.20 for a sample of 8936 students is almost 0.}
\index{p-value!interpretation example} That is, if the null hypothesis is true, such a large sample mean would not be observed in most\footnote{almost all} instances. 

Step 5 compares the p-value to the significance level. Because the p-value is less than the significance level $\alpha$, the null hypothesis is rejected.\footnote{Using critical values instead, for $\alpha=0.05$ and a one sided alternative, the critical value is 1.65. Because the T-statistic is greater than 1.65, $H_0$ is rejected without calculating the p-value.} NIOSH's observation was so unusual assuming the null hypothesis is true that it casts serious doubt on $H_0$. Observing a sample mean of 2.20 provides strong evidence favoring $H_A$. In context, it is most likely that these mines did not meet federal dust exposure regulations in 1992. 

\begin{tipBox}{\tipBoxTitle{It is useful to first draw a picture to find the p-value}
It is useful to draw a picture of the distribution of $\overline{x}$ as though $H_0$ were true (i.e. $\mu$ equals the null value), and shade the region (or regions) of sample means that are at least as favorable to the alternative hypothesis and more extreme than the observed sample mean. The sum of the shaded regions represent the p-value.}
\end{tipBox}

\subsection{Testing hypotheses using confidence intervals}
\label{utilizingOurCI}

Confidence intervals and hypothesis testing may seem disjointed -- one provides a range of values while the other results in a yes or no conclusion, but these two methods arrive at the same conclusions when a null hypothesis is specified.
	
\begin{example}{
The 2000 Census listed the median age of U.S. adults as 43.55 years. With a simple random sample of 100 people from \data{BRFSS}, the CDC wants to see how its estimated population parameter compares with the one listed by the Census.\footnote{The 2000 Census, \url{https://www.census.gov/prod/2001pubs/c2kbr01-12.pdf}, listed the median age instead of mean age. While the Census does not mention normality for the age distribution, the difference in mean and median age is negligible because the U.S. population is so large.} The sample has an average age of 43.9 years and a sample standard deviation of 17.97 years. Conduct a hypothesis test and a confidence interval. Are these related in anyway?}
\label{CIandHypTests}

Beginning with hypothesis testing, the CDC first verifies the assumptions are met. With a sample size of $n=100$, independence and normality hold.  The null and alternative hypotheses are defined as: 
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[$H_0$:] $\mu_{\mathrm{age}}=43.55$
\item[$H_A$:] $\mu_{\mathrm{age}} \neq 43.55$
\end{itemize}
The alternative hypothesis is two-sided because the CDC has no prior belief on the sidedness of this question. 

Let $\alpha$ = 0.05. The T-statistic is
 \[T = \frac{43.9-43.55}{17.97/\sqrt{100}}= \frac{.35}{1.797} = 0.19\] With $n\geq 30$, the T-statistic can follow a normal distribution to calculate the p-value in \textsf{R}. The p-value is 
\begin{verbatim}
> 2*(1-pnorm(0.19))
[1] 0.8493091
\end{verbatim}

For a two-sided alternative, the p-value is greater than $\alpha$. Therefore the sample that the CDC observed can not be considered a "rare event" if the null hypothesis were true. The null hypothesis cannot be rejected. Observing a sample mean of 43.9 years gives the CDC sufficient evidence at the 95\% confidence level that the average U.S. adult age is consistent with the Census estimate of 43.55 years old. 

The CDC then calculates a confidence interval. Under hypothesis testing, the CDC used $\alpha=0.05$, so the CDC chooses to create a 95\% confidence interval with the same sample. 

\begin{align*}
43.9 &\pm 1.96 * \frac{.35}{\sqrt{100}}\\
43.9 &\pm 1.96 * 0.19\\
(43.53,& 44.27)\
\end{align*} 

The CDC is 95\% confident that the average age of all the U.S. adults lies between 43.53 and 44.27 years after observing a sample mean of 43.9 years. The CDC notes that 43.55 years is included in this interval, so there is sufficient evidence that the average age of the U.S. adult population could be 43.55 years. 
\end{example}

The connection between confidence intervals and hypothesis testing is this: rejecting a null hypothesis for a two-sided alternative hypothesis corresponds to the null value not falling within the confidence interval. A confidence interval can be used to perform a hypothesis test only if the alternative hypothesis is two-sided and the significance levels for both are equal.
\begin{comment}
\begin{exercise} \label{htForHousingExpenseForCommunityCollege650} \textbf{NEW EXMAPLE}
An investigator is studying the results of standardized IQ tests in adolescents who suffered from severe asthma during childhood. She claims that those who had childhood asthma perform worse than the population average. For the standardized test she will use, the mean score is 100. What are the null and alternative hypotheses to test whether this claim is accurate? \footnote{$H_0$: The average score is 100, $\mu = 100$. \hspace{3.4mm} $H_A$: The average score is lower than 100, $\mu < 100$.}
\end{exercise}

\begin{example}{In her sample of 100 children, she found a sample mean $\overline{x} = 96.7$ and standard deviation $s = 10$. Construct a 95\% confidence interval for the population mean and evaluate the hypotheses of Exercise~\ref{htForHousingExpenseForCommunityCollege650}.}
$$ SE = \frac{s}{\sqrt{n}} = \frac{10}{\sqrt{100}} = 1 $$
The normal model may be applied to the sample mean because the conditions are met: The data are a simple random sample and we assume that there are more than 1,000 adolescents who have suffered from asthma. The observations are independent and the sample size is also sufficiently large (n=100). The sample size mitigates potential effects of outliers even though the distribution is not provided. This ensures a 95\% confidence interval may be accurately constructed:
$$\overline{x}\ \pm\ z^{\star} SE \quad\to\quad 96.7\ \pm\ 1.96 \times  1 \quad \to \quad (94.74, 98.66) $$
Because the null value 100 does not lie within the confidence interval, a population mean score of 100 from all adolescents who suffered from sever asthma during childhood is implausible, and the null hypothesis is rejected. The data provide statistically significant evidence that adolescents who suffered from severe asthma during childhood do perform worse on standardized IQ tests. 
\end{example}


\textbf{NEWWWWW EXAMPLEEEEEEE}
\end{comment}
\subsection{Decision errors}\label{DecisionErrors}

\index{hypothesis testing!decision errors|(}

Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted. The guilty sometimes walk free. 

Scientists similarly can make the wrong conclusions, induced by sampling variation, within hypothesis testing. Unlike the court system, there exists tools necessary to quantify how often such errors are made. There are four possible scenarios among hypothesis test conclusions, which are summarized in Table~\ref{fourHTScenarios}.

\begin{table}[ht]
\centering
\begin{tabular}{l l c c}
& & \multicolumn{2}{c}{\textbf{Test conclusion}} \\
  \cline{3-4}
\vspace{-3.7mm} \\
& & do not reject $H_0$ &  reject $H_0$ in favor of $H_A$ \\
  \cline{2-4}
\vspace{-3.7mm} \\
& $H_0$ true & correct &  Type~1 Error \\
\raisebox{1.5ex}{\textbf{Truth}} & $H_A$ true & Type 2 Error & correct \\
  \cline{2-4}
\end{tabular}
\caption{Four different scenarios for hypothesis tests.}
\label{fourHTScenarios}
\end{table}

Scientists commit a \term{Type~1 Error} when they reject the null hypothesis when $H_0$ is true. A \term{Type~2 Error} is failing to reject the null hypothesis when the alternative is true.

\begin{exercise} \label{whatAreTheErrorTypesInUSCourts}
In a U.S. court, the defendant is either innocent ($H_0$) or  guilty ($H_A$). What does a Type~1 Error represent in this context? What does a Type 2 Error represent? Table~\ref{fourHTScenarios} may be useful. Disregard mistrials and hung juries. \footnote{If the court makes a Type~1 Error, this means the defendant is innocent ($H_0$ true) but wrongly convicted. A Type 2 Error means the court failed to reject $H_0$ (i.e. failed to convict the person) when the defendant was, in fact, guilty ($H_A$ true).}
\end{exercise}

\begin{exercise} \label{howToReduceType1ErrorsInUSCourts}
How could U.S. courts reduce the Type~1 Error rate? How would this influence the Type 2 Error rate?\footnote{To lower the Type~1 Error rate (incorrect convictions), the standard conviction could be raised from ``beyond a reasonable doubt'' to ``beyond a conceivable doubt'' so fewer people would be wrongly convicted. However, there exists a tradeoff. By increasing the threshold, it would make it more difficult to convict people who are truly guilty. By reducing the Type~1 Error rate, the Type 2 Error rate would increase.}
\end{exercise}

\begin{exercise} \label{howToReduceType2ErrorsInUSCourts}
How could Type~2 Error rate be reduced in U.S. courts? What influence would this have on the Type~1 Error rate?\footnote{A Type~2 Error occurs when the U.S. court does not convict a guilty defendant. U.S. courts correctly convicting more guilty individuals would decrease the Type~2 Error. The standard of conviction could be lowered from ``beyond a reasonable doubt'' to ``beyond a little doubt'' to achieve this. Lowering the bar for guilt, however, results in more wrongful convictions, raising the Type~1 Error rate.}
\end{exercise}

\begin{exercise} \label{errorsinHIVTesting}
Consider a person is getting tested for HIV. What do a Type~1 and Type~2 Error represent in this context? \footnote{The person does not have HIV under the null hypothesis. Type~1 Error is if this person does not have HIV but is tested positive for HIV. Type~2 Error would be failing to detect HIV when the patient actually has HIV. }
\end{exercise}

\index{hypothesis testing!decision errors|)}

Exercises~\ref{whatAreTheErrorTypesInUSCourts} and~\ref{howToReduceType2ErrorsInUSCourts} provide an important lesson: if one type of error is reduced, the other error type is generally increases. 

Hypothesis testing is built around using strong evidence to reject or fail to reject the null hypothesis. But what does \emph{strong evidence} mean? In hypothesis testing, a p-value less than $\alpha$ provides strong enough evidence to reject the null hypothesis. This value for $\alpha$, the \term{significance level}, \index{hypothesis testing!significance level}  \marginpar[\raggedright\vspace{-4mm} $\alpha$\\\footnotesize significance\\level of a\\hypothesis test]{\raggedright\vspace{-4mm} $\alpha$\\\footnotesize significance\\level of a\\hypothesis test} is interpreted as the probability that scientists commit a Type 1 Error, first introduced in Section~\ref{hypothesisFramework}. That is, for $\alpha=0.05$, when $H_0$ is true, scientists do not want to incorrectly reject $H_0$ more than 5\% of the time. 

Hypothesis testing can occur in all industries ranging from manufacturing to medical testing. The question remains: which error type do researchers want to reduce the most? The hypotheses and testing implications help determine if reducing Type 1 Error is more or less desirable than reducing Type 2 Error. The context of the hypothesis test will also determine which level of $\alpha$ scientists want to choose. Different significance levels beyond $\alpha = 0.05$ will be discussed in Section~\ref{significanceLevel}, but the choice of $\alpha$ depends heavily on the study's threshold for committing a Type 1 Error and allowing for a Type 2 Error. 

\subsection{Two-sided versus one-sided hypothesis testing: dos and don'ts}
\label{twoSidedTestsWithPValues}

\index{data!school sleep|(}

Students and scientists alike have the urge to reject the null hypothesis. In many instances, the null hypothesis is stated as the status quo. Rejecting the null hypothesis allows scientists, after spending time and resources, to make important contributions to the field with new discoveries.

The conclusion of a hypothesis test depends largely on the p-value and the alternative hypothesis. Determining an alternative hypothesis can get tricky, and the choice between a one-sided and two sided test can be controversial. Most importantly, it is never acceptable to change two-sided tests into one-sided tests after observing the data. The following exercise considers the consequences of switching mid-process.

\begin{exercise} 
Like the example in Section~\ref{onesidedCoalExample}, a second group of researchers randomly sampled 200 miners across the US in 1992. They were interested in whether the dust exposure level experienced b their sample of miners differed from the limit of 2 $\mathrm{mg}/\mathrm{m}^3$. Write the null and alternative hypotheses for this investigation.\footnote{Because the researchers are interested in any difference, they should use a two-sided alternative: $H_0: \mu = 2$, $H_A: \mu \neq 2$.}
\label{2ndCoalHypSetupExercise}
\end{exercise}

\begin{example}{The second group of researchers randomly samples 200 students and finds a mean of $\overline{x}=2.14 \mathrm{mg}/\mathrm{m}^3$ and a standard deviation of $s=1.15 \mathrm{mg}/\mathrm{m}^3$. Does this provide strong evidence against the $H_0$ stated in Exercise~\ref{2ndCoalHypSetupExercise}? Use a significance level of $\alpha=0.05$.}
First, assumptions must be verified. (1) A simple random sample of less than 10\% of all miners in the U.S. means the observations are independent. (2) The sample size is 200, which is greater than 30. (3) Although the distribution about dust exposures is unknown, a sample size of 200 surely allows the sample mean to be normally distributed with outliers and skewness having little effect. 

Next compute the standard error ($SE_{\overline{x}} = \frac{1.15}{\sqrt{200}} = 0.08$) of the estimate and create a picture to represent the p-value. It should resemble Figure~\ref{2ndSchSleepHTExample}. Both tails are shaded since the alternative hypothesis provided in Exercise \ref{2ndCoalHypSetupExercise} is two-sided.

Calculate the tail areas by first finding the upper tail corresponding to $\overline{x}$:
\begin{eqnarray*}
T = \frac{2.14 - 2}{0.08} = 1.75 \quad\stackrel{\text{table or \textsf{R}}}{\rightarrow}\quad \text{right tail}=0.04
\end{eqnarray*}
Because the normal model is symmetric, the lower tail will have the same area as the upper tail. The p-value is found as the sum of the two shaded tails:
\begin{eqnarray*}
\text{p-value} = \text{left tail} + \text{right tail} = 2\times(\text{right tail}) = 0.08
\end{eqnarray*}
This p-value is is larger than $\alpha=0.05$, so $H_0$ should not be rejected. That is, if $H_0$ is true, it would not be very unusual to see a sample mean this far from 2 $\mathrm{mg}/\mathrm{m}^3$ simply due to sampling variation. There is not sufficient evidence to conclude that the mean is different from 2 $\mathrm{mg}/\mathrm{m}^3$. The mines appear to be following the law.

However, consider if this second group of researchers switched to a one-sided alternative hypothesis, $2<\mu$, before the T-statistic was calculated. Keeping the sample and all other values the same, the p-value under a one-sided alternative would be 0.04. This new p-value is less than $\alpha = 0.05$. $H_0$ would be rejected, a different conclusion than with the two-sided alternative. 
\index{data!school sleep|)}
\end{example}

Exercise~\ref{2ndCoalHypSetupExercise} introduces the dangers of switching alternative hypothesis after observing the data, especially when p-values are close to $\alpha$. In practice, research that concludes using a p-value close to $\alpha$ warrants some skepticism. A p-value of 0.048 is not so different from a p-value of 0.052 but results in very different conclusions. The next example shows that freely switching from two-sided tests to one-sided tests will cause researchers to make twice as many Type~1 Errors as intended.

\begin{example}{Consider two cases at the 95\% confidence level where the researchers change to a one-sided alternative hypothesis after observing the data: (1) The sample mean is larger than the null value and (2) the sample mean is smaller than the null value. 

(1) Suppose the sample mean is larger than the null value, $\mu_0$ (e.g. $\mu_0$ would represent~2 if $H_0$:~$\mu = 2$). If the researchers flipped to a one-sided test instead of a two-sided test, they would use $H_A$: $\mu > \mu_0$ after observing the data.\footnote{They opt to minimize the Type~1 Errors} Any T-statistic greater than 1.65 would result in rejecting $H_0$ under a one-sided alternative. If the null hypothesis is true, the large T-statistic would result in incorrectly rejecting the null hypothesis about 5\% of the time when the sample mean is above the null value, as shown in Figure~\ref{type1ErrorDoublingExampleFigure}.

(2) Suppose the sample mean is smaller than the null value. The researchers would then choose $H_A$: $\mu < \mu_0$ if they changed to a one-sided test. If $\overline{x}$ had a T-statistic smaller than -1.65, $H_0$ would be rejected. If the null hypothesis is true, $H_0$ would be incorrectly rejected about 5\% of the time.}

Putting these two scenarios together, if the researchers were free to switch alternative hypotheses after observing the data, the Type~1 Error rate would be $5\%+5\%=10\%$. This is twice the error rate the researchers prescribed with their significance level of 0.05! 

\begin{figure}
   \centering
   \includegraphics[width=0.7\textwidth]{ch_inference_foundations_oi_biostat/figures/type1ErrorDoublingExampleFigure/type1ErrorDoublingExampleFigure}
   \caption{The shaded regions represent areas where we would reject $H_0$ under the bad practices considered in Example~\ref{swappingHypAfterDataDoublesType1ErrorRate} when $\alpha = 0.05$.}
   \label{type1ErrorDoublingExampleFigure}
\end{figure}

\end{example}

The examples and exercises in this text will be obvious enough to decide on an alternative hypothesis. In practice, however, it can be less straightforward. If the sidedness is uncertain, many scientists opt to use a two-sided alternative because it is more \emph{conservative}. What does conservative mean in this context? Exercise~\ref{2ndCoalHypSetupExercise} and the previous example show that the two-sided tests will fail to reject the null hypothesis more often. The p-value for a two-sided alternative is twice as large, producing a "safer" and more conservative hypothesis test conclusion. 

\begin{caution}{One-sided hypotheses are allowed only \emph{before} seeing data}
{After observing data, it is tempting to turn a two-sided test into a one-sided test to have a statistically significant one-sided conclusion. Avoid this temptation. Remember, the direction of a one-sided test must be made a priori, \emph{before} observing the data. If it is not, the test must be two-sided.}
\end{caution}

\subsection{Choosing a significance level}
\label{significanceLevel}

\index{hypothesis testing!significance level|(}
\index{significance level|(}

Choosing a significance level for a hypothesis test is important. Traditionally, the $\alpha$ level is $0.05$. However, it is often helpful to adjust the significance level based on the application and possible consequences of the test's conclusions.

If making a Type~1 Error is dangerous or especially costly, a small significance level (e.g. smaller than 0.05) should be chosen: the study should demand very strong evidence favoring $H_A$ before $H_0$ is rejected. Many would use $\alpha=0.01$ or some value similar in this situation. 

If a Type 2 Error is more dangerous or more costly than a Type~1 Error, a higher significance level (e.g. 0.10) should be chosen. Hypothesis tests will be cautious of failing to reject $H_0$ when the null is actually false.  Section~\ref{sampleSizeAndPower} will discuss this case in more detail.

\begin{tipBox}{\tipBoxTitle[]{Significance levels should reflect consequences of errors}
The significance level selected for a test should reflect the consequences associated with Type~1 and Type 2 Errors.}
\end{tipBox}

\begin{example}{A medical machine manufacturer is considering a higher quality but more expensive supplier for MIR machine parts. The engineers sample and test a number of parts from the current supplier and parts from the new supplier. They decide that if the high quality parts will last more than 10\% longer, it makes financial sense to switch to this more expensive supplier. Is there good reason to modify the significance level in such a hypothesis test?}
The null hypothesis is that the more expensive parts last no more than 10\% longer while the alternative is that they do last more than 10\% longer. This decision is just one of the many regular factors that impact the construction of the MRI and the company's financial health. A significance level of 0.05 seems reasonable since neither a Type~1 or Type 2 error should be dangerous or (relatively) much more expensive. The machine's accuracy will not be affected by a change in supplier.
\end{example}

\begin{example}{The same MRI manufacturer is considering a slightly more expensive supplier for parts that improve the accuracy not longevity. If the accuracy of the machine's components is shown to be better than the current supplier, they will switch manufacturers. Is there good reason to modify the significance level from $\alpha=0.05$ in such an evaluation?}
The null hypothesis would be that the suppliers' parts are equally reliable and equally accurate in detection. Because safety is involved, the MRI machine company should be eager to switch to the slightly more expensive manufacturer (reject $H_0$) even if the evidence of increased effectiveness is only moderately strong. A slightly larger significance level, such as $\alpha=0.10$, might be appropriate.
\end{example}

\begin{exercise}
A specific part inside of the MRI machine is very expensive to replace. However, the machine usually functions properly even if this part is broken; the machine detects the most common injuries at the same probability with this broken part. The part is replaced only if the doctors are extremely certain it is broken based on a series of measurements. Identify appropriate hypotheses for this test (in plain language) and suggest an appropriate significance level.\footnote{The null hypothesis is that the part is not broken, and the alternative is that it is broken. If there is not sufficient evidence to reject $H_0$, the part will not be replaced. Failing to fix the part if it is broken ($H_0$ false, $H_A$ true) is not severely problematic for common injuries, and replacing the part is expensive. Very strong evidence against $H_0$ should exist before the manufacturers replace the part. The engineers should choose a small significance level, such as $\alpha=0.01$.}
\end{exercise}

\index{significance level|)}
\index{hypothesis testing!significance level|)}
\index{hypothesis testing|)}

%__________________

\section{A Primer on the $t$-distribution (Special Topic)}
\label{tdistribution}

Section~\ref{hypothesisTesting} introduced the T-statistic to follow a $t$-distribution. Section~\ref{pValue} offered a rule of thumb that for $n\geq30$, the p-value of a $t$-distribution becomes almost indistinguishable from the p-value of a normal distribution, however, the characteristics of a $t$-distribution have yet to be explained. The properties of the $t$-distribution are not needed to perform hypothesis testing. However this section introduces the $t$-distribution and its characteristics at a more theoretical level.

\subsection{Introducing the $t$-distribution}
\label{introducingTheTDistribution}

\index{t-distribution|(}
\index{distribution!$t$|(}

In cases where a small sample ($n<30$) is used to calculate the T-statistic, the $t$-distribution is a useful distribution for inference calculations. A $t$-distribution, shown as a solid line in Figure~\ref{tDistCompareToNormalDist}, has a bell shape. However, its tails are thicker than the tails in a normal model, and observations are more likely to fall beyond two standard deviations from the mean than under the normal distribution.\footnote{It is useful to think of the $t$-distribution as having a standard deviation of 1 even though the standard deviation of the $t$-distribution is slightly more than 1.} These extra thick tails of the $t$-distribution ``correct'' the problem of a poorly estimated standard error from a small sample.

\begin{figure}
\centering
\includegraphics[width = \textwidth]{ch_inference_foundations_oi_biostat/figures/tDistCompareToNormalDist/tDistCompareToNormalDist}
\caption{Comparison of a $t$-distribution (solid line) to a normal distribution (dotted line).}
\label{tDistCompareToNormalDist}
\end{figure}

The $t$-distribution, always centered at zero, has a single parameter: degrees of freedom. The \termsub{degrees of freedom (df)}{degrees of freedom (df)!$t$-distribution} describe the precise form of the bell-shaped $t$-distribution. Several $t$-distributions are shown in Figure~\ref{tDistConvergeToNormalDist}. The $t$-distribution has $(n-1)$ degrees of freedom where $n$ is the sample size from the one sample used to calculate the T-Statistic. \footnote{For T-statistics calculated from two samples, the degrees of freedom formula is much more complicated.} With more degrees of freedom, the $t$-distribution looks very much like the standard normal distribution. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/tDistConvergeToNormalDist/tDistConvergeToNormalDist}
\caption{The larger the degrees of freedom, the more closely the $t$-distribution resembles the standard normal model.}
\label{tDistConvergeToNormalDist}
\end{figure}

\begin{termBox}{\tBoxTitle{Degrees of freedom (df)}
The degrees of freedom describe the shape of the $t$-distribution. The larger the degrees of freedom, the more closely the distribution approximates the normal model.}
\end{termBox}

When the degrees of freedom is about 30 or more, the $t$-distribution is nearly indistinguishable from the normal distribution. 

It's very useful to become familiar with the $t$-distribution. The $t$-distribution allows forgreater flexibility than the normal distribution when analyzing numerical data. The \term{t-table}, partially shown in Table~\ref{tTableSample}, is used in place of the normal probability table. A larger $t$-table is in Appendix~\ref{tDistributionTable} on page~\pageref{tDistributionTable}. In~practice, it's more common to use statistical software like \textsf{R} instead of a table to calculate tail areas. 
\begin{table}[hht]
\centering
\begin{tabular}{r | rrr rr}
one tail & \hspace{1.5mm}  0.100 & \hspace{1.5mm} 0.050 & \hspace{1.5mm} 0.025 & \hspace{1.5mm} 0.010 & \hspace{1.5mm} 0.005  \\
two tails & 0.200 & 0.100 & 0.050 & 0.020 & 0.010 \\
\hline
{$df$} \hfill 1  &  {\normalsize  3.08} & {\normalsize  6.31} & {\normalsize 12.71} & {\normalsize 31.82} & {\normalsize 63.66}  \\ 
2  &  {\normalsize  1.89} & {\normalsize  2.92} & {\normalsize  4.30} & {\normalsize  6.96} & {\normalsize  9.92}  \\ 
3  &  {\normalsize  1.64} & {\normalsize  2.35} & {\normalsize  3.18} & {\normalsize  4.54} & {\normalsize  5.84}  \\ 
$\vdots$ & $\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ & \\
17  &  {\normalsize  1.33} & {\normalsize  1.74} & {\normalsize  2.11} & {\normalsize  2.57} & {\normalsize  2.90}  \\ 
\highlightO{18}  &  \highlightO{\normalsize  1.33} & \highlightO{\normalsize  1.73} & \highlightO{\normalsize  2.10} & \highlightO{\normalsize  2.55} & \highlightO{\normalsize  2.88}  \\ 
19  &  {\normalsize  1.33} & {\normalsize  1.73} & {\normalsize  2.09} & {\normalsize  2.54} & {\normalsize  2.86}  \\ 
20  &  {\normalsize  1.33} & {\normalsize  1.72} & {\normalsize  2.09} & {\normalsize  2.53} & {\normalsize  2.85}  \\ 
$\vdots$ & $\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ & \\
400  &  {\normalsize  1.28} & {\normalsize  1.65} & {\normalsize  1.97} & {\normalsize  2.34} & {\normalsize  2.59}  \\ 
500  &  {\normalsize  1.28} & {\normalsize  1.65} & {\normalsize  1.96} & {\normalsize  2.33} & {\normalsize  2.59}  \\ 
$\infty$  &  {\normalsize  1.28} & {\normalsize  1.64} & {\normalsize  1.96} & {\normalsize  2.33} & {\normalsize  2.58}  \\ 
\end{tabular}
\caption{An abbreviated look at the $t$-table. Each row represents a different $t$-distribution. The columns describe the cutoffs for specific tail areas. The row with $df=18$ has been \highlightO{highlighted}.}
\label{tTableSample}
\end{table}

Each row in the $t$-table represents a $t$-distribution with different degrees of freedom. The columns correspond to tail probabilities. For instance, if $df=18$, researchers can examine row 18, highlighted in Table~\ref{tTableSample}. If researchers want to identify the cutoff for an upper tail area of 10\%, they can look in the column where \emph{one tail} is 0.100. This cutoff is 1.33. If they had wanted the cutoff for the lower 10\%, they would find -1.33. The $t$ distribution is symmetric just like the normal distribution.

\begin{example}{What proportion of the $t$-distribution with 18 degrees of freedom falls below -2.10?}
Just like a normal probability problem, Figure~\ref{tDistDF18LeftTail2Point10} provides a picture with the shaded area below -2.10. To find this area, the appropriate row needs to be identified in the $t$-table: \mbox{$df=18$}. 
Identify the column containing the absolute value of -2.10; it~is the third column. The shaded area is only one tail. One tail area for a value in the third row corresponds to 0.025. About 2.5\% of the $t$-distribution with 18 degrees of freedom falls below -2.10. The next example encounters a case where the exact $t$ value is not listed, and \textsf{R} would be more useful. 
\end{example}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{ch_inference_foundations_oi_biostat/figures/tDistDF18LeftTail2Point10/tDistDF18LeftTail2Point10}
\caption{The $t$-distribution with 18 degrees of freedom. The area below -2.10 has been shaded.}
\label{tDistDF18LeftTail2Point10}
\end{figure}

\begin{example}{A $t$-distribution with 20 degrees of freedom is shown in the left panel of Figure~\ref{tDistDF20RightTail1Point65}. Estimate the proportion of the distribution falling above 1.65.}
Look at the $t$-table again, identify the row using the degrees of freedom: $df=20$. Then look for 1.65; it is not listed. It falls between the first and second columns. The tail area of a 1.65 cutoff is bounded by 0.05 and 0.1. Between 5\% and 10\% of the distribution is more than 1.65 standard deviations above the mean of a $t$-distribution with 20 degrees of freedom. This is, however, a wide band. Statistical software like \textsf{R} can provide more precision \begin{verbatim}
> 1-pt(1.65,20)
[1] 0.05728041
\end{verbatim}
where \var{pt()} calculates the shaded area left of 1.65 of a $t$-distribution with 20 degrees of freedom. The shaded area in the upper tail is 0.0573. 
\end{example}

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{ch_inference_foundations_oi_biostat/figures/tDistDF20RightTail1Point65/tDistDF20RightTail1Point65}
\caption{Left: The $t$-distribution with 20 degrees of freedom, with the area above 1.65 shaded. Right: The $t$-distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded.}
\label{tDistDF20RightTail1Point65}
\end{figure}

\begin{example}{A $t$-distribution with 2 degrees of freedom is shown in the right panel of Figure~\ref{tDistDF20RightTail1Point65}. Estimate the proportion of the distribution falling more than 3 units from the mean (above or below).}
As before, first identify the appropriate row: $df=2$. Next, find the columns that capture 3; because $2.92 < 3 < 4.30$, use the second and third columns. Finally, 0.05 and 0.1 are the bounds for the tail areas under ``two tail'' since both tails are shaded in Figure~\ref{tDistDF20RightTail1Point65}. For more precision, \textsf{R} outputs 
\begin{verbatim} > 2*pt(-3, 2)
[1] 0.09546597
\end{verbatim}
\end{example}

\begin{exercise}
What proportion of the $t$-distribution with 19 degrees of freedom falls above -1.79 units?\footnote{Shade area \emph{above} -1.79 on a symmetric distribution (the picture is left as an additional exercise). The small unshaded left tail is between 0.025 and 0.05, so the larger upper region must have an area between 0.95 and 0.975 (1 - the small left tail area). Using \textsf{R} again, > 1-pt(-1.79, 19) [1] 0.9552978}

\index{distribution!$t$|)}
\index{t-distribution|)}

\end{exercise}

\subsection{Conditions for using the $t$-distribution for inference on a sample mean}
\label{tDistSolutionToSEProblem}

To infer on a single mean using the $t$-distribution, two conditions need to be checked. These assumptions are the same for a normal distribution.
\begin{description}
\item[Independence of observations.] This condition was verified before in Section~\ref{changingTheConfidenceLevelSection} as a condition that $\overline{x}$ is nearly normal. The sample should be taken from less than 10\% of the population as a simple random sample. If the data are from an experiment or random process, the observations should be independent. 
\item[Observations come from a nearly normal distribution.] This second condition is difficult to verify with small data sets. To diagnose (i) take a look at a plot of the data for obvious departures from the normal model, and (ii) consider any previous experiences in which the data may not be nearly normal. \footnote{A large enough sample size can sometimes counteract concerns with skewness and make this assumption more lenient.}
\end{description}

\begin{tipBox}{\tipBoxTitle{When to use the $t$-distribution}
Use the $t$-distribution to infer on the sample mean when observations are independent and nearly normal. The nearly normal condition is relaxed as the sample size increases. For example, the data distribution may be moderately skewed when the sample size is at least~30.}
\end{tipBox}

If the T-statistic is modeled after a $t$-distribution instead of a normal distribution, the tools introduced in Chapter~\ref{foundationsForInference} proceed exactly the same except now, \emph{use the $t$-distribution with $n-1$ degrees of freedom} to determine $z^{\star}$ and calculate the p-value for confidence intervals and hypothesis tests.


%__________________

\section{Examining the Central Limit Theorem closer (Special Topic)}
\label{cltSection}

\index{Central Limit Theorem|(}

The normal model for the sample mean tends to be very good when the sample consists of at least 30 independent observations and the population data are not strongly skewed. The Central Limit Theorem, informally defined in Section~\ref{why30}, provides the theory that allows this assumption to be made.

\begin{termBox}{\tBoxTitle{Central Limit Theorem, informal definition}
The distribution of $\overline{x}$ is approximately normal. The approximation can be poor if the sample size is small. However as $n$ increases, the sample mean becomes more and more normally distributed.}
\end{termBox}

Three theoretical cases will be considered to see roughly when the approximation is reasonable. Consider three data sets: one from a \emph{uniform} distribution, one from an \emph{exponential} distribution, and the other from a \emph{log-normal} distribution. Recall the properties of these distributions from Chapter~\ref{modeling}. These distributions are shown in the top panels of Figure~\ref{cltSimulations}. The uniform distribution is symmetric, the exponential distribution may be considered moderately skewed since its right tail is relatively short (few outliers), and the log-normal distribution is strongly skewed and will tend to produce more apparent outliers.\index{skew!example: moderate}\index{skew!example: strong}

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/cltSimulations/cltSimulations}
   \caption{Sampling distributions for the sample mean at different sample sizes. The samples are drawn from three different distributions. The dashed red lines show normal distributions for approximation.}
   \label{cltSimulations}
\end{figure}

The $n=2$ row represents the sampling distribution of $\overline{x}$ when the sample is drawn from each distribution and contains two observations. The The $n=30$ row represents the sampling distribution where each sample contains 30 observations. The dashed line in red represents the closest approximation of the normal distribution. 

\begin{exercise}
Examine the distributions in each row of Figure~\ref{cltSimulations} and the normal approximation overlay. As the sample size becomes larger, how does the normal approximation for each sampling distribution compare? \footnote{The normal approximation becomes better as larger samples are used across all distributions.}
\end{exercise}

\begin{example}{Would the normal approximation be good in all applications where the sample size is at least 30?}
Not necessarily. The normal approximation for the log-normal example is questionable even at a sample size of 30. The more skewed a population distribution or the more common the frequency of outliers, the larger the sample required to adequately approximate the distribution of the sample mean to a normal distribution.
\end{example}

\begin{tipBox}{\tipBoxTitle{With larger $n$, the sampling distribution of $\overline{x}$ becomes more normal}
As the sample size increases, the normal model for $\overline{x}$ becomes more reasonable. The condition in skewness can be relaxed when the sample size is very large.}
\end{tipBox}


\begin{example}{Figure~\ref{pertussisTS} displays a histogram of 169 observations from a 2015 study published in \emph{Parasitology}\footnote{Magpantay FMG, Domenech de Cells M, Rohani P, King AA (2015) Pertussis immunity and epidemiology: mode and duration of vaccine-induced immunity. Parasitology, online in advance of print. \url{http://dx.doi.org/10.1017/S0031182015000979}} on pertussis immunity.\footnote{whooping cough} This study investigated the nature and duration of vaccine protection to explain the resurgence of pertussis in countries with high vaccination coverage. The data, provided by the Italian Ministry of Health, is the number of monthly pertussis notification reports from 1996 to 2009 in selected regions in Italy. Figure~\ref{pertussisTS} and figure~\ref{pertussisHist} both use data from the Italian region of Lombardia. Can the sampling distribution of the sample mean, 23.07, be approximated to a normal or $t$-distribution?}
Conditions first need to be verified. 
These data are referred to as \term{time series data} or data that appears through time arriving in a particular sequence. 
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[(1)] Condition of independence: Consider two observations, the number of reports in March 2008 and the number of reports in April 2008. Are these independent? 
If there were many Italians who contracted pertussis in one month, it would highly influence how many people would contract pertussis the next month. Pertussis is a highly contagious bacterial disease, so the number of monthly reports would presumably increase the month after many individuals contracted pertussis. Similarly if many Italians were vaccinated in March, fewer people would be at risk of pertussis in April. To make the assumption of independence, careful checks should be performed on such data. In many instances like this one time series data are not independent. 
\item[(2)] Condition of size: The sample size is 169. This is greater than 30. 
\item[(3)] Condition of skewness: Figure~\ref{pertussisHist} suggests that the data are very strongly skewed or very distant outliers may be common for this type of data. Outliers can play an important role and affect the distribution of the sample mean and the estimate of the standard error.
\end{itemize}
A normal distribution should not model the sample mean of these 169 observations because of the independence assumption. The very extreme skewness in Figure~\ref{pertussisHist} also poses a challenge but the sample size is sufficiently large. It cannot be assumed that the sample mean is distributed normally. \end{example}

\begin{figure}[ht]
   \centering
   \includegraphics[width = \textwidth]{ch_inference_foundations_oi_biostat/figures/pertussisHist/pertussisHist}
   \caption{Histogram of the number of monthly reports every month from 1996 to 2009. These data include obvious skewness. These are problematic when considering the normality of the sample mean. \index{skew!example: very strong}.}
   \label{pertussisHist}
\end{figure}

\begin{figure}[ht]
   \centering
   \includegraphics[width = \textwidth]{ch_inference_foundations_oi_biostat/figures/pertussisTS/pertussisTS}
   \caption{This is a plot depicting the time series, the number of monthly reports over time. Time series data is generally plotted like this because it is easy to see how values change over time.}
   \label{pertussisTS}
\end{figure}

\begin{caution}
{Examine data structure when considering independence}
{Some data sets are collected in such a way that they have a natural underlying structure between observations, e.g. when observations occur consecutively or they come from the same family etc. Be especially cautious about independence assumptions regarding such data sets.}
\end{caution}

\begin{caution}
{Watch out for strong skew and outliers}
{Strong skew is often identified by the presence of clear outliers. If a data set has prominent outliers, or such observations are somewhat common for the type of data under study, then it is useful to collect a sample with many more observations than 30 if the normal distribution will be used to model $\overline{x}$. There are no simple guidelines for what sample size is big enough for varying situations of skewnss, so proceed with caution when working in the presence of strong skew or more extreme outliers.}
\index{skew!strongly skewed guideline}
\index{Central Limit Theorem|)}
\end{caution}

%__________________
\section{Inference for other estimators}
\label{aFrameworkForInference}

The sample mean is not the only point estimate for which the sampling distribution is nearly normal, and the procedures introduced in Chapter~\ref{foundationsForInference} are not limited to just estimating the population mean. 

The sampling distribution of sample proportions closely resembles the normal distribution when the sample size is sufficiently large. In this section, a number of examples will be introduced where the normal approximation is reasonable and procedures in this chapter can be applied to inference on other estimators like a sample proportion. Chapters~\ref{inferenceForNumericalData} and~\ref{inferenceForCategoricalData} will revisit each of the point estimates introduced in this section along with some other new statistics.

In order to use the tools introduced in Chapter~\ref{foundationsForInference}, these point estimates need to obey some characteristics and assumptions. In addition to the assumptions encountered in this section of independence, size and skewness, the estimates also need to be unbiased. A point estimate is \term{unbiased} if the sampling distribution of the estimate is centered at the parameter it estimates. A biased point estimate consistently is too high, or estimates are always too low. That is, an unbiased estimate does not naturally over or underestimate the parameter. The sample mean is an example of an unbiased point estimate, as are each of the examples introduced in this section.

\subsection{Confidence intervals for nearly normal point estimates}

\index{confidence interval!using normal model|(}

In Section~\ref{confidenceIntervals}, the point estimate $\overline{x}$ with standard error $SE_{\overline{x}}$ was used to create a 95\% confidence interval for the population mean using the following formula:
\begin{align}
\overline{x}\ \pm\ 1.96 \times SE_{\overline{x}}
\label{95PercCIForMeanInGeneralizingSection}
\end{align}
This interval was constructed by noting that the observations are is within 1.96 standard errors of the mean 95\% of the time. This same logic can be used for any normally distributed and unbiased point estimate to calculate a confidence interval at any confidence level.

\begin{termBox}{\tBoxTitle{General confidence interval for the normal sampling distribution case}\label{generalConfidenceIntervalTermBox}%
For any unbiased point estimate, the confidence interval for a nearly normal point estimate is
\begin{eqnarray}
\text{point estimate}\ \pm\ z^{\star}SE
\label{95PercGeneralCIInGeneralizingSection}
\end{eqnarray}
$z^{\star}$ is selected to correspond to the confidence level, and $SE$ represents the standard error. $z^{\star}SE$ is called the \emph{margin of error}\index{margin of error}.}
\end{termBox}

In this section, the computed standard error for each example and exercise is provided without detailing where derivation. Chapter~\ref{inferenceForNumericalData} will provide the formulae and elaborate on other details for each type of point estimate.

\begin{example}{The CDC is interested in estimating the average difference in weights between U.S. men and women using \data{BRFSS BMI} as the sample. The point estimate for the average difference in weights between men and women is $\overline{x}_\mathrm{{men}}-\overline{x}_\mathrm{{women}}= 36.61$ pounds. This point estimate is associated with a nearly normal distribution with SE = 0.35 pounds.\footnote{The calculation for the standard error is quite involved and presented in Chapter~\ref{inferenceForNumericalData}.} What is a reasonable 95\% confidence interval for the difference in gender weights?}
\label{confIntervalForDifferenceOfRunTimeBetweenGenders}
The normal approximation is said to be valid, so apply Equation~\eqref{95PercGeneralCIInGeneralizingSection}:
\begin{eqnarray*}
\text{point estimate}\ \pm\ z^{\star} SE
	\quad\rightarrow\quad 36.61\ \pm\ 1.96\times 0.35
	\quad\rightarrow\quad (35.91, 37.31)
\end{eqnarray*}
The CDC is 95\% confident that men were, on average, between 35.91 to 37.31 pounds heavier than women. That is, the average difference among the weights of all men and women in the U.S. is plausibly between 35.91 and 37.31 pounds with 95\% confidence.
\end{example}

\begin{example}{Does Example~\ref{confIntervalForDifferenceOfRunTimeBetweenGenders} guarantee that if a husband and wife both weighed themselves, the husband would weigh between 35.91 and 37.31 pounds more than the wife?}
The confidence interval above says absolutely nothing about individual observations or specific pairs of observations. It \emph{only} makes a statement about a plausible range of values for the \emph{average} difference between all U.S. men and women.
\end{example}

\begin{exercise}
The proportion of men in the \data{BRFSS BMI} sample is $\hat{p}=0.42$. This sample meets certain conditions that ensure $\hat{p}$ will be nearly normal, and the standard error of the estimate is $SE_{\hat{p}}=0.004$. Create a 90\% confidence interval for the proportion of men in the U.S. adult population. \footnote{Use $z^{\star}=1.65$, and apply the general confidence interval formula:
\begin{eqnarray*}
\hat{p}\ \pm\ z^{\star}SE_{\hat{p}}
	\quad\to\quad 0.42\ \pm\ 1.65\times 0.004
	\quad\to\quad (0.415, 0.427)
\end{eqnarray*}
Thus, the CDC is 90\% confident that between 41.5\% and 42.7\% of the U.S. adult population are men.}
\index{confidence interval!using normal model|)}
\end{exercise}

\subsection{Hypothesis testing for nearly normal point estimates}
\index{hypothesis testing!using normal model|(}

Hypothesis testing can also apply to estimates that are unbiased and nearly normal. The following examples will only use the p-value approach introduced in Section~\ref{pValue} and forgo the critical value shortcut.  

\begin{termBox}{\tBoxTitle[]{Hypothesis testing framework for point estimates distributed normally}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item First write the hypotheses in plain language, then in mathematical notation using the appropriate point estimate and population parameter.
\item State a significance level $\alpha$. Generally use $\alpha=0.05$. 
\item Compute the test-statistic using the point estimate and standard error estimate. 
\item Calculate the p-value by drawing a picture of the sampling distribution under $H_0$. Know which area, according to the alternative, is being shaded to represent the correct p-value . 
\item Use the p-value to evaluate the hypotheses. Write a conclusion within the context of the problem. 
\end{enumerate}
} Conditions must be verified to ensure that the point estimate is nearly normal and unbiased so that the standard error estimate is reasonable. This step can be done before computing the test-statistic.  
\end{termBox}

\begin{exercise} \label{fdaHypSetupForSulph}
A drug called sulphinpyrazone is under consideration to reduce the death rate in heart attack patients. To determine whether the drug was effective, a set of 1,475 patients were recruited into an experiment and randomly split into two groups: a control group that received a placebo and a treatment group that received the new drug. What would be an appropriate null and alternative hypotheses?\footnote{The skeptic's perspective is that the drug does not work at reducing deaths in heart attack patients ($H_0$), while the alternative is that the drug does work ($H_A$).}
\end{exercise}

Formalize the hypotheses from Exercise~\ref{fdaHypSetupForSulph} by letting $p_{control}$ and $p_{treatment}$ represent the proportion of patients who died in the control and treatment groups, respectively. Then the hypotheses can be written as
\begin{eqnarray*}
&&H_0: p_{control} = p_{treatment} \quad\text{(the drug does not work)} \quad \\
&&H_A: p_{control} > p_{treatment} \quad\text{(the drug works)}
\end{eqnarray*}
or equivalently,
\begin{eqnarray*}
&&H_0: p_{control} - p_{treatment} = 0 \quad\text{(the drug does not work)} \quad \\
&&H_A: p_{control} - p_{treatment} > 0 \quad\text{(the drug works)}
\end{eqnarray*}
Strong evidence against the null hypothesis and in favor of the alternative would correspond to an observed difference in death rates,
\begin{eqnarray*}
\text{point estimate} = \hat{p}_{control} - \hat{p}_{treatment}
\end{eqnarray*}
being larger than would be expected from chance alone. The difference in sample proportions is a useful point estimate to evaluate the hypotheses. Note the alternative hypothesis is written to show a positive difference, not just any difference. 

\begin{example}{Evaluate the hypothesis setup from Exericse~\ref{fdaHypSetupForSulph} using data from the study.\footnote{Anturane Reinfarction Trial Research Group. 1980. Sulfinpyrazone in the prevention of sudden death after myocardial infarction. New England Journal of Medicine 302(5):250-256.} In the control group, 60 of 742 patients died. In the treatment group, 41 of 733 patients died. The sample difference in death rates can be summarized as
\begin{eqnarray*}
\text{point estimate} = \hat{p}_{control} - \hat{p}_{treatment} = \frac{60}{742} - \frac{41}{733} = 0.025
\end{eqnarray*}
This point estimate is nearly normal and is an unbiased estimate. The standard error of this sample difference is $SE = 0.013$. Evaluate the hypothesis test at a 5\% significance level: $\alpha=0.05$.}
The hypotheses and significance level have already been stated. Next, calculate the T-statistic. 
\begin{eqnarray}
Z = \frac{\text{point estimate} - \text{null value}}{SE_{\text{point estimate}}}
	= \frac{0.025 - 0}{0.013} = 1.92
\label{zScoreOfPointEstimateForSulphinpyrazoneThisIsFirstTestStatReference}
\end{eqnarray}
Identify the p-value to evaluate the hypotheses. Under the null hypothesis, the sampling distribution is centered at zero since $p_{control}-p_{treatment}=0$ as shown in Figure~\ref{sulphStudyFindPValueUsingNormalApprox}. The p-value is represented by the shaded upper tail. The lower tail does not need to be shaded as this is a one-sided test: a large negative difference in the lower tail does not support the alternative hypothesis and is not evidence against the null hypothesis.

\begin{figure}[bt]
   \centering
   \includegraphics[height=37mm]{ch_inference_foundations_oi_biostat/figures/sulphStudyFindPValueUsingNormalApprox/sulphStudyFindPValueUsingNormalApprox}
   \caption{The distribution of the sample difference if the null hypothesis is true.}
   \label{sulphStudyFindPValueUsingNormalApprox}
\end{figure}

The lower unshaded tail is about 0.973 according to the normal probability table. The upper shaded tail representing the p-value is
\begin{eqnarray*}
\text{p-value} = 1-0.973 = 0.027
\end{eqnarray*}
The p-value is less than the significance level ($\alpha=0.05$), so the null hypothesis is implausible. That is, the null hypothesis is rejected in favor of the alternative, and the researchers conclude with 95\% confidence that the drug is effective at reducing deaths in heart attack patients.
\end{example}

\subsection{Non-normal point estimates}

The ideas of confidence intervals and hypothesis testing may be applied to cases where the point estimate or test statistic is not necessarily normal. There are many reasons why such a situation may arise:
\begin{itemize}
\setlength{\itemsep}{0mm}
\item the sample size is too small for the normal approximation to be valid;
\item the standard error estimate may be poor; or
\item the point estimate tends towards some distribution that is not the normal distribution.
\end{itemize}
For each case where the normal approximation is not valid, the first task is always to understand and characterize the sampling distribution of the point estimate or test statistic. Next, apply the general frameworks for confidence intervals and hypothesis testing to these alternative distributions if possible. Later chapters and other texts will elaborate on this topic and provide guidance on how to infer from these point estimates. 

\subsection{When to retreat}
\label{whenToRetreat}

Statistical tools rely on conditions and assumptions. When these are not met, the inference tools introduced in Chapter~\ref{foundationsForInference} are unreliable. Drawing conclusions from them becomes treacherous. The assumptions typically come in two forms.
\begin{itemize}
\setlength{\itemsep}{0mm}
\item \textbf{The individual observations must be independent.} A random sample from less than 10\% of the population ensures the observations are independent. In experiments, subjects are randomized into groups. If independence fails, then advanced techniques must be used, and in some such cases, inference may not be possible.
\item \textbf{Other conditions focus on sample size and skew.} If the sample size is too small, the skew too strong, or extreme outliers are present, then the normal model for the sample mean will be a poor fit.
\end{itemize}
Verification of conditions and assumptions is always necessary and not limited to inference procedures. 

Whenever conditions are not satisfied, there are three options. The first is to learn new methods that are appropriate for the data. The second route is to consult a statistician. The third route is to ignore the failure of conditions. This last option effectively invalidates any analysis and may discredit novel and interesting findings.

Finally with caution, there may not exist any useful inference tools  when data includes unknown biases, such as convenience samples. For this reason, there are books, courses, and researchers devoted to the techniques of sampling and experimental design. See Sections~\ref{overviewOfDataCollectionPrinciples}-\ref{experimentsSection} for basic principles of data collection.

%__________________
\section{Sample size and power (Special Topic)}
\label{sampleSizeAndPower}

Before it performs any inference, the CDC needs to establish its methodology on gathering data, and determine the amount of resources it wants to spend collecting the data. Finally the CDC must determine how many people, $n$, to gather data from. Sampling and post-processing the data can be extremely costly. Sample size and a test's power\footnote{$P(\text{correctly rejecting the null hypothesis when it is false})$} go hand in hand.  

The Type 2 Error rate and the magnitude of such error are controlled by the sample size. \footnote{Remember the margin of error comes from the confidence interval (point estimate $\pm$ margin of error )where the margin of error = $z^{\star} \cdot SE$ for a certain confidence level} Real differences from the null value, even large ones, may be difficult to detect with small samples. If a very large sample is taken, statistically significant differences can be detected, but the magnitude might be so small that it is of no practical value. This section will describe techniques for selecting an appropriate sample size based on these considerations.

\subsection{Finding a sample size for a certain margin of error}
\label{findingASampleSizeForACertainME}

\index{margin of error|(}

Many companies are concerned about rising healthcare costs. A company can estimate its employees' health characteristics, such as blood pressure, to project its future cost obligations in health insurance. However, if the company is large, it might be too expensive to measure the blood pressure of every employee. The company may choose to take a sample instead.

\begin{example}{Blood pressure oscillates with the beating of the heart, and the systolic pressure is defined as the peak pressure when a person is at rest. The average systolic blood pressure for people in the U.S. is about 130 mmHg with a standard deviation of about 25 mmHg. How large of a sample is necessary to estimate the average systolic blood pressure with a margin of error of 4 mmHg using a 95\% confidence level?}
\label{sampleSizeComputationForSystolicBloodPressure}
First, recall that the margin of error, $z^{\star} \times SE$, is the part that is added and subtracted from the point estimate when computing a confidence interval. Assuming the company is large, $n\geq 30$, 1.96 is used as the critical value for a 95\% confidence level. \footnote{Other assumptions should be verified as well: normality, independence etc.} The margin of error for a 95\% confidence interval estimating the population mean can be written as
\begin{align*}
ME_{95\%} = 1.96\times SE = 1.96\times\frac{\sigma_{employee}}{\sqrt{n}}
\end{align*}
The challenge in this case is to find the sample size $n$ so that this margin of error is less than or equal to 4. This problem is written as an inequality:
\begin{align*}
1.96\times \frac{\sigma_{employee}}{\sqrt{n}} \leq 4
\end{align*}
The company needs to solve for the appropriate value of $n$, but $\sigma_{employee}$ is unknown. They haven't collected any data yet. There is no direct estimate! The company, instead, uses the best estimate available, the approximate standard deviation for the U.S. population, 25 mmHg, and proceeds to solve for $n$:
\begin{align*}
1.96\times \frac{\sigma_{employee}}{\sqrt{n}} \approx 1.96\times\frac{25}{\sqrt{n}}
	&\leq 4 \\
1.96\times\frac{25}{4} &\leq \sqrt{n} \\
\left(1.96\times\frac{25}{4}\right)^2 &\leq n \\
150.06 &\leq n
\end{align*}
This suggests that the sample size should consist of at least 151 employees. The company rounds up because the sample size must be \emph{greater than or equal to 150.06} to ensure a margin of error of 4.
\end{example}

Replacing the employee standard deviation for the U.S. standard deviation in Example~\ref{sampleSizeComputationForSystolicBloodPressure} is potentially controversial. In cases where the standard deviation for the sample is unknown and the sample has yet to be taken, practicing statisticians review scientific literature or market research to make an educated guess. Then they can make a reasonable substitution to calculate the standard error. \footnote{Substitutions and assumptions like these also induce random variation that the hypotheses will be rejected incorrectly. More on this follows Section~\ref{powerType2}.}

\begin{termBox}{\tBoxTitle{Identify a sample size for a particular margin of error}
To estimate the necessary sample size for a maximum margin of error $m$, set up an equation to represent this relationship:
\begin{align*}
m \geq ME = z^{\star}\frac{\sigma}{\sqrt{n}}
\end{align*}
where $z^{\star}$ is the critical value corresponding to the desired confidence level for a nearly normal point estimate, and $\sigma$ is the standard deviation associated with the population. Solve for the sample size,~$n$.\\
If the point estimate is believed not to be nearly normal, use $q^{\star}$ from the $t$-distribution instead. In practice, a nearly normal point estimate is used more often than not.}
\end{termBox}

Calculating for sample size is helpful in planning data collection and requires careful forethought. The Type 2 Error rate is considered next, an important topic in planning data collection and setting a sample size.

\index{margin of error|)}


\subsection{Power and the Type 2 Error rate}
\label{powerType2}

Consider the following two hypotheses:
\begin{itemize}
\setlength{\itemsep}{0.5mm}
\item[$H_0$:] The average blood pressure of employees is the same as the national average, $\mu = 130$.
\item[$H_A$:] The average blood pressure of employees is different than the national average, $\mu \neq 130$.
\end{itemize}
Suppose the alternative hypothesis is actually true. Then the large company might like to know, what is the chance that they make a Type 2 Error? That is, what is the chance that the company fails to reject the null hypothesis even though it should be rejected? The answer is not obvious! If the average blood pressure of the employees is 132 mmHG (just 2 mmHg from the null value), it might be very difficult to detect the difference unless a large sample is used. On the other hand, if the employee average was 140 mmHg, it would be easier to detect a difference.

\begin{example}{Suppose the employee average is 132 mmHg, and the company takes a sample of 100 individuals. By the Central Limit Theorem, the sampling distribution of $\overline{x}$ is approximately $\mathcal{N}(132, 2.5)$. \footnote{$SE = \frac{25}{\sqrt{100}} = 2.5$} What is the probability of successfully rejecting the null hypothesis?}
\label{computePowerIfMuIs132AndMu0Is130}
Use the tools that Chapter~\ref{probability} provided on probability. Divide this question into two normal probability questions. First, identify what values of $\overline{x}$ would represent sufficiently strong evidence to reject $H_0$. Second, use the hypothetical sampling distribution with center $\mu=132$ to find the probability of observing sample means in the areas found in the first step.

\textbf{Step 1.} The distribution of the sample mean if the null hypothesis were true is $\mathcal{N}(130, 2.5)$. The bounds for the two tail areas can be found by identifying the T-statistic corresponding to the 2.5\% tails ($\pm 1.96$), and solving for the bound, $x$, in the T-statistic equation:
\begin{align*}
-1.96 = T_1 &= \frac{x_1 - 130}{2.5}
	&+1.96 = T_2 &= \frac{x_2 - 130}{2.5} \\
x_1 &= 125.1
	&x_2 &= 134.9
\end{align*}
(An equally valid approach is to recognize that $x_1$ is $1.96\times SE$ below the mean and $x_2$ is $1.96\times SE$ above the mean to compute the values.) Figure~\ref{power132And141} shows the null distribution on the left with these two dotted cutoffs.

\textbf{Step 2.} Next, the probability of rejecting $H_0$ successfully is computed if $\overline{x}$ was distributed $\mathcal{N}(132, 2.5)$, not according to the null hypothesis, $\mathcal{N}(130,2.5)$. This is the same as finding the two shaded tails for the second distribution in Figure~\ref{power132And141}. Use the T-statistic method again:
\begin{align*}
&T_{left} = \frac{125.1 - 132}{2.5} = -2.76
	&&T_{right} = \frac{134.9 - 132}{2.5} = 1.16 \\
&area_{left} =0.003
	&&area_{right} =0.123
\end{align*}
The probability of rejecting the null mean, if the true mean is 132, is the sum of these areas: $0.003 + 0.123 = 0.126$.
\end{example}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/power132And141/power132And141}
\caption{The sampling distribution of $\overline{x}$ under two scenarios. Left: $\mathcal{N}(130, 2.5)$. Right: $\mathcal{N}(132, 2.5)$, and the shaded areas in this distribution represent the power of the test.}
\label{power132And141}
\end{figure}

The probability of correctly rejecting the null hypothesis is called the \term{power}. The power varies depending on what the null value is believed to be. In Example~\ref{computePowerIfMuIs132AndMu0Is130}, the difference between the null value and the supposed true mean was relatively small, so the power was also small: only 0.126. However, when the truth is far from the null value, where the standard error is used as a measure of what is far, the power tends to increase.

\begin{exercise}
Suppose the sampling distribution of $\overline{x}$ is centered at 140. That is, $\overline{x}$ comes from $\mathcal{N}(140, 2.5)$. What would the power be under this scenario? It may be helpful to draw $\mathcal{N}(140, 2.5)$ and shade the area representing power on Figure~\ref{power132And141}; use the same cutoff values identified in Example~\ref{computePowerIfMuIs132AndMu0Is130}.\footnote{Draw the distribution $\mathcal{N}(140, 2.5)$, then find the area below 125.1 (about zero area) and above 134.9 (about 0.979). If the true mean is 140, the power is about 0.979.}
\end{exercise}

\begin{exercise}
If the power of a test is 0.979 for a particular mean, what is the Type 2 Error rate for this mean?\footnote{The Type 2 Error rate represents the probability of failing to reject the null hypothesis when the null hypothesis is incorrect. The power is the probability that the null hypothesis is rejected when it is false. Power and Type 2 Error rate are complements of each other. The Type 2 Error rate will be $1-0.979 = 0.021$.}
\end{exercise}

\begin{exercise}
Provide an intuitive explanation for why it is more likely to reject $H_0$ when the true mean is further from the null value.\footnote{Answers may vary a little. When the truth is far from the null value, researchers will calculate a larger T-statistic, and observe a point estimate that also tends to be far from the null value. A further point estimate makes it easier to detect the difference and reject $H_0$.}
\end{exercise}

\subsection{Statistical significance versus practical significance}

When the sample size becomes larger, point estimates become more precise and any real differences in the mean and null value become easier to detect and recognize. Even a very small difference would likely be detected with a large enough sample. Sometimes researchers will decide to take such large samples that even the slightest difference is detected. While the difference is \term{statistically significant}, it might not be \term{practically significant}.

Statistically significant differences are sometimes so minor that they are not practically relevant. This is especially important to research where the goal of the study is to find a meaningful and useful result. Researchers do not want to spend money and resources finding results that hold no practical or applicable value.

The role of a statistician or a scientist conducting a study often includes planning the size of the study and determining the value of $\alpha$. They might first consult experts or scientific literature to learn what would be the smallest meaningful difference from the null value that would still be practically significant. From literature,  they also would obtain some reasonable estimate for the standard deviation. With these important pieces of information, a sufficiently large sample size would be chosen so that the power for the meaningful difference is perhaps aimed toward 80\% or 90\%. While larger sample sizes may still be available, advising statisticians might caution researchers, especially in cases of sensitive research. Statistical rigor in hypothesis testing is absolutely important, but many of these tests must also stand up to practical significance within application and relevance. 









%%%%%%%%%%%%%%HERE IS THE COMMENT%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%HERE IS THE COMMENT%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%HERE IS THE COMMENT%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%HERE IS THE COMMENT%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-----------------------------------------------------------------------------------------------------------------------------------------------------

%%% Use for an example/exercise
\begin{comment}

\subsection{One sample $t$-confidence intervals}
\label{oneSampleTConfidenceIntervals}

\index{data!dolphins and mercury|(}

Dolphins are at the top of the oceanic food chain, which causes dangerous substances such as mercury to concentrate in their organs and muscles. This is an important problem for both dolphins and other animals, like humans, who occasionally eat them. For instance, this is particularly relevant in Japan where school meals have included dolphin at times.
\setlength{\captionwidth}{86mm}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{ch_inference_foundations_oi_biostat/figures/rissosDolphin/rissosDolphin.jpg}  \\
\addvspace{2mm}
\begin{minipage}{\textwidth}
   \caption[rissosDolphinPic]{A Risso's dolphin.\vspace{-1mm} \\
   -----------------------------\vspace{-2mm}\\
   {\footnotesize Photo by Mike Baird (\oiRedirect{textbook-bairdphotos_com}{www.bairdphotos.com}). \oiRedirect{textbook-CC_BY_2}{CC~BY~2.0~license}.}\vspace{-8mm}}
   \label{rissosDolphin}
\end{minipage}
\vspace{3mm}
\end{figure}
\setlength{\captionwidth}{\mycaptionwidth}

Here we identify a confidence interval for the average mercury content in dolphin muscle using a sample of 19 Risso's dolphins from the Taiji area in Japan.\footnote{Taiji was featured in the movie \emph{The Cove}, and it is a significant source of dolphin and whale meat in Japan. Thousands of dolphins pass through the Taiji area annually, and we will assume these 19 dolphins represent a simple random sample from those dolphins. Data reference: Endo T and Haraguchi K. 2009. High mercury levels in hair samples from residents of Taiji, a Japanese whaling town. Marine Pollution Bulletin 60(5):743-747.} The data are summarized in Table~\ref{summaryStatsOfHgInMuscleOfRissosDolphins}. The minimum and maximum observed values can be used to evaluate whether or not there are obvious outliers or skew.

\begin{table}[h]
\centering
\begin{tabular}{ccc cc}
\hline
$n$ & $\overline{x}$ & $s$ & minimum & maximum \\
19   & 4.4	  & 2.3  & 1.7	       & 9.2 \\
\hline
\end{tabular}
\caption{Summary of mercury content in the muscle of 19 Risso's dolphins from the Taiji area. Measurements are in $\mu$g/wet g (micrograms of mercury per wet gram of muscle).}
\label{summaryStatsOfHgInMuscleOfRissosDolphins}
\end{table}

\begin{example}{Are the independence and normality conditions satisfied for this data~set?}
The observations are a simple random sample and consist of less than 10\% of the population, therefore independence is reasonable. The summary statistics in Table~\ref{summaryStatsOfHgInMuscleOfRissosDolphins} do not suggest any skew or outliers; all observations are within 2.5 standard deviations of the mean. Based on this evidence, the normality assumption seems reasonable.
\end{example}

In the normal model, we used $z^{\star}$ and the standard error to determine the width of a confidence interval. We revise the confidence interval formula slightly when using the $t$-distribution:
\begin{eqnarray*}
\overline{x} \ \pm\  t^{\star}_{df}SE
\end{eqnarray*}
\marginpar[\raggedright\vspace{-9mm}

$t^{\star}_{df}$\vspace{1mm}\\\footnotesize Multiplication\\factor for\\$t$ conf. interval]{\raggedright\vspace{-9mm}

$t^{\star}_{df}$\vspace{1mm}\\\footnotesize Multiplication\\factor for\\$t$ conf. interval}The sample mean and estimated standard error are computed just as before ($\overline{x} = 4.4$ and $SE = s/\sqrt{n} = 0.528$). The value $t^{\star}_{df}$ is a cutoff we obtain based on the confidence level and the $t$-distribution with $df$ degrees of freedom. Before determining this cutoff, we will first need the degrees of freedom.

\begin{termBox}{\tBoxTitle{Degrees of freedom for a single sample}
If the sample has $n$ observations and we are examining a single mean, then we use the $t$-distribution with $df=n-1$ degrees of freedom.}
\end{termBox}

In our current example, we should use the $t$-distribution with $df=19-1=18$ degrees of freedom. Then identifying $t_{18}^{\star}$ is similar to how we found $z^{\star}$. 
\begin{itemize}
\setlength{\itemsep}{0mm}
\item For a 95\% confidence interval, we want to find the cutoff $t^{\star}_{18}$ such that 95\% of the $t$-distribution is between -$t^{\star}_{18}$ and $t^{\star}_{18}$.
\item We look in the $t$-table on page~\pageref{tTableSample}, find the column with area totaling 0.05 in the two tails (third column), and then the row with 18 degrees of freedom: $t^{\star}_{18} = 2.10$.
\end{itemize}
Generally the value of $t^{\star}_{df}$ is slightly larger than what we would get under the normal model with~$z^{\star}$.

Finally, we can substitute all our values into the confidence interval equation to create the 95\% confidence interval for the average mercury content in muscles from Risso's dolphins that pass through the Taiji area:
\begin{eqnarray*}
\overline{x} \ \pm\  t^{\star}_{18}SE
	\quad \to \quad
4.4 \ \pm\  2.10 \times 0.528
	\quad \to \quad
(3.29, 5.51)
\end{eqnarray*}
We are 95\% confident the average mercury content of muscles in Risso's dolphins is between 3.29 and 5.51 $\mu$g/wet gram, which is considered extremely high.

\index{data!dolphins and mercury|)}

\begin{termBox}{\tBoxTitle{Finding a $t$-confidence interval for the mean}
Based on a sample of $n$ independent and nearly normal observations, a confidence interval for the population mean is
\begin{eqnarray*}
\overline{x} \ \pm\  t^{\star}_{df}SE
\end{eqnarray*}
where $\overline{x}$ is the sample mean, $t^{\star}_{df}$ corresponds to the confidence level and degrees of freedom, and $SE$ is the standard error as estimated by the sample.}
\end{termBox}

\textC{\pagebreak}

\begin{exercise} \label{croakerWhiteFishPacificExerConditions}
\index{data!white fish and mercury|(}
The FDA's webpage provides some data on mercury content of fish.\footnote{\oiRedirect{textbook-fda_mercury_in_fish_2010}{www.fda.gov/food/foodborneillnesscontaminants/metals/ucm115644.htm}} Based on a sample of 15 croaker white fish (Pacific), a sample mean and standard deviation were computed as 0.287 and 0.069 ppm (parts per million), respectively. The 15 observations ranged from 0.18 to 0.41 ppm. We will assume these observations are independent. Based on the summary statistics of the data, do you have any objections to the normality condition of the individual observations?\footnote{There are no obvious outliers; all observations are within 2 standard deviations of the mean. If there is skew, it is not evident. There are no red flags for the normal model based on this (limited) information, and we do not have reason to believe the mercury content is not nearly normal in this type of fish.}
\end{exercise}

\begin{example}{Estimate the standard error of $\overline{x}=0.287$ ppm using the data summaries in Guided Practice~\ref{croakerWhiteFishPacificExerConditions}. If we are to use the $t$-distribution to create a 90\% confidence interval for the actual mean of the mercury content, identify the degrees of freedom we should use and also find $t^{\star}_{df}$.}
\label{croakerWhiteFishPacificExerSEDFTStar}
The standard error: $SE = \frac{0.069}{\sqrt{15}} = 0.0178$. Degrees of freedom: $df = n - 1 = 14$.

Looking in the column where two tails is 0.100 (for a 90\% confidence interval) and row $df=14$, we identify $t^{\star}_{14} = 1.76$.
\end{example}

\begin{exercise}
Using the results of Guided Practice~\ref{croakerWhiteFishPacificExerConditions} and Example~\ref{croakerWhiteFishPacificExerSEDFTStar}, compute a 90\% confidence interval for the average mercury content of croaker white fish (Pacific).\footnote{$\overline{x} \ \pm\ t^{\star}_{14} SE \ \to\  0.287 \ \pm\  1.76\times 0.0178\ \to\ (0.256, 0.318)$. We are 90\% confident that the average mercury content of croaker white fish (Pacific) is between 0.256 and 0.318 ppm.}

\index{data!white fish and mercury|)}

\end{exercise}

\end{comment} 
%%%