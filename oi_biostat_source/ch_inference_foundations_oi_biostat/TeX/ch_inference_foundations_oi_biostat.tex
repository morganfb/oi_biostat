%!TEX root=../../main.tex

%\begin{doublespace}
\begin{spacing}{1.5}

\chapter{Foundations for inference}
\label{foundationsForInference}

\begin{comment}
Issues pending

-- Expand discussion of CLT, per JV comment

-- Replace 2 with 1.96?  I would rather not, but will discuss with JV

-- Changing confidence levels
	
\end{comment}

On average, how many days a week are high school students physically active? How many hours of sleep per night do they get? These questions about the youth population in the United States could be answered either by collecting information from all 21.2 million high school aged youth (ages 15-19) or by calculating  estimates based on a well-chosen sample of students. The first strategy is simply impossible, while the second represents a primary goal of statistics -- drawing inferences about the characteristics of a population from a sample. In statistical terms, a characteristic of a population, such as the average amount of sleep per night for high school aged students, is called a \term{population parameter}. When carried out correctly, sampling from a population is an efficient way to estimate a population parameter. This chapter introduces the important ideas in drawing estimates from samples by discussing methods of inference for a population mean, $\mu$. 

This chapter also discusses three widely used tools in statistics: point estimates (single number estimates) for a population mean, interval estimates that include both a point estimate and a margin of error, and a method for testing scientific hypotheses about $\mu$. The concepts used in this chapter will appear throughout the rest of the book. While particular equations or formulas may change to reflect the details of a problem at hand, the fundamental ideas will not. 

\index{data!yrbss|(}

The United States Centers for Disease Control and Prevention (CDC) lists one of their roles as "promoting healthy and safe behaviors, communities, and environment."\footnote{\url{http://www.cdc.gov/about/organization/mission.htm}} The first step in health promotion is understanding health behaviors; the CDC periodically conducts several surveys, including the Youth Risk Behavior Surveillance System (YRBSS).\footnote{\url{http://www.cdc.gov/healthyyouth/data/yrbs/index.htm}} Between 1991 and 2013, 2.6 million high school students have participated in more than 1,100 separate surveys. The next few sections discuss the \data{yrbss} dataset, which contains the responses of the 13,583 high school students who participated in the 2013 YRBSS.\footnote{\oiRedirect{textbook-yrbss}{www.cdc.gov/healthyyouth/data/yrbs/data.htm}} Part of this dataset is shown in Table~\ref{yrbssDF}, with the variables are described in Table~\ref{yrbssVariables}.

\begin{table}[h]
\centering
\begin{tabular}{rrllrrlrr}
  \hline
ID & age & gender & grade & height & weight & helmet & active & lifting \\ 
  \hline
1 &  14 & female & 9 &  &  & never &   4 &   0 \\ 
  2 &  14 & female & 9 &  &  & never &   2 &   0 \\ 
  3 &  15 & female & 9 & 1.73 & 84.37 & never &   7 &   0 \\ 
  $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
  13582 &  17 & female & 12 & 1.60 & 77.11 & sometimes &   5 &  \\ 
  13583 &  17 & female & 12 & 1.57 & 52.16 & did not ride &   5 &  \\ 
  \hline
\end{tabular}
\caption{Five cases from the \data{yrbss} dataset. Blank observations represent missing data. For example, the height and weight of students 1 and 2 are missing\textC{\vspace{-2mm}}}
\label{yrbssDF}
\end{table}
% library(openintro); library(xtable); data(yrbss); xtable(rbind(head(yrbss, 4), tail(yrbss, 2))[, c("age", "gender", "grade", "height", "weight", "helmet_12m", "physically_active_7d", "strength_training_7d")])

\begin{table}[h]
\centering\small
\begin{tabular}{l p{110mm}}
\hline
{\bf age} & {\bf Age of the student.} \\
\hline
\var{gender} & {Sex of the student.} \\
\var{grade} & Grade in high school (9-12) \\
\var{height} & Height, in meters. (1 m = ~3.28 ft) \\
\var{weight} & Weight, in kilograms (1 kg = ~2.2 lbs) \\
\var{helmet} & Frequency that the student wore a helmet while biking in the last 12~months. \\
\var{active} & Number of days physically active for 60+ minutes in the last 7 days. \\
\var{lifting} & Number of days of strength training (e.g. lifting weights) in the last 7 days. \\
\hline
\end{tabular}
\caption{Variables and their descriptions for the \data{yrbss} data set.}
\label{yrbssVariables}
\end{table}

\index{data!yrbss.samp|(}

CDC public health scientists used the responses of 13,572 students to estimate the health behaviors of the \term{target population}: the approximately 21.2 million high school aged students in the US population in 2013. 

This chapter illustrates inference for a population mean by treating the CDC sample of 13,582 students as an artificial target population. A random sample of 100 participants (\data{yrbss.samp}) can then be used to estimate health behaviors for the "target population" of 13,572 respondents. In other words, this chapter will demonstrate how, with only the information from 100 students, it is possible to estimate the behaviors for 13,572 students (and by extension, show how the CDC used information from 13,572 students to estimate health behaviors for 21.2 million students).  

While the individuals in \data{yrbss} are not truly a target population, treating them as such allows for the estimates obtained by inference (using \data{yrbss.samp}) to be checked against the "population parameters" of \data{yrbss} -- not possible in a realistic setting, since population parameters (such as the average hours of sleep per night across 21.2 million students) are typically unknown.\footnote{Hence, the need for inference!}

The dataset \data{yrbss.samp} contains the data for 100 student responses randomly sampled from the larger \data{yrbss} dataset (Table~\ref{yrbssSampDF}).\footnote{About 10\% of high schoolers for each variable chose not to answer the question. Multiple regression (see Chapter~\ref{multipleAndLogisticRegression}) was used to predict what those responses would have been. For simplicity, we will assume that these predicted values can be used for the unknown responses.} Histograms summarizing the \var{height}, \var{weight}, \var{active}, and \var{lifting} variables from \data{yrbss.samp} are shown in Figure~\ref{yrbssSampHistograms}.

\begin{table}
\centering
\begin{tabular}{rrllrrlrr}
  \hline
ID & age & gender & grade & height & weight & helmet & active & lifting \\ 
  \hline
5653 &  16 & female & 11 & 1.50 & 52.62 & never &   0 &   0 \\ 
  9437 &  17 & male & 11 & 1.78 & 74.84 & rarely &   7 &   5 \\ 
  2021 &  17 & male & 11 & 1.75 & 106.60 & never &   7 &   0 \\ 
  $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
  2325 &  14 & male & 9 & 1.70 & 55.79 & never &   1 &   0 \\ 
   \hline
\end{tabular}
\caption{Four observations for the \data{yrbss.samp} data set, which represents a simple random sample of 100 high schoolers from the 2013 YRBSS.}
\label{yrbssSampDF}
% library(openintro); library(xtable); data(yrbss); xtable(rbind(head(yrbss.samp, 3), tail(yrbss.samp, 1))[, c("age", "gender", "grade", "height", "weight", "helmet_12m", "physically_active_7d", "strength_training_7d")])
%library(openintro); library(xtable); data(yrbss); data(yrbss.samp); xtable(yrbss.amp[c(1,2,3,100),])
\end{table}


% WARNING: This figure is referenced in Section 4.2
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]
{ch_inference_foundations_oi_biostat/figures/yrbssSampHistograms/yrbssSampHistograms} 
\caption{Histograms of \var{height}, \var{weight}, \var{activity}, and \var{lifting} for the sample data (\data{yrbss.samp}). The \var{height} distribution is approximately symmetric, \var{weight} is moderately skewed to the right, \var{activity} is bimodal or multimodal (with unclear skew), and \var{lifting} is strongly right skewed.\index{skew!example: moderate}\index{skew!example: strong}}
\label{yrbssSampHistograms}
\end{figure}

%__________________
\section[Variability in estimates]{Variability in estimates} %\sectionvideohref{youtube-DNIauUrRIEM&list=PLkIselvEzpM7Pjo94m1e7J5jkIZkbQAl4}}
\label{variabilityInEstimates}

\index{point estimate|(}

The \data{yrbss.samp} data can be used to estimate four features of the 13,582 high school students in \data{yrbss}: 1) average height (in meters), 2) average weight, 3) average number of days per week physically active (for more than 60 minutes at a time), 4) average body mass index (BMI).

Sample means are the natural choice of summary statistic for estimating a population mean. The average height of the students in \data{yrbss.samp} is:

\begin{align*}
\overline{x}_{height} = \frac{1.50 + 1.78 + \dots + 1.70}{100} = 1.697
\end{align*}
%library(openintro); data(yrbss.samp); mean(yrbss.samp$height); yrbss.samp$height

The sample mean $\overline{x} = 1.697$ meters (5 feet, 6.8 inches) is a \term{point estimate} of the population mean. If a second random sample of 100 were taken, the new sample mean would likely be different as a result of \term{sampling variation}.  Estimates generally vary from one sample to another, whereas the population mean is a fixed value; thus, the distinction between a sample mean versus a population mean is important. 

The sample means of \var{weight} and \var{active} provide estimates of the average weight and number of days active per week of YRBSS respondents. On average, students weigh 68.89 kilograms (about 151.6 pounds) and are active 3.75 days per week:
\begin{align*}
\overline{x}_{weight} &= \frac{52.6 + 74.8 + \dots + 55.8}{100} = 68.89
&\overline{x}_{active} &= \frac{0 + 7 + \dots + 1}{100} = 3.75.
\end{align*}
%library(openintro); data(yrbss.samp); d <- yrbss.samp$weight; mean(d); d
%library(openintro); data(yrbss.samp); d <- yrbss.samp$physically_active_7d; mean(d); d

BMI is used by health professionals to gauge whether an individual's weight is consistent with their height. While BMI is not one of the variables in the dataset, it can be calculated from height and weight (measured in metric units) with the formula:

\[ bmi = \frac{weight}{height{^2}}\]

For example, the respondent with ID 5553 has BMI of 23.39 ($52.62/1.5^{2}$). Larger values of BMI indicate higher levels of body fat. Average BMI in \data{yrbss.samp} is found by calculating BMI values for each respondent, then computing the average of the BMI values. Average BMI in \data{yrbss.samp} is 23.92. Since adolescent size and body type can change with age, there are no population norms for BMI in the age range of YRBSS respondents as there are for adults.

%%%  note that bmi cutpoints for children and teens are not the same as for adults. The CDC does not seem to recommend levels for health bmi for teens

Other population parameters, such as population median or population standard deviation, are also estimated using sample versions. Table~\ref{ptEstimatesYrbssActive} shows estimates of the population mean, median, and standard deviation for respondents in \data{yrbss}, using \data{yrbss.samp}, as well as the "population parameters" calculated by using the full \data{yrbss} dataset. The estimates differ slightly from the population parameters, but not by much -- in fact, the estimate for median is equal to the population median. Note that the estimates will vary based on the sample taken; these estimates are specific to the data in \data{yrbss.samp}.

\begin{table}[h]
\centering
\begin{tabular}{ l rr}
\hline
\var{active}	& estimate & parameter  \\
\hline
mean		& 3.75 & 3.90 \\
median		& 4.00 & 4.00 \\
st. dev.		& 2.556 & 2.564 \\
\hline
\end{tabular}
\caption{Point estimates and parameter values for the \var{active} variable. The parameters were obtained by computing the mean, median, and SD for all respondents (i.e. the complete set of responses in \data{yrbss}).}
\label{ptEstimatesYrbssActive}
\end{table}

\begin{exercise} \label{peOfDiffActiveBetweenGender}
How would one estimate the difference in days active for men and women? If $\overline{x}_{men} = 4.3$ and $\overline{x}_{women} = 3.2$, then what is a good point estimate for the population difference?\footnote{If $\overline{x}_{men} = 4.3$ and $\overline{x}_{women} = 3.2$, the difference of the two sample means, $4.3 - 3.2 = 1.1$, would be an estimate of the difference. In other words, it can be concluded from the sample that on average, the male respondents are physically active about 1.1 days per week more than the female respondents.}
\end{exercise}
%library(openintro); library(xtable); data(yrbss); data(yrbss.samp); (x <- by(yrbss.samp$physically_active_7d, yrbss.samp$gender, mean)); diff(x)

While point estimates rarely equal population parameters (the median in Table~\ref{ptEstimatesYrbssActive} is one of those rare examples), they do become more accurate as more data become available. The running mean from the variable \var{active} in \data{yrbss.samp}, shown in Figure~\ref{yrbssActiveRunningMean}, demonstrates this principle. A \term{running mean} is a sequence of sample means in which each mean is calculated using one more observation than the mean preceding it in the sequence; in other words, sample size increases by 1 for each mean that is calculated. For example, the second mean in the sequence is the average of the first two observations, and the third mean in the sequence is the average of the first three observations. 

\begin{figure}[h]
   \centering
   \includegraphics[width=0.72\textwidth]{ch_inference_foundations_oi_biostat/figures/yrbssActiveRunningMean/yrbssActiveRunningMean}
   \caption{The mean computed after gradually adding each individual to the sample (data from \data{yrbss.samp}). The mean tends to approach the true population average as more data become available. Running means calculated from a different random sample from \data{yrbss} would also show the same behavior.}
   \label{yrbssActiveRunningMean}
\end{figure}

%JV: Comments about running mean below

\begin{comment}

\textit{DH: We should add the idea that AZ used in her draft: showing a different sequence of running means would show the same behavior.  Since we have only one sample here, not clear whether it is worth the added complexity.}

\textit{JV: I added that to the caption of the running mean figure. It may or may not be worth it to have a different random sample of 100 to prove the point -- both earlier, when pointing out that estimates are sample-specific, and then now, to illustrate how a different set of running means behaves the same way. If we did add it, it would be easy enough to partition Fig. 4.6 to show two graphs.}

\end{comment}

As the sample size increases, the running mean approaches the population mean of 3.90 days. Thus, the CDC officials can be reasonably confident that means calculated using data from 13,582 students can provide an accurate estimate of the true population mean across 21.2 million students. By using the formulas in the following section, Section~\ref{seOfTheMean}, it is possible to calculate how accurate sample estimates are likely to be.

\subsection{Standard error of the mean}
\label{seOfTheMean}

The point estimate $\overline{x} = 3.75$ days active per week is an estimate of the population mean $\mu$ based on the random sample \data{yrbss.samp}. Another random sample of 100 participants might produce a different value of $\overline{x}$, such as 3.22 days; repeated random sampling would result in additional different values, perhaps 3.67 days, 4.10 days, and so on. Each sample mean can be thought of as a single observation from a random variable $\overline{X}$, that has the same kind of properties as those discussed in Chapter 3. $\overline{X}$ will have a distribution of values, referred to as the \term{sampling distribution of the sample mean}; this distribution also has a mean and standard deviation.

In almost any study, such as those that produced the \data{LEAP} or \data{frog} datasets, conclusions about a population parameter must be drawn from the data collected, which represent a single sample. In those instances, the sampling distribution of $\overline{X}$ is a theoretical concept, not something that can be calculated -- obtaining repeated samples by conducting the studies again is not possible.

However, the concept can be illustrated here by taking repeated random samples from \data{yrbss}, the artificial target population. Figure~\ref{yrbssActive1000SampDist} shows a histogram of sample means from 1,000 random samples of size 100 from \data{yrbss}. The histogram gives a picture of the theoretical sampling distribution of $\overline{X}$ when the sample size is 100. 

\begin{figure}[h]
   \centering
   \includegraphics[width=0.9\textwidth]
{ch_inference_foundations_oi_biostat/figures/yrbssActive1000SampDist/yrbssActive1000SampDist}
   \caption{A histogram of 1000 sample means for number of days physically active per week, where the samples are of size $n=100$.}
   \label{yrbssActive1000SampDist}
\end{figure}

\begin{termBox}{\tBoxTitle{Sampling distribution}
The sampling distribution is the distribution of the point estimates based on samples of a fixed size from a certain population. It is useful to think of a particular point estimate as being drawn from such a distribution.}
\end{termBox}

The sampling distribution shown in Figure~\ref{yrbssActive1000SampDist} is unimodal and symmetric. The center of the histogram is approximately the mean of the random variable $\overline{X}$. Statistical theory can be used to show that the mean of the sampling distribution for $\overline{X}$ is exactly equal to the population mean. 

The \term{standard error} of the sample mean measures the sample-to-sample variability of $\overline{X}$, or in other words, the extent to which values of the repeated sample means oscillate around the population mean. The standard deviation of the 1,000 values of $\overline{X}$ is 0.26, approximately the standard error of $\overline{X}$. However, this method of estimating the standard error is not possible when there is only a single sample. Since the sample represents only one observation from $\overline{X}$, it cannot provide any information about the variability of the entire distribution.

When repeated random samples are not available, the standard error of the sample mean is calculated by dividing the population standard deviation ($\sigma_{x}$) by the square root of the sample size. If $\overline{x}$ is the sample mean of days per week active,

\[SE_{\overline{x}} = \sigma_{\overline{x}} = \dfrac{\sigma_{x}}{\sqrt{n}} = \dfrac{2.6}{\sqrt{100}} = 0.26\]

The probability tools of Section~\ref{randomVariablesSection} can be used to derive the formula $\sigma_{\overline{X}} = \sigma/\sqrt{n}$, but the derivation is not shown here. Conceptually, larger sample sizes result in sampling distributions that have decreasing variability; increasing $n$ for a given distribution with $\sigma_{x}$ will lower the standard error of the sample mean. In other words, increasing sample size causes $\overline{X}$ to be clustered more tightly around the population mean $\mu$, allowing for more accurate estimates of $\mu$ from a single sample.

% \term{standard error (SE)}\index{SE}\marginpar[\raggedright\vspace{-4mm}

% $SE$\\\footnotesize standard\\error]{\raggedright\vspace{-4mm}

% $SE$\\\footnotesize standard\\error} of the estimate.

\begin{termBox}{\tBoxTitle{The SE of the sample mean}
Given $n$ independent observations from a population with standard deviation $\sigma$, the standard error of the sample mean is equal to \vspace{-1mm}
\begin{align*}
SE = \frac{\sigma}{\sqrt{n}}
\label{seOfXBar}
\end{align*}\vspace{-3mm}%
}
\end{termBox}

Of course, the population standard deviation $\sigma$ is typically unknown. Instead, the sample standard deviation $s$ can be used as a reasonably good estimate of $\sigma$, and $s / \sqrt{n}$ as an estimate for the standard error of the sample mean. For example, the estimate of SE using \data{yrbss.samp} is 0.2556 ($2.556 / \sqrt{100}$), which is very close to 0.26.

This estimate tends to be sufficiently good when the sample size is at least 30 and the population distribution is not strongly skewed. In the case of skewed distributions, a larger sample size is necessary.

%\begin{comment}


\begin{itemize}
\setlength{\itemsep}{0mm}	
	\item The population parameters $\mu$ and $\sigma$ are characteristics of the target population from which a sample is drawn. 
	
	\item In a single sample, the arithmetic average of the values in the sample (the sample mean) is denoted by $\overline{x}$. The sample standard deviation is denoted by $s$. 
	
	\item If repeated samples could be taken, the random variable $\overline{X}$ represents the collection of sample means (one for each sample). The distribution of $\overline{X}$ is called its sampling distribution, which itself has a mean and standard deviation. 
	
	\item With random sampling, the mean of the random variable $\overline{X}$ is always equal to the population mean $\mu$.  In the notation of Chapter 3, $\mu_{\overline{X}} = E(\overline{X}) = \mu$.
	
	\item  The standard deviation of $\overline{X}$, written $\sigma_{\overline{X}}$, is called its standard error (SE). 
	
	\item The standard error of the sample mean, as calculated from a single sample of size $n$, is equal to $\dfrac{\sigma}{\sqrt{n}}$. SE is usually estimated by using $s$, such that $SE = \dfrac{s}{\sqrt{n}}$.
	
\end{itemize}

Important features of a population are thought of (or modeled) as parameters of the distribution of values of a random variables, and estimates of those features 

\index{point estimate|)}

\section[Confidence intervals]{Confidence intervals} %\sectionvideohref{youtube-FUaXoKdCre4&list=PLkIselvEzpM7Pjo94m1e7J5jkIZkbQAl4}}
\label{confidenceIntervals}
\subsection{Interval estimates for a population parameter}

A \term{confidence interval} provides an estimate for a population parameter along with a margin of error, giving a plausible range of values for the parameter instead of a single value. When estimating a population mean $\mu$, a confidence interval for $\mu$ has the general form

\[(\overline{x} -m, \ \overline{x} + m) = \overline{x} \pm m \],
where $m$ is the margin of error. The standard error, a measure of the uncertainty associated with the point estimate, is used in the calculation of the margin of error.

%\subsection{An approximate 95\% confidence interval}

The standard error of the sample mean is its standard deviation; thus, by the empirical rule discussed in Section~\ref{empiricalRule} of Chapter 3, the sample mean will be within 2 standard errors of the population mean $\mu$ approximately 95\% of the time. If an interval is constructed that spans 2 standard errors from the point estimate in either direction, a data analyst can be 95\% \term{confident} that the population mean is somewhere within the interval:

\begin{align}
\text{point estimate}\ \pm\ 2\times SE
\label{95PercentConfidenceIntervalFormula}
\end{align}

The phrase "95\% confident" has a subtle interpretation: if many samples were drawn from a population, with each one used to calculate a confidence interval using Equation~\ref{95PercentConfidenceIntervalFormula}, about 95\% of those intervals would contain the population mean $\mu$. Figure~\ref{95PercentConfidenceInterval} illustrates this process with 25 samples taken from \data{yrbss}. 24 of the resulting confidence intervals contain the average number of days per week that respondents are physically active, $\mu$ = 3.90 days, while one interval does not. 

Just as with the sampling distribution of the sample mean, the interpretation of a confidence interval relies on the abstract construct of repeated sampling. A data analyst, who can only observe one sample, does not know whether the population mean lies within the single interval calculated. However, the concept behind the calculation implies that approximately 95 intervals out of 100 will contain the population mean. The value 95\% is an approximation, accurate when the sampling distribution for the sample mean is close to that of a normal distribution. This assumption holds when the sample size is sufficiently large, as will be shown later.

\begin{figure}[hht]
   \centering
   \includegraphics[width=0.78\textwidth]
{ch_inference_foundations_oi_biostat/figures/95PercentConfidenceInterval/95PercentConfidenceInterval}
   \caption{Twenty-five samples of size $n=100$ were taken from \data{yrbss}. For~each sample, a confidence interval was created to try to capture the average number of days per week that students are physically active. Only~1 of these~25 intervals did not capture the true mean, $\mu = 3.90$~days.}
   \label{95PercentConfidenceInterval}
\end{figure}

\begin{exercise}
In Figure~\ref{95PercentConfidenceInterval}, one interval does not contain 3.90 minutes. Does this imply that the mean cannot be 3.90?\footnote{Just as some observations occur more than 2 standard deviations from the mean, some point estimates will be more than 2 standard errors from the parameter. A confidence interval only provides a plausible range of values for a parameter. While we might say other values are implausible based on the data, this does not mean they are impossible.}
\end{exercise}

\begin{example}{The sample mean of days active per week from \data{yrbss.samp} is 3.75~days. The standard error, as estimated using the sample standard deviation, is $SE=\frac{2.564}{\sqrt{100}} = 0.2564$~days. Calculate an approximate 95\% confidence interval for the average days active per week for all YRBSS students.}
We apply Equation~\ref{95PercentConfidenceIntervalFormula}:
\[3.75\ \pm\ 2 \times  0.26 \quad \rightarrow \quad (3.23, 4.27)\]
Based on these data, we can be about 95\% confident that the average days active per week for all YRBSS students was larger than 3.23 but less than 4.27~days. The interval extends out 2 standard errors from the point estimate, $\overline{x}_{active}$.
\end{example}
% library(openintro); library(xtable); d <- yrbss.samp; mean(d$physically_active_7d); sd(d$physically_active_7d); sd(yrbss$physically_active_7d, na.rm=TRUE)

\begin{exercise} \label{95CIExerciseForAgeOfYrbssSamp1}
The sample data suggest that the average YRBSS student height is $\overline{x}_{height} = 1.697$ meters with a standard error of 0.0088 meters (estimated using the sample standard deviation, 0.088 meters). What is an approximate 95\% confidence interval for the average height of all of the YRBSS students?\footnote{Apply Equation~\ref{95PercentConfidenceIntervalFormula}: $1.697 \ \pm \ 2\times 0.0088 \rightarrow (1.6794, 1.7146)$.  We are about 95\% confident the average height of all YRBSS students was between 1.6794 and 1.7146 meters (5.51~to 5.62~feet).}
\end{exercise}
% library(openintro); d <- yrbss.samp; mean(d$height); sd(d$height)

\subsection{The sampling distribution for the mean}

Section~\ref{seOfTheMean} introduced the notion of the sampling distribution for $\overline{X}$, the average number of days physically active per week for samples of size 100. Figure~\ref{yrbssActive1000SampDist} showed the distribution of sample means calculated from 1,000 random samples. Since the complete sampling distribution consists of means for all possible samples of size 100, drawing a much larger number of samples would provide a more accurate view of the distribution; the left panel of Figure~\ref{yrbssActiveBigSampDist} shows the distribution calculated from 100,000 sample means. 

\begin{figure}[hht]
   \centering
   \includegraphics[width=\textwidth]
{ch_inference_foundations_oi_biostat/figures/yrbssActiveBigSampDist/yrbssActiveBigSampDist}
   \caption{The left panel shows a histogram of the sample means for 100,000 random samples. The right panel shows a normal probability plot of those sample means.}
   \label{yrbssActiveBigSampDist}
\end{figure}


The distribution of sample means closely resembles the normal distribution (see Section~\ref{normalDist}). A normal probability plot of these sample means is shown in the right panel of Figure~\ref{yrbssActiveBigSampDist}. All of the points closely fall around a straight line, implying that the distribution of sample means is nearly normal. This result can be explained by the Central Limit Theorem.

%JV: Comments below...

\begin{comment}
\textit{JV: I think it would be worth expanding the discussion of the CLT here, otherwise it seems like an unimportant footnote. Especially useful for students to know that distribution of $\overline{X}$ is normally distributed, even if the population not normally distributed. Not necessary to take the expanded CLT material from OI, but use explanation from lecture.}
\end{comment}

\begin{termBox}{\tBoxTitle{Central Limit Theorem, informal description}
If a sample consists of at least 30 independent observations and the data are not strongly skewed, then the distribution of the sample mean is well approximated by a~normal model.\index{Central Limit Theorem}}
\end{termBox}

% We will apply this informal version of the Central Limit Theorem for now, and discuss its details further in Section~\ref{cltSection}.

%JV: Comments below...

\begin{comment}
\textit{JV: Conceptually, it may be smoother to just start out specifying $1.96 \times SE$ captures 95\%. Since the probability tables are already covered in Unit 3, students would have been exposed to the idea that 2 is an estimate for 1.96. Right now, the transition to 1.96 here reads somewhat awkwardly.}


\end{comment}

The choice of 2 standard errors in Equation~\ref{95PercentConfidenceIntervalFormula} was based on the general guideline that roughly 95\% of the time, observations are within two standard deviations of the mean. Under the normal model, this can be made more accurate by using 1.96 in place~of~2.
\begin{align}
\text{point estimate}\ \pm\ 1.96\times SE
\label{95PercentCIWhenUsingNormalModel}
\end{align}
% If a point estimate, such as $\overline{x}$, is associated with a normal model and standard error $SE$, then we use this more precise 95\% confidence interval.

%%%% revisions to  here, 24 nov 2015

\subsection{Changing the confidence level}
\label{changingTheConfidenceLevelSection}

\index{confidence interval!confidence level|(}

%JV: THIS SUBSECTION IS VERY MESSY!

%might actually be improved by discussing the CLT more thoroughly in earlier section??

%segue from 95% CI's, give conditions for using general formula, explain general formula with 99% as an example...

Ninety-five percent confidence intervals are the most commonly used interval estimates, but intervals with confidence levels other than 95\% can also be constructed.

The general formula for a confidence interval (for the population mean $\mu$) is given by 

\begin{align}
	\overline{x} \pm \ z^{\star} \times SE,
\end{align}

where $z^{\star}$ is chosen according to the confidence level. When calculating a 95\% confidence level, $z^{\star}$ is 1.96, the quantile point on a normal distribution with 0.025 area to the right of that point. In other words, 2.5\% of the normal curve is to the right of 1.96, and 2.5\% is to the left of -1.96; thus, the area within 1.96 standard deviations of the mean captures 95\% of the distribution.

To construct a 99\% confidence interval, $z^{\star}$ must be chosen such that 99\% of the normal curve is captured between -$z^{\star}$ and $z^{\star}$.

\begin{example}{Let $Y$ be a normally distributed random variable. 99\% of the time, $Y$ will be within how many standard deviations of the mean?}
	This is equivalent to asking for the $z$-score with .005 area to the right, or in other words, .5\% of the curve the right of $z$ and .5\% to the left of $-z$. To determine this value, refer to the normal probability table -- find the positive $z$-value that has .005 area to the right and .995 area to the left. The closest two values are 2.57 and 2.58; for convenience, round up to 2.58. Thus, the unobserved random variable $Y$ will be within 2.58 standard deviations of $\mu$ 99\% of the time.
\end{example}

Thus, use 2.58 for $z^{\star}$ to construct a 99\% confidence interval. A data analyst can be 99\% confident that this wider interval captures the population mean. 2.58 is larger than 1.96, resulting in a larger \term{margin of error} (the quantity $z^{\star} \times SE$) and a wider confidence interval, as illustrated in Figure~\ref{choosingZForCI}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]
	{ch_inference_foundations_oi_biostat/figures/choosingZForCI/choosingZForCI}
	\caption{The area between -$z^{\star}$ and $z^{\star}$ increases as $|z^{\star}|$ becomes larger. If the confidence level is 99\%, $z^{\star}$ is chosen such that 99\% of the normal curve is between -$z^{\star}$ and $z^{\star}$, which corresponds to 0.5\% in the lower tail and 0.5\% in the upper tail: $z^{\star}=2.58$.}
	\label{choosingZForCI}
	\index{confidence interval!confidence level|)}
\end{figure}
 
The normal approximation is crucial to the accuracy of these confidence intervals. When the data are reasonably symmetric, the sample size is large ($n \geq 30$), and the observations are independent, it is reasonable to use the normal approximation. Observations are considered independent if they are from a simple random sample and consists of fewer than 10\% of the population. 

%Section~\ref{cltSection} provides a more detailed discussion about when the normal model can safely be applied. 

% WARNING !!!!
% EOCE 4.9 (as of 2nd Edition) references the results of this exercise
\begin{exercise} \label{find99CIForYrbssAgeExercise}
	Create a 99\% confidence interval for the average days active per week of all YRBSS students using \data{yrbss.samp}. The point estimate is $\overline{x}_{active} = 3.75$ and the standard error is $SE_{\overline{x}} = 0.26$.\footnote{The observations are independent (simple random sample, $<10\%$ of the population), the sample size is at least 30 ($n = 100$), and the distribution doesn't have a clear skew (Figure~\ref{yrbssSampHistograms} on page~\pageref{yrbssSampHistograms}); the normal approximation and estimate of SE should be reasonable. Apply the 99\% confidence interval formula: $\overline{x}_{active}\ \pm\ 2.58 \times  SE_{\overline{x}} \rightarrow (3.08, 4.42)$. We are 99\% confident that the average days active per week of all YRBSS students is between 3.08 and 4.42~days.}
\end{exercise}
%library(openintro); data(yrbss.samp); d <- yrbss.samp; mean(d$age); sd(d$age)/sqrt(100)

\begin{comment}

Ninety-five percent confidence intervals are the most commonly used interval estimates, but confidence intervals with confidence levels different from 95\% are straightforward to construct.  A general 95\% confidence interval for a point estimate that comes from a nearly normal distribution is 
\begin{align}
\text{point estimate}\ \pm\ 1.96\times SE
\end{align}
There are three components to this interval: the point estimate; ``1.96''; and the standard error. The choice of $1.96\times SE$ was based on capturing 95\% of the distribution of the sample mean,  since the estimate is within 1.96 standard deviations of the parameter about 95\% of the time. 

\begin{exercise} \label{leadInForMakingA99PercentCIExercise}
If $Y$ is a normally distributed random variable, how often will $Y$ be within 2.58 standard deviations of the mean?\footnote{This is equivalent to asking how often the Z-score will be larger than -2.58 but less than 2.58. (For a picture, see Figure~\ref{choosingZForCI}.) To determine this probability, look up -2.58 and 2.58 in the normal probability table (0.0049 and 0.9951). Thus, there is a $0.9951-0.0049 \approx 0.99$ probability that the unobserved random variable $Y$ will be within 2.58 standard deviations of $\mu$.}
\end{exercise}

To create a 99\% confidence interval, change 1.96 in the 95\% confidence interval formula to  $2.58$. Guided Practice~\ref{leadInForMakingA99PercentCIExercise} highlights that 99\% of the time a normal random variable will be within 2.58 standard deviations of the mean. This approach -- using the $Z$-scores in the normal model to compute confidence levels -- is appropriate when $\overline{x}$ is associated with a normal distribution with mean $\mu$ and standard deviation $SE_{\overline{x}}$. Thus, the formula for a 99\% confidence interval is
\begin{align}
\overline{x}\ \pm\ 2.58\times SE_{\overline{x}}
\label{99PercCIForMean}
\end{align}

A data analyst can be 99\% confident that this wider interval contains the population mean.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]
{ch_inference_foundations_oi_biostat/figures/choosingZForCI/choosingZForCI}
\caption{The area between -$z^{\star}$ and $z^{\star}$ increases as $|z^{\star}|$ becomes larger. If the confidence level is 99\%, $z^{\star}$ is chosen such that 99\% of the normal curve is between -$z^{\star}$ and $z^{\star}$, which corresponds to 0.5\% in the lower tail and 0.5\% in the upper tail: $z^{\star}=2.58$.}
\label{choosingZForCI}
\index{confidence interval!confidence level|)}
\end{figure}

The normal approximation is crucial to the precision of these confidence intervals. Section~\ref{cltSection} provides a more detailed discussion about when the normal model can safely be applied. When the normal model is not a good fit,  alternative distributions that better characterize the sampling distribution will be used.

\textit{when do these alternative distributions appear?}

\begin{termBox}{\tBoxTitle{Conditions for $\overline{X}$ being nearly normal and $SE$ being accurate\label{terBoxOfCondForXBarBeingNearlyNormalAndSEBeingAccurate}}
Important conditions to help ensure the sampling distribution of $\overline{X}$ is nearly normal and the estimate of SE sufficiently accurate:
\begin{itemize}
\setlength{\itemsep}{0mm}
\item The sample observations are independent.
\item The sample size is large: $n\geq30$ is a good rule of thumb.
\item The population distribution is not strongly skewed. 
\end{itemize}
For sample sizes larger than about 45, the condition on skewness can be ignored, unless there are prominent outliers in the sample. }
\end{termBox}

\begin{tipBox}{\tipBoxTitle[]{How to verify sample observations are independent}
If the observations are from a simple random sample and consist of fewer than 10\% of the population, then they are independent.\\[2mm]
Subjects in an experiment are considered independent if they undergo random assignment to the treatment groups. \\[2mm]
}
\end{tipBox}

\begin{tipBox}{\tipBoxTitle[]{Checking for strong skew usually means checking for obvious outliers}
When there are prominent outliers present, the sample should contain at least 100 observations, and in some cases, much more. \\[2mm]
}
\end{tipBox}

% WARNING !!!!
% EOCE 4.9 (as of 2nd Edition) references the results of this exercise
\begin{exercise} \label{find99CIForYrbssAgeExercise}
Create a 99\% confidence interval for the average days active per week of all YRBSS students using \data{yrbss.samp}. The point estimate is $\overline{x}_{active} = 3.75$ and the standard error is $SE_{\overline{x}} = 0.26$.\footnote{The observations are independent (simple random sample, $<10\%$ of the population), the sample size is at least 30 ($n = 100$), and the distribution doesn't have a clear skew (Figure~\ref{yrbssSampHistograms} on page~\pageref{yrbssSampHistograms}); the normal approximation and estimate of SE should be reasonable. Apply the 99\% confidence interval formula: $\overline{x}_{active}\ \pm\ 2.58 \times  SE_{\overline{x}} \rightarrow (3.08, 4.42)$. We are 99\% confident that the average days active per week of all YRBSS students is between 3.08 and 4.42~days.}
\end{exercise}
%library(openintro); data(yrbss.samp); d <- yrbss.samp; mean(d$age); sd(d$age)/sqrt(100)

\begin{termBox}{\tBoxTitle{Confidence interval for any confidence level}
If the point estimate follows the normal model with standard error $SE$, then a confidence interval for the population parameter is
\begin{align*}
\text{point estimate}\ \pm\ z^{\star} SE
\end{align*}
where $z^{\star}$ corresponds to the confidence level selected.}
\end{termBox}

Figure~\ref{choosingZForCI} provides a picture of how to identify $z^{\star}$ based on a confidence level. We~select $z^{\star}$ so that the area between -$z^{\star}$ and $z^{\star}$ in the normal model corresponds to the confidence level. 

\begin{termBox}{\tBoxTitle{Margin of error}
\label{marginOfErrorTermBox}In a confidence interval, $z^{\star}\times SE$ is called the \term{margin of error}.}
\end{termBox}

\textC{\newpage}

\begin{exercise} \label{find90CIForYrbssAgeExercise}
Use the data in Guided Practice~\ref{find99CIForYrbssAgeExercise} to create a 90\% confidence interval for the average days active per week of all YRBSS students.\footnote{First find $z^{\star}$ such that 90\% of the distribution falls between -$z^{\star}$ and $z^{\star}$ in the standard normal model, $N(\mu=0, \sigma=1)$. The value -$z^{\star}$ is found in the normal probability table by looking for a lower tail of 5\% (the other 5\% is in the upper tail): $z^{\star}=1.65$. The 90\% confidence interval is $\overline{x}_{active}\ \pm\ 1.65\times SE_{\overline{x}} \to (3.32, 4.18)$. (The conditions for  conditions for normality and the standard error were verified in the earlier example.)  
We are 90\% confident the average days active per week is between 3.32 and 4.18~days.}

\end{exercise}

\end{comment}

\subsection{Interpreting confidence intervals}
\label{interpretingCIs}

\index{confidence interval!interpretation|(}

The National Health and Nutrition Examination Survey (NHANES) consists of a set of surveys and measurements conducted by the US CDC to assess the health and nutritional status of adults and children in the United States. NHANES is unique in that it combines interviews and physical examinations.  The dataset \data{nhanes.samp} contains 76 variables and is a random sample of 200 individuals from the measurements collected in the years 2009-2010 and 2012-2013. The sample was drawn from a larger sample of 20,293 participants in the \textbf{NHANES} package, available from The Comprehensive R Archive Network (CRAN).\footnote{\url{http://cran.us.r-project.org}} \footnote{The CDC uses a complex sampling design that samples some demographic subgroups with larger probabilities, but \data{nhanes.samp} has been adjusted so that it can be viewed as a random sample of the US population.}  

By adulthood, an individual's height and weight generally stabilize. For people ages 21 and over, body mass index (BMI) is often used as a measure of body fat that can be compared to population norms. Age and BMI information are available in the dataset and can be used to estimate mean BMI in the US population for the years represented by \data{nhanes.samp}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]
	{ch_inference_foundations_oi_biostat/figures/nhanesAdultBmiHist/nhanesAdultBmiHist}
	\caption{The distribution of the variable \var{BMI} for adult participants in NHANES.}
	\label{nhanesAdultBmiHist}
\end{figure}

%DH: should we list sample variables as in ch 1, or just plunge in?  I think we should just plunge in.  We should make the data available, though. JV: Agreed.

\begin{example}{Use \data{nhanes.samp} to calculate a 95\% confidence interval for adult BMI in the US population. \label{exNhanesBmi}}
	In the random sample of 200 participants, BMI is available for 151 of the 152 participants that are 21 years of age or older. As shown in the histogram (Figure~\ref{nhanesAdultBmiHist}), the data are right-skewed, with one larger outlier. The outlier has value 81.3 and corresponds to an individual with height 168.5 cm (66.3 in) and 230.7 kg (508.6 lbs). In the initial analysis of a dataset like this one, such an extreme value of BMI would be checked for accuracy; while it might correspond to an unexpectedly obese individual, it could also represent an error when the data was recorded. Since the accuracy cannot be verified with the data available on CRAN, the following analysis excludes the data point, using a sample size of 150.
	
	The mean and standard deviation in this sample of 150 are 29.7 and 7.7 $\text{kg}/\text{meter}{^2}$, respectively.  The sample size is large enough to justify using the normal approximation when computing the confidence interval.  The standard error of the mean is $\text{SE} = 7.7/\sqrt{150} = 0.63$, so the 95\% confidence interval is given by 
\begin{align*}
	\overline{BMI} \pm (1.96)(\text{SE}) &= 29.7 \pm (1.96)(0.63) \\
	&= (28.5, 30.9)
\end{align*}	
	
	Based on this sample, a data analyst can be 95\% confident that the average BMI of US adults is between 28.5 and 30.9 $\text{kg}/\text{meter}{^2}$.
\end{example}

The correct interpretation of a confidence interval is, "We are XX\% confident that the population parameter is between \dots" While it is tempting to say that a confidence interval captures the population parameter with a certain probability, this is a common error. The confidence level only quantifies how plausible it is that the parameter is within the interval; there is no probability associated with whether a parameter is contained in a specific confidence interval. The confidence coefficient reflects the nature of a procedure that is correct XX\% of the time, given that the assumptions in making the calculations are true.

The conditions about the validity of the normal approximation can be checked using the numerical and graphical summaries discussed in Chapter 1. However, the condition that data should be from a random sample is sometimes overlooked. If the data are not from a random sample, then the confidence interval no longer has interpretive value, since there is no population mean to which the confidence interval applies. For example, while only simple arithmetic is needed to calculate a confidence interval for BMI from the \data{famuss} dataset in Chapter 1, the participants in the study are almost certainly not a random sample from some population. 

%JV: Should we also discuss other common interpretive errors here, perhaps via a guided practice? e.g. $\mu$ equally likely to be anywhere in a given CI

\begin{comment}

The somewhat awkward language used to describe confidence intervals can obscure an important point that was touched upon earlier.  The correct interpretation of a confidence interval is 
\begin{quote}
We are XX\% confident that the population parameter is between...
\end{quote}
It is tempting to say that a confidence interval as captures the population parameter with a certain probability. This is a common error: while it might be useful to think of it as a probability, the confidence level only quantifies how plausible it is that the parameter is in the interval.  The concept of probability applies to event following the play of chance.  There is no probability associated with whether or not a parameter is contained in a specific confidence interval.  The confidence coefficient reflects that the data analyst is using a procedure that will be correct 95\% of the time, if the assumptions upon which the calculation of a confidence  interval are true, at least approximately.


The assumptions about the validity of the normal approximation can be checked using some of the graphical displays discussed in Chapter 1, and students rarely neglect to check those assumptions. The assumption about the data being a random sample from a well-defined cannot be checked, however, but examining the data, and is sometimes overlooked. If that assumption is not true, the confidence interval loses its meaning because there is no population mean to which the confidence interval applies. Fussing over whether to use the term probability vs confidence in an interpretation becomes moot.  There are many instances in statistics where a calculation is easy but has no value.  In the \data{famuss} data, for instance, only simple arithmetic is needed to calculate the mean and standard deviation of the bmi for study participants, and to use those values to calculate a confidence interval.  But the participants in the FAMuSS study are almost certainly not a random sample from some population. 

\end{comment}

\index{confidence interval!interpretation|)}
\index{confidence interval|)}

\section[Hypothesis testing]{Hypothesis testing} %\sectionvideohref{youtube-NVbPE1_Cbx8&list=PLkIselvEzpM7Pjo94m1e7J5jkIZkbQAl4}}
\label{hypothesisTesting}

\index{hypothesis testing|(}

Do Americans tend to be overweight? This question is easy to pose, but more difficult to answer. The "obesity epidemic" in the United States is often discussed in the popular press, and many people feel they have anecdotal evidence that suggests body weights are generally increasing. A more scientific answer to this question should be based on a representative sample of US adults and account for the fact that weight generally increases with height. Body mass index (BMI), discussed in Section~\ref{interpretingCIs}, is one simple measure of body fat that adjusts for height. The World Health Organization (WHO) and other agencies use BMI to set standard guidelines for weight status. The current guidelines are shown in Table~\ref{whoBmiGuidelines}. 

%\footnote{\url{http://apps.who.int/bmi/index.jsp?introPage=intro_3.html}}. 

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|}
\hline 
Category & BMI range\tabularnewline
\hline 
\hline 
Underweight & $<18.50$\tabularnewline
\hline 
Normal (healthy weight) & 18.5-24.99\tabularnewline
\hline 
Overweight & $\geq 25$\tabularnewline
\hline 
Obese & $\geq30$\tabularnewline
\hline
\end{tabular}
\caption{Standard weight categories defined by health agencies.} 
\label{whoBmiGuidelines}
\end{center}
\end{table}

The question can be more precisely formulated as, "Is the average BMI of US adults larger than 21.7 (the middle of the range for normal BMI's)?" The average BMI from the 150 adults in \data{nhanes.samp} was 29.7, with a 95\% confidence interval of (28.5, 30.9). This interval certainly suggests that the average BMI in the US population is higher than 21.7, and even higher than 24.99, the upper limit of the normal weight category. 

However, it is important to remember that it is not impossible for the population average to be outside the interval estimate, even if that would be inconsistent with the data in the sample. Perhaps by random chance, a sample was selected in which individuals of high BMI are overrepresented, causing the sample mean to be higher than the population mean. One measure of the strength of evidence against a working hypothesis is be the likelihood of observing a sample mean as extreme as the one observed if the working hypothesis is true; what is the likelihood of observing a sample mean as large as 29.7 if the population average BMI in the US is actually 21.7?

This question can be addressed by hypothesis testing, a method for calculating the likelihood of observing the value of a parameter estimate under a working hypothesis about the value of the corresponding population parameter. Hypothesis testing can be approached either informally or formally; while both methods essentially reach the same conclusions, each has distinct advantages. The formal method provides a direct estimate of the strength of evidence against the working hypothesis, while the less formal approach can sometimes lead to a better understanding of the logic behind hypothesis testing.

The informal approach is based on what is known about a sample mean and its variability. Hypothesis testing starts with a working hypothesis; in this case, the working hypothesis is that the US population BMI matches the midpoint of the normal range, 21.7. Under this assumption, the mean of the hypothetical sampling distribution for the sample average of BMI will also be 21.7, since the population mean and sample mean should agree. The standard error of the sample mean is an approximate measure of how far the sample mean $\overline{\text{BMI}}$ is from the center of its distribution. In this example, the observed sample mean 29.7 is $(\overline{BMI} - \mu_{BMI})/s =  (29.7 - 21.7)/0.63 = 12.7$. In other words, the observed sample mean is 12.7 standard deviations to the right of 21.7. The sampling distribution of $\overline{x}_{BMI}$ is well approximated by a normal distribution. For a normally distributed random variable, the area to the right of 12.7 standard deviation units from the mean is less than 0.001; the likelihood of a sample with such an extreme mean, if the population is actually centered at 21.7, is vanishingly small. The event is so unusual that it is reasonable to conclude that the working hypothesis is wrong -- the data suggests that population average BMI is larger than 21.7.

The formal approach refers to the working hypothesis as the null hypothesis. In addition to the null hypothesis, the formal approach also starts with an alternative claim that might be true if the null hypothesis is wrong. In nearly all scientific investigations, the investigator believes or suspects that the null hypothesis is not true. 

\begin{termBox}{\tBoxTitle{Null and alternative hypotheses}
{The \term{null hypothesis ($H_0$)} often represents either a skeptical perspective or a claim to be tested. The \term{alternative hypothesis ($H_A$)} represents an alternative claim under consideration and is often represented by a range of possible parameter values.}}
\end{termBox}

In the formal approach, the null hypothesis ($H_0$) is not rejected unless the evidence in favor of the alternative hypothesis ($H_A$) is so strong that the only reasonable conclusion is to reject $H_0$ in favor of $H_A$. The logic is similar to the principle of presumption of innocence. In the criminal trials of many nations, including the US, a defendant is assumed innocent until proven guilty; a verdict of guilty is only returned if it has been established beyond a reasonable doubt that the defendant is guilty. Likewise, the null hypothesis is assumed to be true until sufficient evidence suggests otherwise. 

The next section presents the basic principles of formal hypothesis testing.

\subsection{The Formal Approach to Hypothesis Testing}
\label{formalHypothesisTesting}


\subsubsection{Step 1: Formulating null and alternative hypotheses}

The null and alternative hypotheses are labeled $H_0$ and $H_A$, respectively.  In the BMI example, the two hypotheses are:

\begin{itemize}
	\item $H_0: \mu_{\text{bmi}} = 21.7$. The population average BMI is 21.7\footnote{The null hypothesis can also be written as $H_0: \mu_{\text{bmi}} \leq 21.7$, since the alternative hypothesis is a one-sided alternative.}
	
	\item $H_A: \mu_{\text{bmi}} > 21.7$. the population average BMI is larger than 21.7.
	
\end{itemize}	
	
The alternative hypothesis $H_A: \mu_{\text{bmi}} > 21.7$ is called a \term{one-sided alternative}. Since substantial literature exists about the rising prevalence of obesity in America, it is reasonable to assume that if $H_0$ is not true, the population average BMI would be larger than 21.7, rather than smaller. An investigator studying BMI change in an area experiencing food shortages would likely formulate the alternative hypothesis as $H_A: \mu_{\text{bmi}} < 21.7$. 

 Because of the substantial literature about obesity in the United States, it is reasonable to assume that if $H_0$ is not true, the population average BMI would be larger than 21.7.  An investigator studying a similar problem in another setting, perhaps in the developing world where diets are changing but drought has led to food shortages, would likely use formulate the alternative as $ H_A:\mu_{\text{bmi}} \ne 21.7$. In other settings, it might be more appropriate to use a \term{two-sided alternative}, $\mu_{\text{bmi}} \neq 21.7$. %JV: clear example in line with others needed here}.

More generally, when testing a hypothesis about a population mean $\mu$, the null and alternative hypotheses are written as:

\begin{itemize}
	\item One-sided alternative: \[H_0: \mu \geq \mu_0, \ H_A: \mu < \mu_0\] or \[H_0: \mu \leq \mu_0, \  H_A: \mu > \mu_0\]
	
	\item Two-sided alternative: \[H_0: \mu = \mu_0, \ H_A: \mu \neq \mu_0\]
\end{itemize}

%JV: Use this notation for $H_0$ with one-sided alternatives, or stick to $H_0: \mu = \mu_{0}$?

In this general form, $\mu$ denotes a population mean and $\mu_0$ is the numeric value specified by the investigator for the null hypothesis. In the BMI example, $\mu_0 = 21.7$.  As in other settings, it is important to be mindful of the distinction between a population mean $\mu$, its specific numerical value $\mu_0$ under $H_0$,  and the observed sample mean $\overline{x}$.  Null and alternative hypotheses are statements about the underlying population, not the observed values from a sample.

\subsubsection{Step 2: Specifying a significance level, $\alpha$}

For the \data{nhanes.samp} BMI data, the observed sample mean was more than 12 standard deviations to the right of the population mean specified in $H_0$ -- an undeniably extreme observation. In contrast, if the sample mean had only been 2 standard deviations to the right of 21.7, it would have been less obvious whether observing a sample mean 2 or more standard deviations larger than 21.7 qualified as a rare enough event to reject the null hypothesis. 

Thus, it is important to specify in advance how rare an event must be in order to represent sufficient evidence against $H_0$. When testing a statistical hypothesis, an investigator specifies a \term{significance level}, $\alpha$, that defines "rare". Typically, $\alpha = 0.05$, though it may be larger or smaller, depending on context; this is discussed in more detail in Section~\ref{significanceLevel}. The value of $\alpha$ will be compared with the probability of the sample mean being observed under the null hypothesis.

\begin{comment}
When testing a null hypothesis $H_0:\mu = \mu_0$ versus the one-sided alternative $H_A:\mu > \mu_0$, $H_0$ is rejected in favor of $H_A$ if, for the observed value $\overline{x}$,
\[P(\overline{X} > \overline{x}) < \alpha\]

where the probability is calculated using the sampling distribution for $\overline{X}$ under $H_0$.  The distinction between $\overline{X}$ and its observed value $\overline{x}$ is an important one. The random variable $\overline{X}$ takes different values in different samples; its observed value $\overline{x}$
is its specific value in the dataset at  hand.  The seemingly small distinction between upper and lower case $X$ and $x$ denotes an important conceptual difference.

When testing $H_0: \mu = \mu_0$ versus  $H_A:\mu < \mu_0$, $H_0$ is rejected in favor of $H_A$ if, 
\[P(\overline{X} < \overline{x}) < \alpha\]

With two-sided alternative  hypotheses, rare events are those in which the sample mean is either unexpectedly large or small.   When testing $H_0: \mu = \mu_0$ versus  $H_A:\mu \ne \mu_0$, $H_0$ is rejected in favor of $H_A$ if, 
\[P(\overline{X} < \overline{x} \text{ or } \overline{X} > \overline{x}) < \alpha\] 

Because the two events in the probability statement are disjoint, the is equivalent to rejecting $H_0$ if 
\[P(\overline{X} < \overline{x}) + P(\overline{X} > \overline{x}) < \alpha\]

\end{comment}

\subsubsection{Step 3: Calculating the test statistic}

Probabilities for sample means are easily calculated using the normal approximation and a transformation to an approximate standard normal variable, so the test statistic most often chosen is 

\begin{align}
t=\frac{\overline{x}-\mu_0}{s/\sqrt{n}}
\end{align}

where $\overline{x}$ is the sample mean, $s$ is the sample standard deviation and $n$ is the number of observations in the sample. The test statistic quantifies the number of standard deviations between the sample mean $\overline{x}$ and the population mean $\mu$.

\subsubsection{Step 4: Calculating the $p$-value}

The \term{$p$-value} is the probability of observing a sample mean as or more extreme than the observed value, under the assumption that the null hypothesis is true. In samples of size 40 or more, $T$ will have a standard normal distribution, unless the data are strongly skewed or extreme outliers are present. Thus, the $p$-value is given by

\begin{itemize}
	\item $P(Z \geq t)$ for $H_A: \mu > \mu_0$,
	
	\item $P(Z \leq t)$ for $H_A: \mu < \mu_0$,
	
	\item $P(Z \leq -t) + P(Z \geq t) = P(Z \geq |t| )$ for $H_A: \mu \neq \mu_0.$
\end{itemize}

\subsubsection{Step 5: Drawing a conclusion}

Once the $p$-value is calculated, $p$ and $\alpha$ can be directly compared. If $p > \alpha$, the observed sample mean is not extreme enough to warrant rejecting $H_0$; more formally stated, there is insufficient evidence to reject $H_0$. If $p < \alpha$, there is sufficient evidence to reject $H_0$ and accept $H_A$. 

For a conclusion to be informative, however, it must be presented in the context of the problem; it is not useful to simply state whether $H_0$ is rejected or not. In the \data{nhanes.samp} BMI data, the $p$-value is extremely small, with the $t$-statistic lying far to the right of the population mean. Thus, the data support the conclusion that the average BMI in the United States is larger than 21.7. 

%JV: Suggested revision -- directly follow w/ Examples of Hypothesis Testing, then Decisions Errors. Expand Decision Errors to have the example that was originally in the Examples section. Main focus of next section should be to clearly illustrate the steps of formal hypothesis. Examples should also include how to choose between one-sided and two-sided tests... After Decision Errors, Choosing Significance Levels, and CI for the last section. 

\subsection{Two examples of hypothesis testing}
\label{hypothesisTestingExamples}

\index{data!school sleep|(}

\subsubsection{One-sided test}

A poll by the National Sleep Foundation found that college students, on average, sleep about 7 hours per night. Researchers at a rural school are interested in showing that students at their school sleep longer than seven hours a night on average. They plan to conduct a study using a sample of students.

The null hypothesis for this test is that the students at this school sleep an average of 7 hours per night, as the national poll results suggest. The alternative hypothesis reflects the stated interest of the researchers -- that students at this school average more than 7 hours of sleep per night. Formally, these hypotheses are: $H_0: \mu = 7$, $H_A: \mu > 7$. Using a one-sided alternative leads to a \term{one-sided} hypothesis test. They intend to conduct a test at a significance level of $\alpha = 0.05$.

The researchers collected a sample random sample of $n = 110$ students on campus. Figure~\ref{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours} shows a histogram of the sleep measurements. Students averaged 7.42 hours of sleep, with standard deviation 1.75 hours. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{ch_inference_foundations_oi_biostat/figures/histOfSleepForCollegeThatWasCheckingForMoreThan7Hours/histOfSleepForCollegeThatWasCheckingForMoreThan7Hours}
	\caption{Distribution of a nightly sleep for 110 college students. These data are strongly right skewed.\index{skew!example: strong}}
	\label{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours}
\end{figure}

Although the data are skewed, with some outliers, the normal approximation can still be used to estimate the standard error because the sample size is sufficiently large. 

\begin{example}{In the study, the sample size is $n = 110$. Students in the sample slept an average of 7.42 per night, with standard deviation 1.75 hours. Calculate the $t$-statistic.}
		
Use the formula for calculating a $t$-statistic. The value $\mu_{0}$ is given by the null hypothesis: $\mu_{0} = 7$. 

\[t=\frac{\overline{x}-\mu_0}{s/\sqrt{n}} = \frac{7.42 - 7}{1.75 / \sqrt{110}} = 2.52\]

\end{example}

Standardizing $\overline{x}$ by converting it to a $t$-statistic allows for a $p$-value to be easily calculated from a standardized normal distribution. The probability $P(Z \geq 2.52)$ equals 0.006; this value is represented by the shaded tail in Figure~\ref{pValueOneSidedSleepStudy}. The $p$-value of 0.006 indicates that if the null hypothesis is true, then the probability of observing a sample mean at least as large as 7.42 hours is only 0.006. The observed sample mean is so extreme with respect to the null hypothesis that it represents evidence against the null hypothesis.

The $p$-value is less than the significance level $\alpha = 0.05$; thus, there is sufficient evidence to reject $H_0$ in favor of accepting $H_A$. The data suggest that the true average hours of sleep per night for students at the rural school is larger than 7 hours. 

\begin{figure}[hht]
	\centering
	\includegraphics[width=0.83\textwidth]{ch_inference_foundations_oi_biostat/figures/pValueOneSidedSleepStudy/pValueOneSidedSleepStudy}
	\caption{If the null hypothesis is true, then the population can be modeled by a normal distribution centered at $\mu = 7$. The shaded area in the right tail represents the probability of observing a sample mean as or more extreme than $\overline{x} = 7.42$.} 
	\label{pValueOneSidedSleepStudy}
\end{figure}

\begin{exercise}
The $p$-value for the data was 0.006. Suppose the investigators had used a significance level of 0.01 in the study. Would the evidence have been strong enough to reject the null hypothesis? What if the significance level were $\alpha = 0.001$?\footnote{There is sufficient evidence to reject $H_{0}$ when $p < \alpha$. Thus, the evidence would still have been strong enough for $\alpha = 0.01$, but not $\alpha = 0.001$.}
\end{exercise}

\subsubsection{Two-sided test}

In the earlier example, researchers investigated whether the students at a rural school slept longer than 7 hours each night. Suppose that a group of researchers at a different college want to evaluate whether the students at their school differ from the norm of 7 hours. 

For this scenario, while the null hypothesis ($H_0: \mu = 7 \text{ hours}$) remains the same as before, the alternative hypothesis is a two-sided one, $H_A: \mu \neq 7\text{ hours}$. The researchers are looking for evidence against the null in either direction; in other words, whether there is evidence that students are either sleeping less than 7 hours per night or more than 7 hours. 

\begin{example}{At the second college, the researchers randomly sampled 122 students. The data have mean $\overline{x} = 6.83 \text{ hours}$ and standard deviation $s = 1.8 \text{ hours}$. Let $\alpha$ = 0.05. Do the data provide sufficient evidence against $H_0$?} 
	
	First, calculate the $t$-statistic. 
	
	\[t=\frac{\overline{x}-\mu_0}{s/\sqrt{n}} = \frac{6.83 - 7}{1.8 / \sqrt{122}} = -1.04\]
	
	When calculating the $p$-value, note that the area from both the right and left tails must be accounted for, since this is a two-sided hypothesis test. Since the normal model is symmetric, one tail has the same area as the other, and the $p$-value is the sum of the two (Figure~\ref{2ndSchSleepHTExample}). Thus, the $p$-value is given by:
	\begin{align*}
	p = P(Z \leq -t) + P(Z \geq t) &= P(Z \geq |t|) \\
		&= 2 \times P(Z \leq -t) \\
		&= 2 \times 0.149 \\
		&= 0.298
	\end{align*}

	Since the $p$-value is larger than $\alpha$, there is insufficient evidence to reject $H_0$. If $H_0$ is true, and the population mean is 7 hours of sleep per night, then it would not be unusual to observe a sample mean as or more extreme than 1.04 standard deviations away from the mean -- this would occur 29.8\% of the time. Thus, the data do not provide evidence that the true average hours of sleep per night for students at this school is different from 7 hours.
\end{example}
	
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]
	{ch_inference_foundations_oi_biostat/figures/2ndSchSleepHTExample/2ndSchSleepHTExample}
	\caption{$H_A$ is two-sided, so both tails must be counted for the $p$-value.}
	\label{2ndSchSleepHTExample}
\end{figure}	
	
\index{data!school sleep|)}

\subsubsection{Choosing between one-sided and two-sided tests}

\textit{To be added: more examples of contexts where a one-sided test is more appropriate and contexts when a two-sided test is recommended, detailed explanation of why two-sided tests are more "conservative", e.g. when a two-sided test rejects and a one-sided test accepts.} 

%JV: Not sure that the OI example of one-sided vs. two-sided explained in relation to error is very effective. Show an example of why two-sided tests are more conservative, e.g. when a two-sided test rejects and a one-sided accepts

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]
	{ch_inference_foundations_oi_biostat/figures/twoSidedTestConservative/twoSidedTestConservative}
	\caption{Under a one-sided test at significance level $\alpha$ = 0.05, a $t$-statistic of 1.80 is within the rejection region. However, it would not be within the rejection region under a two-sided test with $\alpha$ = 0.05.}
	\label{twoSidedTestConservative}
\end{figure}

\subsection{Decision errors}

Hypothesis tests can potentially result in incorrect decisions, such as rejecting the null hypothesis when the null is actually true. There are four possible ways that the conclusion of a test can be either right or wrong, as shown in Table~\ref{fourHTScenarios}. 

\begin{table}[ht]
	\centering
	\begin{tabular}{l l c c}
		& & \multicolumn{2}{c}{\textbf{Test conclusion}} \\
		\cline{3-4}
		\vspace{-3.7mm} \\
		& & do not reject $H_0$ &  reject $H_0$ in favor of $H_A$ \\
		\cline{2-4}
		\vspace{-3.7mm} \\
		& $H_0$ true & Correct Decision &  Type~1 Error \\
		\raisebox{1.5ex}{\textbf{Truth}} & $H_A$ true & Type~2 Error & Correct Decision\\
		\cline{2-4}
	\end{tabular}
	\caption{Four different scenarios for hypothesis tests.}
	\label{fourHTScenarios}
\end{table}

Rejecting the null hypothesis when the null is true is referred to as a \term{Type 1 error}, while a \term{Type 2 error} refers to failing to reject the null hypothesis when the alternative is true. 

The probability of making a Type 1 error is equivalent to the significance level $\alpha$, since $\alpha$ determines the cutoff point for rejecting the null hypothesis. For example, if $\alpha$ is set to 0.05, then there is a 5\% chance of incorrectly rejecting $H_0$. 

Section~\ref{hypothesisTesting} introduced the logic of formal hypothesis testing by comparing it to the principle of presumption of innocence; a defendant is assumed innocent until proven guilty beyond a reasonable doubt. The following exercises extend the analogy to illustrate the relationship between Type 1 and Type 2 errors.

\begin{exercise} \label{whatAreTheErrorTypesInUSCourts}
	In a trial, the defendant is either innocent ($H_0$) or guilty ($H_A$). After hearing evidence from both the prosecution and the defense, the court must reach a verdict. What does a Type~1 Error represent in this context? What does a Type~2 Error represent?\footnote{If the court makes a Type~1 error, this means the defendant is innocent, but wrongly convicted (rejecting $H_0$ when $H_0$ is true). A Type~2 error means the court failed to convict a defendant that was guilty (failing to reject $H_0$ when $H_0$ is false).}
\end{exercise}

\begin{example}{How might the rate of Type 1 errors be reduced? What effect would this have on the rate of Type 2 errors?}
	
	To lower the rate of Type 1 error, the court could raise the standards for conviction such that fewer people are wrongly convicted. In a test, this is equivalent to lowering $\alpha$ (e.g. to 0.01 instead of 0.05); in other words, requiring an observation to be more extreme to qualify as sufficient evidence against $H_0$. However, this would also inevitably mean that fewer people who are actually guilty are convicted, raising the rate of Type 2 errors.
\end{example}

\begin{exercise} \label{howToReduceType2ErrorsInUSCourts}
	How might the rate of Type 2 errors be reduced? What effect would this have on the rate of Type 1 errors?\footnote{To lower the rate of Type 2 error, the court could lower the standards for conviction, or in other words, lower the bar for what constitutes sufficient evidence of guilt (increase $\alpha$, e.g. to 0.10 instead of 0.05). This will result in more guilty people being convicted, but also increase the rate of wrongful convictions, increasing the Type 1 error.}
\end{exercise}

\index{hypothesis testing!decision errors|)}

\subsubsection{Choosing a significance level}

As demonstrated by the exercises, reducing the error rate of one type of error results in making more of the other type. Due to this relationship between Type 1 and Type 2 error, it is helpful to adjust the significance level of a hypothesis test based on the consequences of any conclusions reached.

%\subsection{Choosing a significance level}
\label{significanceLevel}

\index{hypothesis testing!significance level|(}
\index{significance level|(}

If making a Type 1 error is especially dangerous or costly, a small significance level (e.g. 0.01) is advisable. Under this scenario, it is best to err on the side of caution about rejecting the null hypothesis, so very strong evidence favoring $H_A$ is required in order to reject $H_0$. Conversely, if a Type 2 error is relatively more dangerous, then a higher significance level (e.g. 0.10) is used. 

For example, consider the decision making process involved in drug research. In the early testing of a relatively benign drug, it may be important to continue further testing even if there is not strong initial evidence for any beneficial effect. If the scientists conducting the research know that any initial positive results will eventually require confirmation in a larger study, they might choose to use $\alpha = 0.10$ to reduce the chances of making a Type 2 error: prematurely ending research into what may turn out to be a promising drug. 

A government agency responsible for approving drugs to be marketed to the general population, however, would likely be biased towards minimizing the chances of making a Type 1 error -- approving a drug that turns out to be unsafe or ineffective. As a result, they might conduct tests at significance level 0.01 in order to reduce the chances of concluding that a drug works when it is in fact ineffective. 

\index{significance level|)}
\index{hypothesis testing!significance level|)}
\index{hypothesis testing|)}

\begin{comment}

\subsection{Hypothesis testing and confidence intervals}

\textit{JV: The OI section "Hypothesis testing using confidence intervals" doesn't seem very useful. Recommend putting in this section what we generally cover in 102 -- comparison of the information a CI gives vs. a hypothesis test at same $\alpha$.}

\vspace{3cm}

\begin{comment}

\begin{comment}

\subsection{Decision errors}

\textit{DH: this original OI section needs revision, and may rely on concepts not well explained in the text above}
\index{hypothesis testing!decision errors|(}

A confidence interval may be incorrect and not contain the population parameter, though the study investigator will not know of the error because the population parameter is unknown.  Similarly, hypothesis tests can also lead to incorrect decisions.  By thinking of hypothesis testing as a decision problem, it is possible to quantify how often such errors are likely to  happen.

In testing, there are two competing hypotheses: the null and the alternative. In the conclusion of a test (step 5), an investigator makes a statement about whether or not she finds the alternative hypothesis credible. There are four ways the investigator's statement can be either right or wrong, and they are summarized in Table~\ref{fourHTScenarios}.

\begin{table}[ht]
\centering
\begin{tabular}{l l c c}
& & \multicolumn{2}{c}{\textbf{Test conclusion}} \\
  \cline{3-4}
\vspace{-3.7mm} \\
& & do not reject $H_0$ &  reject $H_0$ in favor of $H_A$ \\
  \cline{2-4}
\vspace{-3.7mm} \\
& $H_0$ true & Correct Decision &  Type~1 Error \\
\raisebox{1.5ex}{\textbf{Truth}} & $H_A$ true & Type~2 Error & Correct Decision\\
  \cline{2-4}
\end{tabular}
\caption{Four different scenarios for hypothesis tests.}
\label{fourHTScenarios}
\end{table}

A \term{Type~1 Error} is rejecting the null hypothesis when $H_0$ is true. A \term{Type~2 Error} is failing to reject the null hypothesis when the alternative is true.

\begin{exercise} \label{whatAreTheErrorTypesInUSCourts}
In a US court, the defendant is either innocent ($H_0$) or  guilty ($H_A$). What does a Type~1 Error represent in this context? What does a Type~2 Error represent? Table~\ref{fourHTScenarios} may be useful.\footnote{If the court makes a Type~1 Error, this means the defendant is innocent ($H_0$ true) but wrongly convicted. A Type~2 Error means the court failed to reject $H_0$ (i.e. failed to convict the person) when she was in fact guilty ($H_A$ true).}
\end{exercise}

\begin{exercise} \label{howToReduceType1ErrorsInUSCourts}
How could we reduce the Type~1 Error rate in US courts? What influence would this have on the Type~2 Error rate?\footnote{To lower the Type~1 Error rate, we might raise our standard for conviction from ``beyond a reasonable doubt'' to ``beyond a conceivable doubt'' so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type~2 Errors.}
\end{exercise}

\begin{exercise} \label{howToReduceType2ErrorsInUSCourts}
How could we reduce the Type~2 Error rate in US courts? What influence would this have on the Type~1 Error rate?\footnote{To lower the Type~2 Error rate, we want to convict more guilty people. We could lower the standards for conviction from ``beyond a reasonable doubt'' to ``beyond a little doubt''. Lowering the bar for guilt will also result in more wrongful convictions, raising the Type~1 Error rate.}
\end{exercise}

\index{hypothesis testing!decision errors|)}

Exercises~\ref{whatAreTheErrorTypesInUSCourts}-\ref{howToReduceType2ErrorsInUSCourts} provide an important lesson: if we reduce how often we make one type of error, we make more of the other type.

Hypothesis testing is built around rejecting or failing to reject the null hypothesis. The null hypothesis $H_0$ is not rejected unless there is strong evidence against it. But what precisely does \emph{strong evidence} mean? As a general rule of thumb, for those cases where the null hypothesis is true,  $H_0$ is not incorrectly rejected more than 5\% of the time. This corresponds to a $\alpha$ of 0.05. The significance level is not always chosen to be 0.05; the appropriateness of different significance levels in Section~\ref{significanceLevel}.

\subsection{Two examples of hypothesis testing}
\label{hypothesisTestingExamples}

% \index{hypothesis testing!p-value|(}

The $p$-value is a way of quantifying the strength of the evidence against the null hypothesis and in favor of the alternative. 

\index{data!school sleep|(}

\begin{exercise} \label{skepticalPerspOfRuralSchoolSleepExercise}
A poll by the National Sleep Foundation found that college students average about 7 hours of sleep per night. Researchers at a rural school are interested in showing that students at their school sleep longer than seven hours on average, and they would like to demonstrate this using a sample of students. What would be an appropriate skeptical position for this research?\footnote{A skeptic would have no reason to believe that sleep patterns at this school are different than the sleep patterns at another school.}
\end{exercise}

The null hypothesis for this test is that students at this school average 7 hours of sleep per night. The alternative hypothesis reflects the interests of the research: the students average more than 7 hours of sleep. Formally, these hypotheses are
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[$H_0$:] $\mu = 7$.
\item[$H_A$:] $\mu > 7$.
\end{itemize}

Using $\mu > 7$ as the alternative is leads to a \term{one-sided} hypothesis test. In this investigation, there is no apparent interest in learning whether the mean is less than 7~hours.\footnote{This is entirely based on the interests of the researchers. Had they been only interested in the opposite case -- showing that their students were actually averaging fewer than seven hours of sleep but not interested in showing more than 7 hours -- then set the alternative would have been $\mu < 7$.} 

In general, a two-sided test should be used unless it was made clear prior to data collection that the test should be one-sided. Switching a two-sided test to a one-sided test after observing the data is dangerous because it can inflate the Type~1 Error rate. \textit{explanation comes later but should be move here.  Also, it may be better to do a two-sided test with this example.  We can do that in a revision.}

\begin{tipBox}{\tipBoxTitle{One-sided and two-sided tests}
When you are interested in checking for an increase or a decrease, but not both, use a one-sided test. When you are interested in any difference from the null value~--~an increase or decrease~--~then the test should be two-sided.\vspace{0.5mm}}
\end{tipBox}

The researchers at the rural school conducted a simple random sample of $n=110$ students on campus. They found that these students averaged 7.42 hours of sleep with standard deviation  1.75 hours.  Figure~\ref{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours} shows a histogram of the sleep measurements.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{ch_inference_foundations_oi_biostat/figures/histOfSleepForCollegeThatWasCheckingForMoreThan7Hours/histOfSleepForCollegeThatWasCheckingForMoreThan7Hours}
\caption{Distribution of a night of sleep for 110 college students. These data are strongly skewed.\index{skew!example: strong}}
\label{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours}
\end{figure}

%JV: The actual size of the student body wasn't stated before, so it is not a given that the sample represents less than 10\% of the population.

The normal model for the sample mean can be used here because: (1) this is a simple random sample from less than 10\% of the student body, the observations are independent; (2)~the sample size in the sleep study is sufficiently large since it is greater than 30; (3)~The data show strong skew in Figure~\ref{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours} and the presence of a couple of outliers. This skew and the outliers are acceptable for a sample size of $n=110$. 

\begin{exercise} \label{findSEOfFirstSleepStudyCheckingGreaterThan7Hours}
In the sleep study, the sample standard deviation was 1.75 hours and the sample size is 110. Calculate the standard error of $\bar{x}$.\footnote{The standard error can be estimated from the sample standard deviation and the sample size: $SE_{\bar{x}} = \frac{s_x}{\sqrt{n}} = \frac{1.75}{\sqrt{110}} = 0.17$.}
\end{exercise}

The hypothesis test for the sleep study will be evaluated using a significance level of $\alpha = 0.05$. If $H_0$ is true, the sample mean is from a distribution that is nearly normal with mean 7 and standard deviation of about $SE_{\bar{x}} = 0.17$. Such a distribution is shown in Figure~\ref{pValueOneSidedSleepStudy}.

\begin{figure}[hht]
   \centering
   \includegraphics[width=0.83\textwidth]{ch_inference_foundations_oi_biostat/figures/pValueOneSidedSleepStudy/pValueOneSidedSleepStudy}
   \caption{If the null hypothesis is true, then the sample mean $\bar{x}$ came from this nearly normal distribution. The right tail describes the probability of observing such a large sample mean if the null hypothesis is true.}
   \label{pValueOneSidedSleepStudy}
\end{figure}

The shaded tail in Figure~\ref{pValueOneSidedSleepStudy} represents the chance of observing such a large mean if the null hypothesis is true. That is, the shaded tail represents the \mbox{p-value}. All means larger than the sample mean, $\bar{x} = 7.42$ are shaded, because they are more favorable to the alternative hypothesis than the observed mean.

The $p$-value is the tail area of this normal distribution, calculated using techniques in Section~\ref{normalDist}. First compute the Z-score of the sample mean, $\bar{x} = 7.42$:
\begin{eqnarray*}
Z = \frac{\bar{x} - \text{null value}}{SE_{\bar{x}}} = \frac{7.42 - 7}{0.17} = 2.47
\end{eqnarray*}
Using the normal probability table, the lower unshaded area is  be 0.993. Thus the shaded area is $1-0.993 = 0.007$. {\em If the null hypothesis is true, the probability of observing a sample mean at least as large as 7.42 hours for a sample of 110 students is only 0.007.}\index{p-value!interpretation example} That is, if the null hypothesis is true, we would not often see such a large mean.

The hypotheses are tested by comparing the $p$-value to the significance level. Because the $p$-value is less than the significance level ($p$-value $=0.007 < 0.05=\alpha$), the null hypothesis is rejected. The data are so unusual with respect to the null hypothesis that they casts serious doubt on $H_0$ and provide strong evidence favoring $H_A$.

\begin{termBox}{\tBoxTitle{$p$-value as a tool in hypothesis testing}
The smaller the p-value, the stronger the data favor $H_A$ over $H_0$. A small p-value (usually $<0.05$) corresponds to sufficient evidence to reject $H_0$ in favor of $H_A$.}
\index{hypothesis testing!p-value|)}
\end{termBox}

\begin{tipBox}{\tipBoxTitle{It is useful to first draw a picture to find the p-value}
It is useful to draw a picture of the distribution of $\bar{x}$ as though $H_0$ was true (i.e.~$\mu$~equals the null value), and shade the region (or regions) of sample means that are at least as favorable to the alternative hypothesis. These shaded regions represent the p-value.}
\end{tipBox}


The $p$-value is constructed in such a way that it can be directly compared to the significance level ($\alpha$) to determine whether or not to reject $H_0$. This method ensures that the Type~1 Error rate does not exceed the significance level standard. 

\begin{figure}[ht]
   \centering
   \includegraphics[width=0.9\textwidth]
{ch_inference_foundations_oi_biostat/figures/pValueOneSidedSleepStudyExplained/pValueOneSidedSleepStudyExplained}
   \caption{To identify the p-value, the distribution of the sample mean is considered as if the null hypothesis was true. Then the p-value is defined and computed as the probability of the observed $\bar{x}$ or an $\bar{x}$ even more favorable to $H_A$ under this distribution.}
   \label{pValueOneSidedSleepStudyExplained}
\end{figure}

\begin{exercise}
If the null hypothesis is true, how often should the p-value be less than 0.05?\footnote{About 5\% of the time. If the null hypothesis is true, then the data only has a 5\% chance of being in the 5\% of data most favorable to $H_A$.}
\index{data!school sleep|)}
\end{exercise}

\begin{exercise}
Suppose we had used a significance level of 0.01 in the sleep study. Would the evidence have been strong enough to reject the null hypothesis? (The p-value was 0.007.) What if the significance level was $\alpha = 0.001$? \footnote{We reject the null hypothesis whenever $p$-$value < \alpha$. Thus, we would still reject the null hypothesis if $\alpha = 0.01$ but not if the significance level had been $\alpha = 0.001$.}
\end{exercise}

Here is an example with a two-sided alternative. In one-sided tests, the single tail in the direction of the alternative hypothesis is shaded. For example, when the alternative had the form $\mu > 7$, then the p-value was represented by the upper tail (Figure~\ref{pValueOneSidedSleepStudyExplained}).  In a two-sided test, since evidence in either direction is favorable to $H_A$ so both tails are shaded.

\begin{exercise} \label{2ndSchSleepHypSetupExercise}
The earlier example investigated whether the students at a school slept longer than 7 hours each night. Suppose a second group of researchers wanted to evaluate whether the students at their college differ from the norm of 7 hours. Write the null and alternative hypotheses for this investigation.\footnote{Because the researchers are interested in any difference, they should use a two-sided setup: $H_0: \mu = 7$, $H_A: \mu \neq 7$.}
\end{exercise}

\begin{example}{Suppose the second college randomly sampled 122 students and found a mean of $\bar{x} = 6.83$ hours and a standard deviation of $s=1.8$ hours. Does this provide strong evidence against $H_0$ in Guided Practice~\ref{2ndSchSleepHypSetupExercise}? Use a significance level of $\alpha=0.05$.}
The required assumptions hold here. (1) A simple random sample of less than 10\% of the student body means the observations are independent. (2) The sample size is 122, which is greater than 30. (3) Based on the information in the earlier distribution, the sample size will be sufficient to to mitigate the effect of skewness or outliers.

The standard error of the estimate is $SE_{\bar{x}} = \frac{s}{\sqrt{n}} = 0.16$;  a picture  representing the $p$-value is shown in Figure~\ref{2ndSchSleepHTExample}. Both tails are shaded. An estimate of 7.17 or more provides at least as strong of evidence against the null hypothesis and in favor of the alternative as the observed estimate, $\bar{x} = 6.83$.

The calculation the tail areas starts by first finding the lower tail corresponding to $\bar{x}$:
\begin{eqnarray*}
Z = \frac{6.83 - 7.00}{0.16} = -1.06 \quad\stackrel{table}{\rightarrow}\quad \text{left tail} = 0.1446
\end{eqnarray*}
Because the normal model is symmetric, the right tail will have the same area as the left tail. The $p$-value is found as the sum of the two shaded tails:
\begin{eqnarray*}
\text{p-value} = \text{left tail} + \text{right tail} = 2\times(\text{left tail}) = 0.2892
\end{eqnarray*}
This p-value is relatively large (larger than $\alpha=0.05$), so  $H_0$ is not rejected. That is, if $H_0$ is true, it would not be very unusual to see a sample mean this far from 7 hours simply due to sampling variation. 
\end{example}

\index{data!school sleep|)}

\begin{example}{It not correct to change two-sided tests to one-sided tests after observing the data.  This example explores the consequences of making that error. Using $\alpha=0.05$, freely switching from two-sided tests to one-sided tests cause twice as many Type~1 Errors as intended.} \label{swappingHypAfterDataDoublesType1ErrorRate}
Suppose the sample mean was larger than the null value, $\mu_0$ (e.g. $\mu_0$ would represent~7 if $H_0$:~$\mu = 7$). Flipping to a one-sided test $H_A$: $\mu > \mu_0$, any observation with a Z-score greater than 1.65, would lead to  rejecting  $H_0$. If the null hypothesis is true, the null hypothesis is incorrectly rejected about 5\% of the time when the sample mean is above the null value, as shown in Figure~\ref{type1ErrorDoublingExampleFigure}.

Suppose the sample mean was smaller than the null value.  Changing to a one-sided alternative $H_A$: $\mu < \mu_0$, a $\bar{x}$ with a Z-score smaller than -1.65, we would lead to rejecting $H_0$. If the null hypothesis is true, this would happen about 5\% of the time.

These two scenarios, it can be shown that a Type~1 Error will occur about $5\%+5\%=10\%$ of the time whenever the test is swapped for ``best'' one-sided test for the data. This is twice the error rate prescribed with significance level: $\alpha=0.05$.

\begin{figure}
   \centering
   \includegraphics[width=0.7\textwidth]{ch_inference_foundations_oi_biostat/figures/type1ErrorDoublingExampleFigure/type1ErrorDoublingExampleFigure}
   \caption{The shaded regions represent areas where we would reject $H_0$ under the bad practices considered in Example~\ref{swappingHypAfterDataDoublesType1ErrorRate} when $\alpha = 0.05$.}
   \label{type1ErrorDoublingExampleFigure}
\end{figure}

\end{example}

\begin{caution}{One-sided hypotheses are allowed only \emph{before} seeing data}
{After observing data, it is tempting to turn a two-sided test into a one-sided test. Avoid this temptation. Hypotheses must be set up \emph{before} observing the data. If~they are not, the test should be two-sided.}
\end{caution}


\subsection{Hypothesis testing using confidence intervals}

When a 95\% confidence interval is used  for testing in the situation where $H_0$ is true, an error is made whenever the point estimate is at least 1.96 standard errors away from the population parameter. This happens about 5\% of the time (2.5\% in each tail). Similarly, using a 99\% confidence interval to evaluate a hypothesis is equivalent to a significance level of $\alpha = 0.01$.

A confidence interval provides important but limited information about a test.  Consider the following two scenarios:
\begin{itemize}
\setlength{\itemsep}{0mm}
\item The null value (the parameter value under the null hypothesis) is in the 95\% confidence interval but just barely, so $H_0$ is not rejected. It would be useful to report in some way,however, that it was a close decision.
\item The null value is very far outside of the interval, so we reject $H_0$. This dise not communicate that the decision was not even close. Such a case is depicted in Figure~\ref{whyWeWantPValue}.
\end{itemize}
The $p$-value introduced in Section~\ref{formalHypothesisTesting}, is helpful in these cases. The $p$-value method also extends to hypothesis tests where confidence intervals cannot be easily constructed or applied.

\begin{figure}[hht]
\centering
\includegraphics[width=0.75\textwidth]
{ch_inference_foundations_oi_biostat/figures/whyWeWantPValue/whyWeWantPValue}
\caption{It would be helpful to quantify the strength of the evidence against the null hypothesis. In this case, the evidence is extremely strong.}
\label{whyWeWantPValue}
\end{figure}


%% the next section is taken directly from OI and needs to be revised, and trimmed.

\subsection{Choosing a significance level}
\label{significanceLevel}

\index{hypothesis testing!significance level|(}
\index{significance level|(}

Choosing a significance level for a test is important in many contexts, and the traditional level is 0.05. However, it is often helpful to adjust the significance level based on the application. We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test.

If making a Type~1 Error is dangerous or especially costly, we should choose a small significance level (e.g. 0.01). Under this scenario we want to be very cautious about rejecting the null hypothesis, so we demand very strong evidence favoring $H_A$ before we would reject $H_0$.

If a Type~2 Error is relatively more dangerous or much more costly than a Type~1 Error, then we should choose a higher significance level (e.g. 0.10). Here we want to be cautious about failing to reject $H_0$ when the null is actually false.

In the early testing of the effectiveness of a relatively benign drug in treating a disease, it may be important to continue further testing even if the evidence for a beneficial effect is not quite as strong as is used in more traditional significance.  If scientists studying the drug know that initial positive results will have to be confirmed in larger study, they might use $\alpha = 0.10$ instead of the stricter $0.05$.  If, on the other hand, the drug is likely to show serious, perhaps even fatal side effects, it may be prudent to use $\alpha = 0.01$ in initial testing, to reduce the chance of concluding that the drug works when it is not effective.


\begin{tipBox}{\tipBoxTitle[]{Significance levels should reflect consequences of errors}
The significance level selected for a test should reflect the consequences associated with Type~1 and Type~2 Errors.}
\end{tipBox}


\index{significance level|)}
\index{hypothesis testing!significance level|)}
\index{hypothesis testing|)}

\end{comment}

\end{spacing}
 


