%!TEX root=../../main.tex

%\begin{doublespace}
\begin{spacing}{1.5}

\chapter{Foundations for inference}
\label{foundationsForInference}

\begin{comment}
Issues pending

-- Remove the poker example in the clt section.


-- Missing section on using confidence intervals for tests.

	
\end{comment}

On average, how many days a week are high school students physically active? How many hours of sleep per night do they get? These questions about the youth population in the United States could be answered either by collecting information from all 21.2 million high school aged youth (ages 15-19) or by calculating  estimates based on a well-chosen sample of students. The first strategy is simply impossible, while the second represents a primary goal of statistics -- drawing inferences about the characteristics of a population from a sample. In statistical terms, a characteristic of a population, such as the average amount of sleep per night for high school aged students, is called a \term{population parameter}. When carried out correctly, sampling from a population is an efficient way to estimate a population parameter. This chapter introduces the important ideas in drawing estimates from samples by discussing methods of inference for a population mean, $\mu$. 

This chapter also discusses three widely used tools in statistics: point estimates (single number estimates) for a population mean, interval estimates that include both a point estimate and a margin of error, and a method for testing scientific hypotheses about $\mu$. The concepts used in this chapter will appear throughout the rest of the book. While particular equations or formulas may change to reflect the details of a problem at hand, the fundamental ideas will not. 

\index{data!yrbss|(}

The United States Centers for Disease Control and Prevention (CDC) lists one of their roles as "promoting healthy and safe behaviors, communities, and environment."\footnote{\url{http://www.cdc.gov/about/organization/mission.htm}} The first step in health promotion is understanding health behaviors; the CDC periodically conducts several surveys, including the Youth Risk Behavior Surveillance System (YRBSS).\footnote{\url{http://www.cdc.gov/healthyyouth/data/yrbs/index.htm}} Between 1991 and 2013, 2.6 million high school students have participated in more than 1,100 separate surveys. The next few sections discuss the dataset \data{yrbss}, which contains the responses of the 13,583 high school students who participated in the 2013 YRBSS.\footnote{\oiRedirect{textbook-yrbss}{www.cdc.gov/healthyyouth/data/yrbs/data.htm}} Part of this dataset is shown in Table~\ref{yrbssDF}, with the variables described in Table~\ref{yrbssVariables}.

\begin{table}[h]
\centering
\begin{tabular}{rrllrrlrr}
  \hline
ID & age & gender & grade & height & weight & helmet & active & lifting \\ 
  \hline
1 &  14 & female & 9 &  &  & never &   4 &   0 \\ 
  2 &  14 & female & 9 &  &  & never &   2 &   0 \\ 
  3 &  15 & female & 9 & 1.73 & 84.37 & never &   7 &   0 \\ 
  $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
  13582 &  17 & female & 12 & 1.60 & 77.11 & sometimes &   5 &  \\ 
  13583 &  17 & female & 12 & 1.57 & 52.16 & did not ride &   5 &  \\ 
  \hline
\end{tabular}
\caption{Five cases from the \data{yrbss} dataset. Blank observations represent missing data. For example, the height and weight of students 1 and 2 are missing\textC{\vspace{-2mm}}}
\label{yrbssDF}
\end{table}
% library(openintro); library(xtable); data(yrbss); xtable(rbind(head(yrbss, 4), tail(yrbss, 2))[, c("age", "gender", "grade", "height", "weight", "helmet_12m", "physically_active_7d", "strength_training_7d")])

\begin{table}[h]
\centering\small
\begin{tabular}{l p{110mm}}
\hline
{\bf age} & {\bf Age of the student.} \\
\hline
\var{gender} & {Sex of the student.} \\
\var{grade} & Grade in high school (9-12) \\
\var{height} & Height, in meters. (1 m = ~3.28 ft) \\
\var{weight} & Weight, in kilograms (1 kg = ~2.2 lbs) \\
\var{helmet} & Frequency that the student wore a helmet while biking in the last 12~months. \\
\var{active} & Number of days physically active for 60+ minutes in the last 7 days. \\
\var{lifting} & Number of days of strength training (e.g. lifting weights) in the last 7 days. \\
\hline
\end{tabular}
\caption{Variables and their descriptions for the \data{yrbss} data set.}
\label{yrbssVariables}
\end{table}

\index{data!yrbss.samp|(}

CDC public health scientists used the responses of 13,572 students to estimate the health behaviors of a \term{target population}: the approximately 21.2 million high school aged students in the US population in 2013. 

This chapter illustrates inference for a population mean by treating the CDC sample of 13,582 students as an artificial target population. A random sample of 100 participants (\data{yrbss.samp}) can then be used to estimate health behaviors for the "target population" of 13,572 respondents. In other words, this chapter will demonstrate how, with only the information from 100 students, it is possible to estimate the behaviors for 13,572 students (and by extension, show how the CDC used information from 13,572 students to estimate health behaviors for 21.2 million students).  

While the individuals in \data{yrbss} are not truly a target population, treating them as such allows for the estimates obtained by inference (using \data{yrbss.samp}) to be checked against the "population parameters" of \data{yrbss} -- not possible in a realistic setting, since population parameters (such as the average hours of sleep per night across 21.2 million students) are typically unknown.\footnote{Hence, the need for inference!}

The dataset \data{yrbss.samp} contains the data for 100 student responses randomly sampled from the larger \data{yrbss} dataset (Table~\ref{yrbssSampDF}).\footnote{About 10\% of high schoolers for each variable chose not to answer the question. Multiple regression (see Chapter~\ref{multipleAndLogisticRegression}) was used to predict what those responses would have been. For simplicity, we will assume that these predicted values can be used for the unknown responses.} Histograms summarizing the \var{height}, \var{weight}, \var{active}, and \var{lifting} variables from \data{yrbss.samp} are shown in Figure~\ref{yrbssSampHistograms}.

\begin{table}
\centering
\begin{tabular}{rrllrrlrr}
  \hline
ID & age & gender & grade & height & weight & helmet & active & lifting \\ 
  \hline
5653 &  16 & female & 11 & 1.50 & 52.62 & never &   0 &   0 \\ 
  9437 &  17 & male & 11 & 1.78 & 74.84 & rarely &   7 &   5 \\ 
  2021 &  17 & male & 11 & 1.75 & 106.60 & never &   7 &   0 \\ 
  $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
  2325 &  14 & male & 9 & 1.70 & 55.79 & never &   1 &   0 \\ 
   \hline
\end{tabular}
\caption{Four observations for the \data{yrbss.samp} data set, which represents a simple random sample of 100 high schoolers from the 2013 YRBSS.}
\label{yrbssSampDF}
% library(openintro); library(xtable); data(yrbss); xtable(rbind(head(yrbss.samp, 3), tail(yrbss.samp, 1))[, c("age", "gender", "grade", "height", "weight", "helmet_12m", "physically_active_7d", "strength_training_7d")])
%library(openintro); library(xtable); data(yrbss); data(yrbss.samp); xtable(yrbss.amp[c(1,2,3,100),])
\end{table}


% WARNING: This figure is referenced in Section 4.2
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]
{ch_inference_foundations_oi_biostat/figures/yrbssSampHistograms/yrbssSampHistograms} 
\caption{Histograms of \var{height}, \var{weight}, \var{activity}, and \var{lifting} for the sample data (\data{yrbss.samp}). The \var{height} distribution is approximately symmetric, \var{weight} is moderately skewed to the right, \var{activity} is bimodal or multimodal (with unclear skew), and \var{lifting} is strongly right skewed.\index{skew!example: moderate}\index{skew!example: strong}}
\label{yrbssSampHistograms}
\end{figure}

%__________________
\section[Variability in estimates]{Variability in estimates} %\sectionvideohref{youtube-DNIauUrRIEM&list=PLkIselvEzpM7Pjo94m1e7J5jkIZkbQAl4}}
\label{variabilityInEstimates}

\index{point estimate|(}

The \data{yrbss.samp} data can be used to estimate four features of the 13,582 high school students in \data{yrbss}: 1) average height (in meters), 2) average weight, 3) average number of days per week physically active (for more than 60 minutes at a time), 4) average body mass index (BMI).

Sample means are the natural choice of summary statistic for estimating a population mean. The average height of the students in \data{yrbss.samp} is:

\begin{align*}
\overline{x}_{\text{height}} = \frac{1.50 + 1.78 + \dots + 1.70}{100} = 1.697.
\end{align*}
%library(openintro); data(yrbss.samp); mean(yrbss.samp$height); yrbss.samp$height

The sample mean $\overline{x} = 1.697$ meters (5 feet, 6.8 inches) is a \term{point estimate} of the population mean. If a second random sample of 100 were taken, the new sample mean would likely be different as a result of \term{sampling variation}.  Estimates generally vary from one sample to another, whereas the population mean is a fixed value; thus, the distinction between a sample mean versus a population mean is important. 

The sample means of \var{weight} and \var{active} provide estimates of the average weight and number of days active per week of YRBSS respondents. On average, students weigh 68.89 kilograms (about 151.6 pounds) and are active 3.75 days per week:
\begin{align*}
\overline{x}_{weight} &= \frac{52.6 + 74.8 + \dots + 55.8}{100} = 68.89
&\overline{x}_{active} &= \frac{0 + 7 + \dots + 1}{100} = 3.75.
\end{align*}
%library(openintro); data(yrbss.samp); d <- yrbss.samp$weight; mean(d); d
%library(openintro); data(yrbss.samp); d <- yrbss.samp$physically_active_7d; mean(d); d

BMI is used by health professionals to gauge whether an individual's weight is consistent with their height. While BMI is not one of the variables in the dataset, it can be calculated from height and weight (measured in metric units) with the formula:

\[ \text{bmi} = \frac{\text{weight}}{\text{height}{^2}}\]

For example, the respondent with ID 5553 has BMI of 23.39 ($52.62/1.5^{2}$). Larger values of BMI indicate higher levels of body fat. Average BMI in \data{yrbss.samp} is found by calculating BMI values for each respondent, then computing the average of the BMI values. Average BMI in \data{yrbss.samp} is 23.92. Since adolescent size and body type can change with age, there are no population norms for BMI in the age range of YRBSS respondents as there are for adults.  For an adult, a BMI of 23.4 would be considered within the range of normal weight, neither under- nor over-weight.

%%%  note that bmi cutpoints for children and teens are not the same as for adults. The CDC does not seem to recommend levels for health bmi for teens

Other population parameters, such as population median or population standard deviation, are also estimated using sample versions. Table~\ref{ptEstimatesYrbssActive} shows estimates of the population mean, median, and standard deviation for respondents in \data{yrbss}, using \data{yrbss.samp}, as well as the "population parameters" calculated by using the full \data{yrbss} dataset. The estimates differ slightly from the population parameters, but not by much -- in fact, the estimate for median is equal to the population median. Note that the estimates will vary based on the sample taken; these estimates are specific to the data in \data{yrbss.samp}.

\begin{table}[h]
\centering
\begin{tabular}{ l rr}
\hline
\var{active}	& estimate & parameter  \\
\hline
mean		& 3.75 & 3.90 \\
median		& 4.00 & 4.00 \\
st. dev.		& 2.556 & 2.564 \\
\hline
\end{tabular}
\caption{Point estimates and parameter values for the \var{active} variable. The parameters were obtained by computing the mean, median, and SD for all respondents (i.e. the complete set of responses in \data{yrbss}).}
\label{ptEstimatesYrbssActive}
\end{table}

\begin{exercise} \label{peOfDiffActiveBetweenGender}
How would one estimate the difference in days active for men and women? If $\overline{x}_{\text{men}} = 4.3$ and $\overline{x}_{\text{women}} = 3.2$, then what is a good point estimate for the population difference?\footnote{If $\overline{x}_{\text{men}} = 4.3$ and $\overline{x}_{\text{women}} = 3.2$, the difference of the two sample means, $4.3 - 3.2 = 1.1$, would be an estimate of the difference. In other words, it can be concluded from the sample that on average, the male respondents are physically active about 1.1 days per week more than the female respondents.}
\end{exercise}
%library(openintro); library(xtable); data(yrbss); data(yrbss.samp); (x <- by(yrbss.samp$physically_active_7d, yrbss.samp$gender, mean)); diff(x)

While point estimates rarely equal population parameters (the median in Table~\ref{ptEstimatesYrbssActive} is one of those rare examples), they do become more accurate as more data become available. The running mean from the variable \var{active} in \data{yrbss.samp}, shown in Figure~\ref{yrbssActiveRunningMean}, demonstrates this principle. A \term{running mean} is a sequence of sample means in which each mean is calculated using one more observation than the mean preceding it in the sequence; sample size increases by 1 for each mean that is calculated. For example, the second mean in the sequence is the average of the first two observations, and the third mean in the sequence is the average of the first three observations. 

\begin{figure}[h]
   \centering
   \includegraphics[width=0.72\textwidth]{ch_inference_foundations_oi_biostat/figures/yrbssActiveRunningMean/yrbssActiveRunningMean}
   \caption{The mean computed after gradually adding each individual to the sample (data from \data{yrbss.samp}). The mean tends to approach the true population average as more data become available. Running means calculated from a different random sample from \data{yrbss} would also show the same behavior.}
   \label{yrbssActiveRunningMean}
\end{figure}

%JV: Comments about running mean below

\begin{comment}

\textit{DH: We should add the idea that AZ used in her draft: showing a different sequence of running means would show the same behavior.  Since we have only one sample here, not clear whether it is worth the added complexity.}

\textit{JV: I added that to the caption of the running mean figure. It may or may not be worth it to have a different random sample of 100 to prove the point -- both earlier, when pointing out that estimates are sample-specific, and then now, to illustrate how a different set of running means behaves the same way. If we did add it, it would be easy enough to partition Fig. 4.6 to show two graphs.}

\end{comment}

As the sample size increases, the running mean approaches the population mean of 3.90 days. Thus, the CDC officials can be reasonably confident that means calculated using data from 13,582 students provide an accurate estimate of the population mean for the 21.2 million students.  Section~\ref{seOfTheMean} provides formulas for calculating how accurate sample estimates are likely to be.

\subsection{Standard error of the mean}
\label{seOfTheMean}

The point estimate $\overline{x} = 3.75$ days active per week is an estimate of the population mean $\mu$ based on the random sample \data{yrbss.samp}. Another random sample of 100 participants might produce a different value of $\overline{x}$, such as 3.22 days; repeated random sampling would result in additional different values, perhaps 3.67 days, 4.10 days, and so on. Each sample mean can be thought of as a single observation from a random variable $\overline{X}$, that has the same kind of properties as those discussed in Chapter 3. $\overline{X}$ will have a distribution of values, referred to as the \term{sampling distribution of the sample mean}; this distribution also has a mean and standard deviation.

In almost any study, such as those that produced the \data{LEAP} or \data{frog} datasets, conclusions about a population parameter must be drawn from the data collected from a single sample. The sampling distribution of $\overline{X}$ is a theoretical concept, not something that can be calculated -- obtaining repeated samples by conducting the studies may times is not possible. The concept of a sampling distribution can be illustrated, however, by taking repeated random samples from \data{yrbss}, the artificial target population. Figure~\ref{yrbssActive1000SampDist} shows a histogram of sample means from 1,000 random samples of size 100 from \data{yrbss}. The histogram provides a close approximation of the theoretical sampling distribution of $\overline{X}$ for sample sizes of 100. 

\begin{figure}[h]
   \centering
   \includegraphics[width=0.9\textwidth]
{ch_inference_foundations_oi_biostat/figures/yrbssActive1000SampDist/yrbssActive1000SampDist}
   \caption{A histogram of 1000 sample means for number of days physically active per week, where the samples are of size $n=100$.}
   \label{yrbssActive1000SampDist}
\end{figure}

\begin{termBox}{\tBoxTitle{Sampling distribution}
The sampling distribution is the distribution of the point estimates based on samples of a fixed size from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.}
\end{termBox}

The sampling distribution shown in Figure~\ref{yrbssActive1000SampDist} is unimodal and symmetric. The center of the histogram is approximately the mean of the random variable $\overline{X}$. Statistical theory can be used to show that the mean of the sampling distribution for $\overline{X}$ is exactly equal to the population mean $\mu$. 

The \term{standard error} of the sample mean measures the sample-to-sample variability of $\overline{X}$, or the extent to which values of the repeated sample means oscillate around the population mean. The standard deviation of the 1,000 values of $\overline{X}$ is 0.26, approximately the standard error of $\overline{X}$. However, this method of estimating the standard error is not possible when there is only a single sample. The sample provides only one observation from $\overline{X}$ and cannot provide any information about the variability of the sampling distribution.

When repeated random samples are not available, the standard error of the sample mean is calculated by dividing the population standard deviation ($\sigma_{x}$) by the square root of the sample size $n$. If $\overline{x}$ is the sample mean of days per week active,

\[\text{SE}_{\overline{x}} = \sigma_{\overline{x}} = \dfrac{\sigma_{x}}{\sqrt{n}} = \dfrac{2.6}{\sqrt{100}} = 0.26.\]

The probability tools of Section~\ref{randomVariablesSection} can be used to derive the formula $\sigma_{\overline{X}} = \sigma_x/\sqrt{n}$, but the derivation is not shown here. Conceptually, larger sample sizes result in sampling distributions that have decreasing variability. In the increasing sample size causes $\overline{X}$ to be clustered more tightly around the population mean $\mu$, allowing for more accurate estimates of $\mu$ from a single sample.

% \term{standard error (SE)}\index{SE}\marginpar[\raggedright\vspace{-4mm}

% $SE$\\\footnotesize standard\\error]{\raggedright\vspace{-4mm}

% $SE$\\\footnotesize standard\\error} of the estimate.

\begin{termBox}{\tBoxTitle{The standard error (SE) of the sample mean}
Given $n$ independent observations from a population with standard deviation $\sigma$, the standard error of the sample mean is equal to \vspace{-1mm}
\begin{align*}
\text{SE} = \frac{\sigma}{\sqrt{n}}.
\label{seOfXBar}
\end{align*}\vspace{-3mm}%
}
\end{termBox}

Since the population standard deviation $\sigma$ is typically unknown, the sample standard deviation $s$ can be used as a reasonably good estimate of $\sigma$, and $s / \sqrt{n}$ as an estimate for the standard error of the sample mean. For example, the estimate of SE using \data{yrbss.samp} is 0.2556 ($2.556 / \sqrt{100}$), which is very close to 0.26.  This estimate tends to be sufficiently good when the sample size is at least 30 and the population distribution is not strongly skewed. In the case of skewed distributions, a larger sample size is necessary.

%\begin{comment}

The terminology for point estimates can be confusing, and is worth restating:  

\begin{itemize}
\setlength{\itemsep}{0mm}	
	\item The population parameters $\mu$ and $\sigma$ are characteristics of the target population from which a sample is drawn. 
	
	\item In a single sample, the arithmetic average of the values in the sample (the sample mean) is denoted by $\overline{x}$. The sample standard deviation is denoted by $s$. 
	
	\item If repeated samples could be taken, the distribution of the random variable $\overline{X}$ is the collection of sample means (one for each sample). The distribution of $\overline{X}$ is called its sampling distribution, which itself has a mean $\mu_{\overline{X}}$ and standard deviation $\sigma_{\overline{X}}$. 
	
	\item With random sampling, the mean of the random variable $\overline{X}$ is always equal to the population mean $\mu$.  In the notation of Chapter 3, $\mu_{\overline{X}} = E(\overline{X}) = \mu$.
	
	\item  The standard deviation of $\overline{X}$, written $\sigma_{\overline{X}}$, is called its standard error (SE). 
	
	\item The standard error of the sample mean, as calculated from a single sample of size $n$, is equal to $\dfrac{\sigma}{\sqrt{n}}$. The standard error is abbreviated by SE and is usually estimated by using $s$, such that $SE = \dfrac{s}{\sqrt{n}}$.
	
\end{itemize}

\index{point estimate|)}

\section[Confidence intervals]{Confidence intervals} %\sectionvideohref{youtube-FUaXoKdCre4&list=PLkIselvEzpM7Pjo94m1e7J5jkIZkbQAl4}}
\label{confidenceIntervals}
\subsection{Interval estimates for a population parameter}

A \term{confidence interval} provides an estimate for a population parameter along with a margin of error, giving a plausible range of values for the parameter instead of a single value. When estimating a population mean $\mu$, a confidence interval for $\mu$ has the general form

\[(\overline{x} -m, \ \overline{x} + m) = \overline{x} \pm m, \]
where $m$ is the margin of error. The standard error, a measure of the uncertainty associated with the point estimate, is used in the calculation of the margin of error. Intervals that have this form are called \term{two-sided confidence intervals} because they provide both lower and upper bounds, $\overline{x} - m$ and $\overline{x} + m$, respectively. One-sided sided intervals are discussed in Section~\ref{onesidedCIs}.

%\subsection{An approximate 95\% confidence interval}

The standard error of the sample mean is its standard deviation; thus, by the empirical rule discussed in Section~\ref{empiricalRule} of Chapter 3, the sample mean will be within 2 standard errors of the population mean $\mu$ approximately 95\% of the time. If an interval is constructed that spans 2 standard errors from the point estimate in either direction, a data analyst can be 95\% \term{confident} that the population mean is somewhere within the interval

\begin{align}
\text{point estimate}\ \pm\ 2\times SE.
\label{95PercentConfidenceIntervalFormula}
\end{align}

The phrase "95\% confident" has a subtle interpretation: if many samples were drawn from a population, with each one used to calculate a confidence interval using Equation~\ref{95PercentConfidenceIntervalFormula}, about 95\% of those intervals would contain the population mean $\mu$. Figure~\ref{95PercentConfidenceInterval} illustrates this process with 25 samples taken from \data{yrbss}. Twenty-four of the resulting confidence intervals contain the populatiob average number of days per week that respondents are physically active, $\mu$ = 3.90 days, while one does not. 

Just as with the sampling distribution of the sample mean, the interpretation of a confidence interval relies on the abstract construct of repeated sampling. A data analyst, who can only observe one sample, does not know whether the population mean lies within the single interval calculated. However, the concept behind the calculation implies that approximately 95 intervals out of 100 will contain the population mean. The value 95\% is an approximation, accurate when the sampling distribution for the sample mean is close to that of a normal distribution. This assumption holds when the sample size is sufficiently large (guidelines for `sufficiently large' are given in Section~\ref{ch4Summary}).

\begin{figure}[hht]
   \centering
   \includegraphics[width=0.78\textwidth]
{ch_inference_foundations_oi_biostat/figures/95PercentConfidenceInterval/95PercentConfidenceInterval}
   \caption{Twenty-five samples of size $n=100$ were taken from \data{yrbss}. For~each sample, a confidence interval was created to try to capture the average number of days per week that students are physically active. Only~1 of these~25 intervals did not capture the true mean, $\mu = 3.90$~days.}
   \label{95PercentConfidenceInterval}
\end{figure}

\begin{comment}

\begin{exercise}
In Figure~\ref{95PercentConfidenceInterval}, one interval does not contain 3.90 minutes. Does this imply that the mean cannot be 3.90?\footnote{Just as some observations occur more than 2 standard deviations from the mean, some point estimates will be more than 2 standard errors from the parameter. A confidence interval only provides a plausible range of values for a parameter. While we might say other values are implausible based on the data, this does not mean they are impossible.}
\end{exercise}

\end{comment}

\begin{example}{The sample mean of days active per week from \data{yrbss.samp} is 3.75~days. The standard error, as estimated using the sample standard deviation, is $\text{SE}=\frac{2.564}{\sqrt{100}} = 0.2564$~days. Calculate an approximate 95\% confidence interval for the average days active per week for all YRBSS students.}
Using Equation~\ref{95PercentConfidenceIntervalFormula}:
\[3.75\ \pm\ 2 \times  0.26 \quad \rightarrow \quad (3.23, 4.27)\]
Based on these data, we can be about 95\% confident that the average days active per week for all YRBSS students was larger than 3.23 but less than 4.27~days. The interval extends out 2 standard errors from the point estimate, $\overline{x}_{\text{active}}$.
\end{example}
% library(openintro); library(xtable); d <- yrbss.samp; mean(d$physically_active_7d); sd(d$physically_active_7d); sd(yrbss$physically_active_7d, na.rm=TRUE)

\begin{exercise} \label{95CIExerciseForAgeOfYrbssSamp1}
The sample data suggest that the average YRBSS student height is $\overline{x}_{\text{height}} = 1.697$ meters with a standard error of 0.0088 meters (estimated using the sample standard deviation, 0.088 meters). What is an approximate 95\% confidence interval for the average height of all of the YRBSS students?\footnote{Apply Equation~\ref{95PercentConfidenceIntervalFormula}: $1.697 \ \pm \ 2\times 0.0088 \rightarrow (1.6794, 1.7146)$.  We are about 95\% confident the average height of all YRBSS students was between 1.6794 and 1.7146 meters (5.51~to 5.62~feet).}
\end{exercise}
% library(openintro); d <- yrbss.samp; mean(d$height); sd(d$height)

\subsection{The sampling distribution for the mean}

 Figure~\ref{yrbssActive1000SampDist} showed an approximation of the distribution of sample means for the variable \data{active} calculated from 1,000 random samples of size $n= 100$. Since the complete sampling distribution consists of means for all possible samples of size 100, drawing a much larger number of samples would provide a more accurate view of the distribution; the left panel of Figure~\ref{yrbssActiveBigSampDist} shows the distribution calculated from 100,000 sample means. 

\begin{figure}[hht]
   \centering
   \includegraphics[width=\textwidth]
{ch_inference_foundations_oi_biostat/figures/yrbssActiveBigSampDist/yrbssActiveBigSampDist}
   \caption{The left panel shows a histogram of the sample means for 100,000 random samples. The right panel shows a normal probability plot of those sample means.}
   \label{yrbssActiveBigSampDist}
\end{figure}

A normal probability plot of these sample means is shown in the right panel of Figure~\ref{yrbssActiveBigSampDist}. All of the points closely fall around a straight line, implying that the distribution of sample means is nearly normal (see Section~\ref{normalDist}). This result is due to by the Central Limit Theorem, summarized here and covered in more detail in Section~\ref{cltSection}.

\begin{termBox}{\tBoxTitle{Central Limit Theorem, informal description}
If a sample consists of at least 30 independent observations and the data are not strongly skewed, then the distribution of the sample mean is well approximated by a~normal model.\index{Central Limit Theorem}}
\end{termBox}



\begin{comment}
\textit{JV: Conceptually, it may be smoother to just start out specifying $1.96 \times SE$ captures 95\%. Since the probability tables are already covered in Unit 3, students would have been exposed to the idea that 2 is an estimate for 1.96. Right now, the transition to 1.96 here reads somewhat awkwardly.}

\textit{I don't think the transition is awkward; prefer to leave it be for now}


\end{comment}

The choice of 2 standard errors in Equation~\ref{95PercentConfidenceIntervalFormula} was based on the general guideline that roughly 95\% of the time, observations are within two standard deviations of the mean. Under the normal model, this can be made more accurate by using 1.96 in place~of~2.
\begin{align}
\text{point estimate}\ \pm\ 1.96\times \text{SE}
\label{95PercentCIWhenUsingNormalModel}
\end{align}
% If a point estimate, such as $\overline{x}$, is associated with a normal model and standard error $SE$, then we use this more precise 95\% confidence interval.

The Central Limit Theorem is discussed in more detail in Section~\ref{cltSection}.

\subsection{Changing the confidence level}
\label{changingTheConfidenceLevelSection}

\index{confidence interval!confidence level|(}

%JV: THIS SUBSECTION IS VERY MESSY!

%might actually be improved by discussing the CLT more thoroughly in earlier section??

%segue from 95% CI's, give conditions for using general formula, explain general formula with 99% as an example...

Ninety-five percent confidence intervals are the most commonly used interval estimates, but intervals with confidence levels other than 95\% can also be constructed.

The general formula for a confidence interval (for the population mean $\mu$) is given by 
\begin{align}
	\overline{x} \pm \ z^{\star} \times SE,
\end{align}
where $z^{\star}$ is chosen according to the confidence level. When calculating a 95\% confidence level, $z^{\star}$ is 1.96, the quantile point on a normal distribution with 0.025 area to the right of that point; 2.5\% of the normal curve is to the right of 1.96, and 2.5\% is to the left of -1.96.  The area within 1.96 standard deviations of the mean captures 95\% of the distribution.

To construct a 99\% confidence interval, $z^{\star}$ must be chosen such that 99\% of the normal curve is captured between -$z^{\star}$ and $z^{\star}$.

\begin{example}{Let $Y$ be a normally distributed random variable. Ninety-nine percent of the time, $Y$ will be within how many standard deviations of the mean?}
	This is equivalent to asking for the $z$-score with 0.005 area to the right of $z$ and 0.005 to the left of $-z$. In the normal probability table, this is the $z$-value that with .005 area to its right and .995 area to its left. The closest two values are 2.57 and 2.58; for convenience, round up to 2.58. The unobserved random variable $Y$ will be within 2.58 standard deviations of $\mu$ 99\% of the time, as shown in Figure~\ref{choosingZForCI}.
\end{example}


\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]
	{ch_inference_foundations_oi_biostat/figures/choosingZForCI/choosingZForCI}
	\caption{The area between -$z^{\star}$ and $z^{\star}$ increases as $|z^{\star}|$ becomes larger. If the confidence level is 99\%, $z^{\star}$ is chosen such that 99\% of the normal curve is between -$z^{\star}$ and $z^{\star}$, which corresponds to 0.5\% in the lower tail and 0.5\% in the upper tail: $z^{\star}=2.58$.}
	\label{choosingZForCI}
	\index{confidence interval!confidence level|)}
\end{figure}
 
A 99\% confidence interval will have the form 
\begin{align}
	\overline{x} \pm \ 2.58 \times \text{SE},
\end{align}
 and will consequently be wider than a 95\% interval for $\mu$ calculated from the same data.  A data analyst will be 99\% confident that this wider interval contains $\mu$ .
 
 
The normal approximation is important to the accuracy of these confidence intervals. When the data are reasonably symmetric, the sample size is moderately large ($n \geq 30$), and the observations are independent, it is reasonable to use the normal approximation. Observations are considered independent if they are from a simple random sample and consists of fewer than 10\% of the population. 

%Section~\ref{cltSection} provides a more detailed discussion about when the normal model can safely be applied. 

% WARNING !!!!
% EOCE 4.9 (as of 2nd Edition) references the results of this exercise
\begin{exercise} \label{find99CIForYrbssAgeExercise}
	Create a 99\% confidence interval for the average days active per week of all YRBSS students using \data{yrbss.samp}. The point estimate is $\overline{x}_{active} = 3.75$ and the standard error is $SE_{\overline{x}} = 0.26$.\footnote{The observations are independent (simple random sample, $<10\%$ of the population), the sample size is at least 30 ($n = 100$), and the distribution doesn't have a clear skew (Figure~\ref{yrbssSampHistograms} on page~\pageref{yrbssSampHistograms}); the normal approximation and estimate of SE should be reasonable. Apply the 99\% confidence interval formula: $\overline{x}_{active}\ \pm\ 2.58 \times  SE_{\overline{x}} \rightarrow (3.08, 4.42)$. We are 99\% confident that the average days active per week of all YRBSS students is between 3.08 and 4.42~days.}
\end{exercise}
%library(openintro); data(yrbss.samp); d <- yrbss.samp; mean(d$age); sd(d$age)/sqrt(100)

\begin{comment}

Ninety-five percent confidence intervals are the most commonly used interval estimates, but confidence intervals with confidence levels different from 95\% are straightforward to construct.  A general 95\% confidence interval for a point estimate that comes from a nearly normal distribution is 
\begin{align}
\text{point estimate}\ \pm\ 1.96\times SE
\end{align}
There are three components to this interval: the point estimate; ``1.96''; and the standard error. The choice of $1.96\times SE$ was based on capturing 95\% of the distribution of the sample mean,  since the estimate is within 1.96 standard deviations of the parameter about 95\% of the time. 

\begin{exercise} \label{leadInForMakingA99PercentCIExercise}
If $Y$ is a normally distributed random variable, how often will $Y$ be within 2.58 standard deviations of the mean?\footnote{This is equivalent to asking how often the Z-score will be larger than -2.58 but less than 2.58. (For a picture, see Figure~\ref{choosingZForCI}.) To determine this probability, look up -2.58 and 2.58 in the normal probability table (0.0049 and 0.9951). Thus, there is a $0.9951-0.0049 \approx 0.99$ probability that the unobserved random variable $Y$ will be within 2.58 standard deviations of $\mu$.}
\end{exercise}

To create a 99\% confidence interval, change 1.96 in the 95\% confidence interval formula to  $2.58$. Guided Practice~\ref{leadInForMakingA99PercentCIExercise} highlights that 99\% of the time a normal random variable will be within 2.58 standard deviations of the mean. This approach -- using the $Z$-scores in the normal model to compute confidence levels -- is appropriate when $\overline{x}$ is associated with a normal distribution with mean $\mu$ and standard deviation $SE_{\overline{x}}$. Thus, the formula for a 99\% confidence interval is
\begin{align}
\overline{x}\ \pm\ 2.58\times SE_{\overline{x}}
\label{99PercCIForMean}
\end{align}

A data analyst can be 99\% confident that this wider interval contains the population mean.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]
{ch_inference_foundations_oi_biostat/figures/choosingZForCI/choosingZForCI}
\caption{The area between -$z^{\star}$ and $z^{\star}$ increases as $|z^{\star}|$ becomes larger. If the confidence level is 99\%, $z^{\star}$ is chosen such that 99\% of the normal curve is between -$z^{\star}$ and $z^{\star}$, which corresponds to 0.5\% in the lower tail and 0.5\% in the upper tail: $z^{\star}=2.58$.}
\label{choosingZForCI}
\index{confidence interval!confidence level|)}
\end{figure}

The normal approximation is crucial to the precision of these confidence intervals. Section~\ref{cltSection} provides a more detailed discussion about when the normal model can safely be applied. When the normal model is not a good fit,  alternative distributions that better characterize the sampling distribution will be used.

\textit{when do these alternative distributions appear?}

\begin{termBox}{\tBoxTitle{Conditions for $\overline{X}$ being nearly normal and $SE$ being accurate\label{terBoxOfCondForXBarBeingNearlyNormalAndSEBeingAccurate}}
Important conditions to help ensure the sampling distribution of $\overline{X}$ is nearly normal and the estimate of SE sufficiently accurate:
\begin{itemize}
\setlength{\itemsep}{0mm}
\item The sample observations are independent.
\item The sample size is large: $n\geq30$ is a good rule of thumb.
\item The population distribution is not strongly skewed. 
\end{itemize}
For sample sizes larger than about 45, the condition on skewness can be ignored, unless there are prominent outliers in the sample. }
\end{termBox}

\begin{tipBox}{\tipBoxTitle[]{How to verify sample observations are independent}
If the observations are from a simple random sample and consist of fewer than 10\% of the population, then they are independent.\\[2mm]
Subjects in an experiment are considered independent if they undergo random assignment to the treatment groups. \\[2mm]
}
\end{tipBox}

\begin{tipBox}{\tipBoxTitle[]{Checking for strong skew usually means checking for obvious outliers}
When there are prominent outliers present, the sample should contain at least 100 observations, and in some cases, much more. \\[2mm]
}
\end{tipBox}

% WARNING !!!!
% EOCE 4.9 (as of 2nd Edition) references the results of this exercise
\begin{exercise} \label{find99CIForYrbssAgeExercise}
Create a 99\% confidence interval for the average days active per week of all YRBSS students using \data{yrbss.samp}. The point estimate is $\overline{x}_{active} = 3.75$ and the standard error is $SE_{\overline{x}} = 0.26$.\footnote{The observations are independent (simple random sample, $<10\%$ of the population), the sample size is at least 30 ($n = 100$), and the distribution doesn't have a clear skew (Figure~\ref{yrbssSampHistograms} on page~\pageref{yrbssSampHistograms}); the normal approximation and estimate of SE should be reasonable. Apply the 99\% confidence interval formula: $\overline{x}_{active}\ \pm\ 2.58 \times  SE_{\overline{x}} \rightarrow (3.08, 4.42)$. We are 99\% confident that the average days active per week of all YRBSS students is between 3.08 and 4.42~days.}
\end{exercise}
%library(openintro); data(yrbss.samp); d <- yrbss.samp; mean(d$age); sd(d$age)/sqrt(100)

\begin{termBox}{\tBoxTitle{Confidence interval for any confidence level}
If the point estimate follows the normal model with standard error $SE$, then a confidence interval for the population parameter is
\begin{align*}
\text{point estimate}\ \pm\ z^{\star} SE
\end{align*}
where $z^{\star}$ corresponds to the confidence level selected.}
\end{termBox}

Figure~\ref{choosingZForCI} provides a picture of how to identify $z^{\star}$ based on a confidence level. The point $z^{\star}$ is selected  so that the area between -$z^{\star}$ and $z^{\star}$ in the normal model corresponds to the confidence level. 

\begin{termBox}{\tBoxTitle{Margin of error}
\label{marginOfErrorTermBox}In a confidence interval, $z^{\star}\times SE$ is called the \term{margin of error}.}
\end{termBox}

\textC{\newpage}

\begin{exercise} \label{find90CIForYrbssAgeExercise}
Use the data in Guided Practice~\ref{find99CIForYrbssAgeExercise} to create a 90\% confidence interval for the average days active per week of all YRBSS students.\footnote{First find $z^{\star}$ such that 90\% of the distribution falls between -$z^{\star}$ and $z^{\star}$ in the standard normal model, $N(\mu=0, \sigma=1)$. The value -$z^{\star}$ is found in the normal probability table by looking for a lower tail of 5\% (the other 5\% is in the upper tail): $z^{\star}=1.65$. The 90\% confidence interval is $\overline{x}_{active}\ \pm\ 1.65\times SE_{\overline{x}} \to (3.32, 4.18)$. (The conditions for  conditions for normality and the standard error were verified in the earlier example.)  
We are 90\% confident the average days active per week is between 3.32 and 4.18~days.}

\end{exercise}

\end{comment}


\subsection{Interpreting confidence intervals}
\label{interpretingCIs}

\index{confidence interval!interpretation|(}

The National Health and Nutrition Examination Survey (NHANES) consists of a set of surveys and measurements conducted by the US CDC to assess the health and nutritional status of adults and children in the United States. NHANES is unique in that it combines interviews and physical examinations.  The dataset \data{nhanes.samp} contains 76 variables and is a random sample of 200 individuals from the measurements collected in the years 2009-2010 and 2012-2013. The sample was drawn from a larger sample of 20,293 participants in the \textbf{NHANES} package, available from The Comprehensive R Archive Network (CRAN).\footnote{\url{http://cran.us.r-project.org}} \footnote{The CDC uses a complex sampling design that samples some demographic subgroups with larger probabilities, but \data{nhanes.samp} has been adjusted so that it can be viewed as a random sample of the US population.}  

By adulthood, an individual's height and weight generally stabilize. For people ages 21 and over, body mass index (BMI) is often used as a measure of body fat that can be compared to population norms. Age and BMI information are available in the dataset and can be used to estimate mean BMI in the US population for the years represented by \data{nhanes.samp}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]
	{ch_inference_foundations_oi_biostat/figures/nhanesAdultBmiHist/nhanesAdultBmiHist}
	\caption{The distribution of the variable \var{BMI} for adult participants in NHANES.}
	\label{nhanesAdultBmiHist}
\end{figure}

%DH: should we list sample variables as in ch 1, or just plunge in?  I think we should just plunge in.  We should make the data available, though. JV: Agreed.

\begin{example}{Use \data{nhanes.samp} to calculate a 95\% confidence interval for adult BMI in the US population. \label{exNhanesBmi}}
	In the random sample of 200 participants, BMI is available for 151 of the 152 participants that are 21 years of age or older. As shown in the histogram (Figure~\ref{nhanesAdultBmiHist}), the data are right-skewed, with one larger outlier. The outlier has value 81.3 and corresponds to an individual with height 168.5 cm (66.3 in) and 230.7 kg (508.6 lbs). In the initial analysis of a dataset like this one, such an extreme value of BMI would be checked for accuracy; while it might correspond to an unexpectedly obese individual, it could also represent an error when the data was recorded. Since the accuracy cannot be verified with the data available on CRAN, the following analysis excludes the data point, using a sample size of 150.
	
	The mean and standard deviation in this sample of 150 are 29.7 and 7.7 $\text{kg}/\text{meter}{^2}$, respectively.  The sample size is large enough to justify using the normal approximation when computing the confidence interval.  The standard error of the mean is $\text{SE} = 7.7/\sqrt{150} = 0.63$, so the 95\% confidence interval is given by 
\begin{align*}
	\overline{x}_{\text{BMI}} \pm (1.96)(\text{SE}) &= 29.7 \pm (1.96)(0.63) \\
	&= (28.5, 30.9).
\end{align*}	
	
	Based on this sample, a data analyst can be 95\% confident that the average BMI of US adults is between 28.5 and 30.9 $\text{kg}/\text{meter}{^2}$.
\end{example}

The correct interpretation of a confidence interval is, "We are XX\% confident that the population parameter is between \dots" While it is tempting to say that a confidence interval captures the population parameter with a certain probability, this is a common error. The confidence level only quantifies how plausible it is that the parameter is within the interval; there is no probability associated with whether a parameter is contained in a specific confidence interval. The confidence coefficient reflects the nature of a procedure that is correct XX\% of the time, given that the assumptions in making the calculations are true.

The conditions about the validity of the normal approximation can be checked using the numerical and graphical summaries discussed in Chapter 1. However, the condition that data should be from a random sample is sometimes overlooked. If the data are not from a random sample, then the confidence interval no longer has interpretive value, since there is no population mean to which the confidence interval applies. For example, while only simple arithmetic is needed to calculate a confidence interval for BMI from the \data{famuss} dataset in Chapter 1, the participants in the study are almost certainly not a random sample from some population. 

%JV: Should we also discuss other common interpretive errors here, perhaps via a guided practice? e.g. $\mu$ equally likely to be anywhere in a given CI

%DH:  strictly speaking, no probability statements can or should be made about $\mu$, not even about where it is `likely' to be in a confidence interval.

\subsection{One-sided confidence intervals}
\label{onesidedCIs}

One-sided confidence intervals for a population mean provide just a lower bound or an upper bound, but not both.  One-sided lower confidence intervals have the form
\[
    \overline{x} - m;
\]
one-sided upper confidence intervals are of the form 
\[
    \overline{x} + m.
\]
One-sided intervals less often used that two-sided intervals by they can be useful in some settings.

The margin of error $m$ for a one-sided interval is calculated slightly differently than a two-sided interval.  The intent of a 95\% one-sided upper confidence interval, for instance, is to provide an upper bound $m$ so that a data analyst can be 95\% confident that a population mean $\mu$ is less than $\overline{x} + m$.  Since all of the possible 5\% error lies to the right of $\overline{x} + m$, the calculation of $m$ uses the point on the normal distribution that has 0.05 area in the right tail, $z^{*} = 1.645$ instead of 1.96 in the two-sided interval.  A one-sided upper 95\%  confidence interval will have the form
\begin{align*}
	\overline{x} + z^{*} \frac{s}{\sqrt{n}} = \overline{x} + 1.645 \frac{s}{\sqrt{n}}.
\end{align*}
The upper bound in a one-sided interval will be closer to $\overline{x}$ than the upper bound in a two-sided interval with the same confidence coefficient, but there  will be no lower bound.  

A lower one-sided interval for a mean $\mu$ will be of the form
\begin{align*}
  \overline{x} - z^{*} \frac{s}{\sqrt{n}}.
\end{align*}

\begin{example}

Using the NHANES BMI data, calculate a lower 95\% confidence bound for average BMI.  A one-sided lower bound is found by calculating the lower confidence interval. For the 150 observations in the BMI data, the mean and standard deviation are $\overline{x} = 29.7$  $s = 7.7$ kg/meter$^2$.  The lower 95\% bound is $29.9 - 7.7/\sqrt{150} = 29.27$. Based on these data, we can be 95\% confident that the population mean BMI among US adults is at least 29.27 kg/meter$^{2}$.\end{example}

\textit{a bit more here about when to use what, or perhaps defer that to later}


\index{confidence interval!interpretation|)}
\index{confidence interval|)}

\section[Hypothesis testing]{Hypothesis testing} %\sectionvideohref{youtube-NVbPE1_Cbx8&list=PLkIselvEzpM7Pjo94m1e7J5jkIZkbQAl4}}
\label{hypothesisTesting}

\index{hypothesis testing|(}

Do Americans tend to be overweight? This question is easy to pose, but more difficult to answer definitively. The "obesity epidemic" in the United States is often discussed in the press, and many people have anecdotal evidence suggesting body weights are generally increasing. A more scientific answer to this question should be based on a representative sample of US adults and account for the fact that weight generally increases with height. Body mass index (BMI), discussed in Section~\ref{interpretingCIs}, is one measure of body fat that adjusts for height. The World Health Organization (WHO) and other agencies use BMI to set normative guidelines for body weight. The current guidelines are shown in Table~\ref{whoBmiGuidelines}. 

%\footnote{\url{http://apps.who.int/bmi/index.jsp?introPage=intro_3.html}}. 

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|}
\hline 
Category & BMI range\tabularnewline
\hline 
\hline 
Underweight & $<18.50$\tabularnewline
\hline 
Normal (healthy weight) & 18.5-24.99\tabularnewline
\hline 
Overweight & $\geq 25$\tabularnewline
\hline 
Obese & $\geq30$\tabularnewline
\hline
\end{tabular}
\caption{WHO body weight categories based on BMI.} 
\label{whoBmiGuidelines}
\end{center}
\end{table}

The question about body weight in the United States can be formulated as a statistical hypothesis in several ways, one of which is  "Is the average BMI of US adults larger than 21.7 (the middle of the range for normal BMI's)?" The average BMI from the 150 adults in \data{nhanes.samp} was 29.7, with a 95\% confidence interval of (28.5, 30.9). This interval certainly suggests that the average BMI in the US population is higher than 21.7, and even higher than 24.99, the upper limit of the normal weight category.  It is possible for the population average, however, to be outside the interval estimate, even though that would be inconsistent with the data in the sample. By random chance, a sample may have been selected in which individuals of high BMI are overrepresented, causing the sample mean to be higher than the population mean. One measure of the strength of evidence against a working hypothesis is the likelihood of observing a sample mean as or more extreme as the one observed if the working hypothesis were true. What is the likelihood of observing a sample mean as large as 29.7 if the population average BMI in the US is actually 21.7?

Hypothesis testing is a method for calculating how unlikely an observation is under a working hypothesis, called the null hypothesis.  When testing a statistical hypothesis, one assumes that the data come from a distribution specified by the null hypothesis, calculates the likelihood that a statistic (called a test statistic) will have a value as extreme as the value observed if the data were drawn from the null hypothesis distribution, and rejects the null hypothesis only when that likelihood is small.  Hypothesis testing can be approached either informally or formally; while both methods reach essentially  the same conclusions, each has distinct advantages. The formal method provides a direct estimate of the strength of evidence against the working hypothesis, while the less formal approach can sometimes lead to a better understanding of the logic behind hypothesis testing.

The informal approach is based on what is known about a sample mean and its variability. Hypothesis testing starts with a null hypothesis, in this case, that the US population BMI matches the midpoint of the normal range, 21.7. Under this assumption, the mean of the hypothetical sampling distribution for the sample average of BMI will also be 21.7. The standard error of the sample mean is an approximate measure of how far the sample mean $\overline{x}_{\text{BMI}}$ is from the center of a distribution with population mean 21.7. In this example, $(\overline{x}_{\text{BMI}} - \mu_{\text{BMI}})/s =  (29.7 - 21.7)/0.63 = 12.7$.  The observed sample mean is 12.7 standard deviations to the right of 21.7. The sampling distribution of $\overline{x}_{\text{BMI}}$ is well approximated by a normal distribution. For a normally distributed random variable, the area to the right of 12.7 standard deviation units from the mean is less than 0.001; the likelihood of a sample with such an extreme mean, if the population is actually centered at 21.7, is vanishingly small. The event is so unusual that it is reasonable to conclude that the working hypothesis is wrong -- the data suggest that population average BMI is larger than 21.7.

The formal approach also starts with null hypothesis and adds an alternative claim that will be true if the null hypothesis is wrong. In nearly all scientific investigations, the investigator believes or suspects that the null hypothesis is not true and conducts a study to establish evidence for his or her conjecture.

\begin{termBox}{\tBoxTitle{Null and alternative hypotheses}
{The \term{null hypothesis ($H_0$)} often represents either a skeptical perspective or a claim to be tested. The \term{alternative hypothesis ($H_A$)} is an alternative claim and is often represented by a range of possible parameter values.}}
\end{termBox}

In the formal approach, the null hypothesis ($H_0$) is not rejected unless the evidence contradicting it is so strong that the only reasonable conclusion is to reject $H_0$ in favor of $H_A$. The logic is similar to the principle of presumption of innocence in many legal systems. In the United States, a defendant is assumed innocent until proven guilty; a verdict of guilty is only returned if it has been established beyond a reasonable doubt that the defendant is not innocent (i.e., guilty). 

The next section presents the steps of formal hypothesis testing.

\subsection{The Formal Approach to Hypothesis Testing}
\label{formalHypothesisTesting}


\subsubsection{Step 1: Formulating null and alternative hypotheses}

The null and alternative hypotheses are labeled $H_0$ and $H_A$, respectively.  In the BMI example, the two hypotheses are:

\begin{itemize}
	\item $H_0: \mu_{\text{bmi}} = 21.7$. The population average BMI is 21.7.
	
	\item $H_A: \mu_{\text{bmi}} > 21.7$. the population average BMI is larger than 21.7.
	
\end{itemize}	
The alternative hypothesis $H_A: \mu_{\text{bmi}} > 21.7$ is called a \term{one-sided alternative}.  The null hypothesis in this setting is sometimes  written as $H_0: \mu_{\text{bmi}} \leq 21.7$, since the alternative hypothesis is a one-sided, but we will not use that notation.
	 

Because of the literature about obesity in the United States, it is reasonable to assume that if $H_0$ is not true, the population average BMI would be larger than 21.7.  An investigator studying a similar problem in another setting, perhaps in the developing world where drought has led to food shortages, might formulate the alternative as $ H_A:\mu_{\text{bmi}} < 21.7$. In other settings where subgroups of the population have  substantial differences in access to food because of income differences, such as India or Brazil, it might be more appropriate to use a \term{two-sided alternative}, $\mu_{\text{bmi}} \neq 21.7$. 

More generally, when testing a hypothesis about a population mean $\mu$, the null and alternative hypotheses are written as:

\begin{itemize}
	\item For a one-sided alternative: \[H_0: \mu = \mu_0, \ H_A: \mu < \mu_0\] or \[H_0: \mu = \mu_0, \  H_A: \mu > \mu_0;\]
	
	\item For a two-sided alternative: \[H_0: \mu = \mu_0, \ H_A: \mu \neq \mu_0.\]
\end{itemize}


The symbol $\mu$ denotes a population mean and $\mu_0$ is the numeric value specified by  the null hypothesis. In the BMI example, $\mu_0 = 21.7$.  As in other settings, it is important to be mindful of the distinction between a population mean $\mu$, its specific numerical value $\mu_0$ under $H_0$,  and an observed sample mean $\overline{x}$.  Null and alternative hypotheses are statements about the underlying population, not the observed values from a sample.

\subsubsection{Step 2: Specifying a significance level, $\alpha$}

For the \data{nhanes.samp} BMI data, the observed sample mean was more than 12 standard deviations to the right of the population mean specified in $H_0$ -- an undeniably extreme observation. In contrast, if the sample mean had only been 2 standard deviations to the right of 21.7, it would have been less obvious whether observing a sample mean 2 or more standard deviations larger than 21.7 qualified as a rare enough event to reject the null hypothesis. 

When testing a statistical hypothesis, an investigator specifies a \term{significance level}, $\alpha$, that defines "rare". Typically, $\alpha = 0.05$, though it may be larger or smaller, depending on context; this is discussed in more detail in Section~\ref{significanceLevel}. The value of $\alpha$ will be compared with the probability that the sample mean is as or more extreme under the null hypothesis.

It is important to specify in the design of a study how rare an event must be in order to represent sufficient evidence against $H_0$.  Otherwise, it is tempting to define `rare' using the likelihood of the observed data, because many outcomes could be deemed rare `post hoc' by redefining the notion of rare.  This is akin to adding lines to a tennis court after one player has hit a questionable serve -- any serve can be made `good' by adjusting the lines to the serve. 

\subsubsection{Step 3: Calculating the test statistic}

Probabilities for sample means are easily calculated using the normal approximation and a transformation to an approximate standard normal variable, so the test statistic most often chosen is 

\begin{align*}
t=\frac{\overline{x}-\mu_0}{s/\sqrt{n}},
\end{align*}
where $\overline{x}$ is the sample mean, $s$ is the sample standard deviation and $n$ is the number of observations in the sample. The test statistic quantifies the number of standard deviations between the sample mean $\overline{x}$ and the population mean $\mu$.  For the 150 responses in the sample of adult BMI data, $\overline{x} = 29.7$, $s = 7.7$, so the test statistic has value

\begin{align*}
t &= \frac{29.7 - 21.7}{7.7/\sqrt{150}} \\
  &= 12.7.
\end{align*}

\subsubsection{Step 4: Calculating the $p$-value}

The \term{$p$-value} is the probability of observing a sample mean as or more extreme than the observed value, under the assumption that the null hypothesis is true. In samples of size 40 or more, $t$ will have a standard normal distribution, unless the data are strongly skewed or extreme outliers are present. Thus, the $p$-value is given by

\begin{itemize}
	\item[$\bullet$] $P(Z \geq t)$ for $H_A: \mu > \mu_0$,
	
	\item[$\bullet$] $P(Z \leq t)$ for $H_A: \mu < \mu_0$,
	
	\item[$\bullet$] $P(Z \leq -t) + P(Z \geq t) = P(Z \geq |t| )$ for $H_A: \mu \neq \mu_0.$
\end{itemize}
In these probabilities, $Z$ denotes a normally distributed variable with mean 0 and standard deviation 1.

For this one-sided test $ p = P(Z \geq 12.7) < 0.001$.

\subsubsection{Step 5: Drawing a conclusion}

Once the $p$-value is calculated, $p$ and $\alpha$ can be directly compared. If $p > \alpha$, the observed sample mean is not extreme enough to warrant rejecting $H_0$; more formally stated, there is insufficient evidence to reject $H_0$. If $p \leq \alpha$, there is sufficient evidence to reject $H_0$ and accept $H_A$. 

For a conclusion to be informative, however, it must be presented in the context of the problem; it is not useful to simply state whether $H_0$ is rejected or not. In the \data{nhanes.samp} BMI data, the $p$-value is extremely small, with the $t$-statistic lying far to the right of the population mean. Thus, the data support the conclusion that the average BMI in the United States is larger than 21.7. 

%JV: Suggested revision -- directly follow w/ Examples of Hypothesis Testing, then Decisions Errors. Expand Decision Errors to have the example that was originally in the Examples section. Main focus of next section should be to clearly illustrate the steps of formal hypothesis. Examples should also include how to choose between one-sided and two-sided tests... After Decision Errors, Choosing Significance Levels, and CI for the last section. 

%DH: have not followed JV recommendation, but I may yet.  Argument for putting decision errors here is to get the background stuff over and done with.  Go back to this after initial revision finished.

\subsection{Two examples}

\begin{example}
	
Too much mercury in the diet of a human being can have serious health consequences, especially for pregnant women.  There is no international standard for levels of mercury in saltwater fish that would indicate that a particular species of fish should not be consumed, but the general consensus seems to be that levels above 0.5 micrograms per gram (also labeled 0.5 parts per million net weight, or ppm net weight) is a useful standard. Berger and Gochfeld \footnote{J. Burger, M. Gochfeld, Science of the Total Environment 409 (2011) 1418–1429} studied mercury levels in saltwater fish off the coast of New Jersey, and provide estimates of mercury content for 19 species.  Based on their work, are bluefin tuna potentially dangerous or, perhaps, particularly safe to eat?

Let $\mu$ be the population average content of mercury for bluefin caught tuna off the coast of New Jersey. A two-sided test of the hypothesis $\mu = 0.50$ ppm (net weight) can be used to assess the evidence for either safety or potential danger in the mercury content of these fish.  Here are the steps:

\textit{Specifying the  null and alternative hypotheses}.   $H_0: \mu = 0.50$ ppm (net weight) vs $H_A: \mu \neq 0.50$ ppm (net weight).

\textit{Specify the significance level, $\alpha$}.  A significance level of $\alpha = 0.05$ seems reasonable. 


\textit{Calculate the test statistic}.  The test statistic will be the usual $t$-statistic. The paper by Berger and Gochfeld contains the summary statistics for a sample of 23 bluefin tuna.  The sample mean mercury level was $\overline{x}_{\text{mercury}} = 0.52$ ppm, with standard deviation $s_{\text{mercury}}= 0.16$ ppm.  The  $t$-statistic has value
\begin{align*}
t &= \frac{\overline{x}_{\text{mercury}}-\mu_0}{s/\sqrt{n}} \\
  &= \frac{0.53 - 0.50} {0.16/\sqrt{23}} \\
  &= 0.859.
\end{align*}

\textit{Calculate the $p$-value}.

For this two-sided alternative $H_A: \neq < 0.60$, the $p$-value will be 

\begin{align*}
	P(Z \leq -t) + P(Z \geq t)&= 2P(Z \leq - 0.859) \\
	&= 0.390
\end{align*}



\textit{picture here will be useful}

\textit{Draw a conclusion}.  The $p$-value is larger than the specified significance level $\alpha$.  The null hypothesis is not rejected since the data do not contain evidence to support the claim that the mercury content of bluefin tuna from the coast of New Jersey differs significantly from  potentially dangerous levels.  There is not statistically significant evidence in these data either that there should be a moratorium on catching this species or to recommend them as clearly safe.  Unfortunately, the results of studies are often ambiguous.  Based on these data a regulatory such as the US Environmental Protection Agency might decide to monitor this species more closely.

\end{example}

\begin{example}
	
In 2015, the National Sleep Foundation published new guidelines for the amount of sleep Americans should be getting \footnote{Sleep Health: Journal of the National Sleep Foundation, Vol. 1, Issue 1, p40 - 43}. The report recommends 7-9 hours a sleep per night for adults.  Is there evidence that Americans sleep time is less than 7 hours per night?

The NHANES survey asked respondents how many hours per night they slept, and responses are available in the NHANES dataset used to examine BMI and we use the sample of 151 adults used to study BMI.

\textit{Specifying the  null and alternative hypotheses}.  Let $\mu$ be the population average of sleep per night of US adults.  The null hypothesis corresponds to sleeping an average of 7 hours per night, or $H_0: \mu = 7$ hours. Since the question asks for evidence that average sleep might be less than 7, $H_A: \mu < 7$.

\textit{Specify the significance level, $\alpha$}.  A significance level of $\alpha = 0.05$ seems reasonable. 


\textit{Calculate the test statistic}.  The test statistic will be the usual $t$-statistic. For this sample of 151 adults, the average reported hours of sleep was $\overline{x}_{\text{sleep}} =7.18$, with standard deviation $\text{sd}_{\text{sleep}}= 1.33$.  The $t$-statistic has value


\begin{align*}
t &= \frac{\overline{x}_{\text{sleep}}-\mu_0}{s/\sqrt{n}} \\
  &= \frac{7.18 - 7.00} {1.33/\sqrt{151}} \\
  &= 1.65.
\end{align*}

\textit{Calculate the $p$-value}.

For this one-sided alternative $H_A: \mu < 7.0$, the $p$-value will be 

\begin{align*}
	P(Z \leq t) &= P(Z < 1.65) \\
	&= 0.95.
\end{align*}

Note that this $p-value$ is much greater than 0.05, because instead of observing a value of $\overline{x}$ that was smaller than $\mu_0 = 7$, the value is larger than  7.  The difference in the $t-statistic$ is in the opposite direction of the specified alternative.

\textit{picture here will be useful}

\textit{Draw a conclusion}.  The $p$-value is larger than the specified significance level $\alpha$.  The null hypothesis is not rejected since the data do not contain evidence to support the claim that adults sleep less than 7 hours per night.

\end{example}

One common error in one-sided tests is to assume that the $p$-value will always be the area in the smaller of the two tails to the right or left of the observed value, instead of the area that corresponds to the alternative hypothesis.  Comparing $\overline{x}$ with $\mu_0$ in this case shows that in fact average sleep is larger, not smaller, than 7 hours per night.

\subsection{Decision errors}

Hypothesis tests can potentially result in incorrect decisions, such as rejecting the null hypothesis when the null is true. There are four possible ways that the conclusion of a test can be either right or wrong, as shown in Table~\ref{fourHTScenarios}. 

\begin{table}[ht]
	\centering
	\begin{tabular}{l l c c}
		& & \multicolumn{2}{c}{\textbf{Test conclusion}} \\
		\cline{3-4}
		\vspace{-3.7mm} \\
		& & Do not reject $H_0$ &  Reject $H_0$ in favor of $H_A$ \\
		\cline{2-4}
		\vspace{-3.7mm} \\
		& $H_0$ True & Correct Decision &  Type~1 Error \\
		\raisebox{1.5ex}{\textbf{Truth}} & $H_A$ True & Type~2 Error & Correct Decision\\
		\cline{2-4}
	\end{tabular}
	\caption{Four different scenarios for hypothesis tests.}
	\label{fourHTScenarios}
\end{table}

Rejecting the null hypothesis when the null is true is referred to as a \term{Type 1 error}, while a \term{Type 2 error} refers to failing to reject the null hypothesis when the alternative is true. 

The probability of making a Type 1 error is the same as the significance level $\alpha$, since $\alpha$ determines the cutoff point for rejecting the null hypothesis. For example, if $\alpha$ is set to 0.05, then there is a 5\% chance of incorrectly rejecting $H_0$. 

Section~\ref{hypothesisTesting} introduced the logic of formal hypothesis testing by comparing it to the principle of presumption of innocence; a defendant is assumed innocent until proven guilty beyond a reasonable doubt. The following exercises extend the analogy to illustrate the relationship between Type 1 and Type 2 errors.

\begin{exercise} \label{whatAreTheErrorTypesInUSCourts}
	In a trial, the defendant is either innocent ($H_0$) or guilty ($H_A$). After hearing evidence from both the prosecution and the defense, the court must reach a verdict. What does a Type~1 Error represent in this context? What does a Type~2 Error represent?\footnote{If the court makes a Type~1 error, this means the defendant is innocent, but wrongly convicted (rejecting $H_0$ when $H_0$ is true). A Type~2 error means the court failed to convict a defendant that was guilty (failing to reject $H_0$ when $H_0$ is false).}
\end{exercise}

\begin{example}{How might the rate of Type 1 errors be reduced? What effect would this have on the rate of Type 2 errors?}
	
	To lower the rate of Type 1 error, the court could raise the standards for conviction such that fewer people are wrongly convicted. In a test, this is equivalent to lowering $\alpha$ (e.g. to 0.01 instead of 0.05); in other words, requiring an observation to be more extreme to qualify as sufficient evidence against $H_0$. However, this would also inevitably mean that fewer people who are actually guilty are convicted, raising the rate of Type 2 errors.
\end{example}

\begin{exercise} \label{howToReduceType2ErrorsInUSCourts}
	How might the rate of Type 2 errors be reduced? What effect would this have on the rate of Type 1 errors?\footnote{To lower the rate of Type 2 error, the court could lower the standards for conviction, or in other words, lower the bar for what constitutes sufficient evidence of guilt (increase $\alpha$, e.g. to 0.10 instead of 0.05). This will result in more guilty people being convicted, but also increase the rate of wrongful convictions, increasing the Type 1 error.}
\end{exercise}

\index{hypothesis testing!decision errors|)}

\subsubsection{Choosing a significance level}

Exerise~\ref{xxx} shows that reducing the error probability of one type of error increases the chance of making the other type. Because of this the significance level is often adjusted based on the consequences of any conclusions reached.

\label{significanceLevel}

\index{hypothesis testing!significance level|(}
\index{significance level|(}

Generally, the type 1 error probability is chosen to be 0.05.  Science seems to have settled on the Goldilocks view that a 5\% chance of a type 1 error is acceptable  -- a small enough so that type 1 errors occur on average 5 out of 100 times, yet large enough to prevent the null hypothesis from almost never being rejected. If a Type 1 error is especially dangerous or costly,  $\alpha$ is typically chosen to be small (e.g. 0.01). Under this scenario, it is best to err on the side of caution about rejecting the null hypothesis, so very strong evidence against $H_0$ is required in order to reject $H_0$. Conversely, if a Type 2 error is relatively more dangerous, then a larger value of $\alpha$ (e.g. 0.10) is used. Hypothesis tests with larger values of $\alpha$ will reject $H_0$ more often.

For example, in the early testing of a drug with little in the way of side effects, it may be important to continue further testing even if there is not very strong initial evidence for any beneficial effect. If the scientists conducting the research know that any initial positive results will eventually require confirmation in a larger study, they might choose to use $\alpha = 0.10$ to reduce the chances of making a Type 2 error: prematurely ending research into what may turn out to be a promising drug. 

A government agency responsible for approving drugs to be marketed to the general population, however, would likely be biased towards minimizing the chances of making a Type 1 error -- approving a drug that turns out to be unsafe or ineffective. As a result, they might conduct tests at significance level 0.01 in order to reduce the chances of concluding that a drug works when it is in fact ineffective. The US FDA and the European Medical Agency (EMA), customarily require that two independent studies show the efficacy of a new drug or regimen using $\alpha = 0.05$, though other values are sometimes used.


\index{significance level|)}
\index{hypothesis testing!significance level|)}
\index{hypothesis testing|)}



\begin{comment}

\subsection{Two more examples}
\label{hypothesisTestingExamples}

\index{data!school sleep|(}

\subsubsection{One-sided test}


\textit{Not revising this for now, since I am inclined to replace these two examples.  They are somewhat artificial and there does not seem to be a ref for the dataset.}

A poll by the National Sleep Foundation found that college students, on average, sleep about 7 hours per night. Suppose researchers at a rural school are interested in showing that students at their school sleep longer than seven hours a night on average. They plan to conduct a study using a sample of students.

The null hypothesis is that the students at this school sleep an average of 7 hours per night, as the national poll results suggest. The alternative hypothesis reflects the conjecture of the researchers -- that students at this school average more than 7 hours of sleep per night. Formally, $H_0: \mu = 7$ and $H_A: \mu > 7$. Using a one-sided alternative leads to a \term{one-sided} hypothesis test. They intend to conduct a test at the usual significance level of $\alpha = 0.05$.

The researchers collected a sample random sample of $n = 110$ students on campus. Figure~\ref{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours} shows a histogram of the sleep measurements. Students averaged 7.42 hours of sleep, with standard deviation 1.75 hours. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{ch_inference_foundations_oi_biostat/figures/histOfSleepForCollegeThatWasCheckingForMoreThan7Hours/histOfSleepForCollegeThatWasCheckingForMoreThan7Hours}
	\caption{Distribution of a nightly sleep for 110 college students. These data are strongly right skewed.\index{skew!example: strong}}
	\label{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours}
\end{figure}

Although the data are skewed, with some outliers, the normal approximation can still be used to estimate the standard error because the sample size is sufficiently large. 

\begin{example}{In the study, the sample size is $n = 110$. Students in the sample slept an average of 7.42 per night, with standard deviation 1.75 hours. Calculate the $t$-statistic.}
		
Use the formula for calculating a $t$-statistic. The value $\mu_{0}$ is given by the null hypothesis: $\mu_{0} = 7$. 

\[t=\frac{\overline{x}-\mu_0}{s/\sqrt{n}} = \frac{7.42 - 7}{1.75 / \sqrt{110}} = 2.52\]

\end{example}

Standardizing $\overline{x}$ by converting it to a $t$-statistic allows for a $p$-value to be easily calculated from a standardized normal distribution. The probability $P(Z \geq 2.52)$ equals 0.006; this value is represented by the shaded tail in Figure~\ref{pValueOneSidedSleepStudy}. The $p$-value of 0.006 indicates that if the null hypothesis is true, then the probability of a type 1 error (i.e., observing a sample mean at least as large as 7.42) hours is 0.006. The $p$-value is less than the significance level $\alpha = 0.05$; thus, there is sufficient evidence to reject $H_0$ in favor of accepting $H_A$. The data suggest that the true average hours of sleep per night for students at the rural school is larger than 7 hours. 

\begin{figure}[hht]
	\centering
	\includegraphics[width=0.83\textwidth]{ch_inference_foundations_oi_biostat/figures/pValueOneSidedSleepStudy/pValueOneSidedSleepStudy}
	\caption{If the null hypothesis is true, then the population can be modeled by a normal distribution centered at $\mu = 7$. The shaded area in the right tail represents the probability of observing a sample mean as or more extreme than $\overline{x} = 7.42$.} 
	\label{pValueOneSidedSleepStudy}
\end{figure}

\begin{exercise}
The $p$-value for the data was 0.006. Suppose the investigators had used a significance level of 0.01 in the study. Would the evidence have been strong enough to reject the null hypothesis? What if the significance level were $\alpha = 0.001$?\footnote{There is sufficient evidence to reject $H_{0}$ when $p < \alpha$. Thus, the evidence would still have been strong enough for $\alpha = 0.01$, but not $\alpha = 0.001$.}
\end{exercise}

\subsubsection{Two-sided test}

In the earlier example, researchers investigated whether the students at a rural school slept longer on average than 7 hours each night. Suppose that a group of researchers at a different college want to evaluate whether the students at their school differ from the norm of 7 hours. 

For this scenario, the null hypothesis ($H_0: \mu = 7 \text{ hours}$) remains the same, but the alternative hypothesis two-sided, $H_A: \mu \neq 7\text{ hours}$. The researchers are looking for evidence against the null in either direction; in other words, whether there is evidence that students are either sleeping less than 7 hours per night or more than 7 hours. 

\begin{example}{At the second college, the researchers randomly sampled 122 students. The data have mean $\overline{x} = 6.83 \text{ hours}$ and standard deviation $s = 1.8 \text{ hours}$. Let $\alpha$ = 0.05. Do the data provide sufficient evidence against $H_0$?} 
	
	First, calculate the $t$-statistic. 
	
	\[t=\frac{\overline{x}-\mu_0}{s/\sqrt{n}} = \frac{6.83 - 7}{1.8 / \sqrt{122}} = -1.04\]
	
	When calculating the $p$-value, note that the area from both the right and left tails must be accounted for, since this is a two-sided hypothesis test. Since the normal model is symmetric, one tail has the same area as the other, and the $p$-value is the sum of the two (Figure~\ref{2ndSchSleepHTExample}). The $p$-value is given by:
	\begin{align*}
	p = P(Z \leq -t) + P(Z \geq t) &= P(Z \geq |t|) \\
		&= 2 \times P(Z \leq -t) \\
		&= 2 \times 0.149 \\
		&= 0.298
	\end{align*}
Since the $p$-value is larger than $\alpha$, there is insufficient evidence to reject $H_0$. If $H_0$ is true, and the population mean is 7 hours of sleep per night, then it would not be unusual to observe a sample mean as or more extreme than 1.04 standard deviations away from the mean -- this would occur 29.8\% of the time. The data do not provide evidence that the true average hours of sleep per night for students at this school is different from 7 hours.
\end{example}
	
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]
	{ch_inference_foundations_oi_biostat/figures/2ndSchSleepHTExample/2ndSchSleepHTExample}
	\caption{$H_A$ is two-sided, so both tails must be counted for the $p$-value.}
	\label{2ndSchSleepHTExample}
\end{figure}	
	
\index{data!school sleep|)}

\end{comment}

\subsection{Hypothesis testing and confidence intervals}

\textit{JV: The OI section "Hypothesis testing using confidence intervals" doesn't seem very useful. Recommend putting in this section what we generally cover in 102 -- comparison of the information a CI gives vs. a hypothesis test at same $\alpha$.}

In a confidence interval for a mean, a potential population mean that falls outside the confidence interval is inconsistent with the observed data, or, equivalently, the data are inconsistent with that potential value for the population mean.  Viewed this way, a confidence interval provides information similar to a hypothesis test.  A null hypothesis about a population mean is rejected if an observed value of a sample mean is inconsistent with the null hypothesis  -- the sample mean lies in one of the extreme tails.  This leads to a useful way to conduct significance tests based on a confidence interval.

\begin{termBox}{\tBoxTitle{The relationship between two-sided hypothesis tests and confidence intervals}
{When testing the null hypothesis $H_0:\mu = \mu_0$ against the two-sided alternative $H_A: \mu \neq \mu_0$, with significance level $\alpha$, $H_0$ will be rejected whenever the $100(1-\alpha/2)\%$ confidence interval for $\mu$ does not contain $\mu_0$. 
}}
\end{termBox}
For the confidence level 95\%, a hypothesis test for a mean will be rejected at $\alpha = 0.05$ when the 95\% confidence interval does not contain the null hypothesis value of the population mean.

\begin{example}
	
In the published data on mercury in bluefin tuna off the coast of New Jersey, $\overline{x}_{\text{mercury}} = 0.53$ and $s_{\text{mercury}} = 0.16$ ppm.  The 95\% confidence interval for the population mean mercury content is 
\[
0.53 \pm 1.96 \frac{0.16}{\sqrt{21}} = (0.462, 0.598).
\]

The confidence interval in this case is helpful in understanding the somewhat ambiguous conclusion drawn from the data. It is a relatively wide interval containing values that might be regarded as safe (0.47 ppm) and others that would be regarded as potential dangerous (0.58 ppm).
\end{example}


The same relationship holds for one-sided tests and confidence intervals, although it is used less often.

\begin{termBox}{\tBoxTitle{The relationship between one-sided hypothesis tests and confidence intervals}
{
\begin{itemize}
\item When testing the null hypothesis $H_0:\mu = \mu_0$ against the one-sided alternative $H_A: \mu > \mu_0$, with significance level $\alpha$, $H_0$ will be rejected whenever $\mu_0$ is larger than the upper bound of the $100(1-\alpha)\%$ confidence interval for $\mu$. This is equivalent to $\mu_0$ having a value outside the one-sided confidence interval.

\item When testing the null hypothesis $H_0:\mu = \mu_0$ against the one-sided alternative $H_A: \mu < \mu_0$, with significance level $\alpha$, $H_0$ will be rejected whenever $\mu_0$ is samller than the lower bound of the $100(1-\alpha)\%$ confidence interval for $\mu$. This is equivalent to $\mu_0$ having a value outside the one-sided confidence interval.
\end{itemize}
}}
\end{termBox}

\begin{example}

In the analysis of the BMI data, the one-sided test $H_0: \mu{\text{BMI}} = 21.7$ vs $H_A:\mu{\text{BMI}} > 21.7$ was rejected with a highly significant $p$-value, and the lower 95\% confidence bound for $\mu_{\text{BMI}}$ was 29.3.  Since 21.7 is outside the one-sided interval, the null hypothesis could also be rejected after inspecting the one-sided confidence interval.

Confidence intervals and tests do provide slightly different information. The interval provides a range of plausible values for a parameter, a test does not.  The $p$-value in a test provides a measure of the strength of the evidence against the null hypothesis, something that is not available in a confidence interval.  In practice, both a test and a confidence interval are computed.

\end{example}

\subsection{Choosing between one-sided and two-sided tests}

\begin{comment}

Outline  Main points

Choice of one vs two sided should be driven by context.  Give some examples. Begin with some clear examples.

 Important to site an example where two-sided might not seem natural but is better.  Probably drug testing.

Choice can be more difficult than it would appear.  Example of a test that would be rejected on-sided, but not too sided.

We will not have discussed power, but can say that for a fixed sample size, one-sided test will reject more often; probability of a type II error will be smaller.  To have the same probability of a type II error, the sample size can be smaller.

Do we want to get into the term `conservative'?  Odd term, since it is not really descriptive.




	
\end{comment}

\textit{To be added: more examples of contexts where a one-sided test is more appropriate and contexts when a two-sided test is recommended, detailed explanation of why two-sided tests are more "conservative", e.g. when a two-sided test rejects and a one-sided test accepts.} 

%JV: Not sure that the OI example of one-sided vs. two-sided explained in relation to error is very effective. Show an example of why two-sided tests are more conservative, e.g. when a two-sided test rejects and a one-sided accepts

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]
	{ch_inference_foundations_oi_biostat/figures/twoSidedTestConservative/twoSidedTestConservative}
	\caption{Under a one-sided test at significance level $\alpha$ = 0.05, a $t$-statistic of 1.80 is within the rejection region. However, it would not be within the rejection region under a two-sided test with $\alpha$ = 0.05.}
	\label{twoSidedTestConservative}
\end{figure}

\section[A Closer Look at the Central Limit Theorem]{A Closer Look at the Central Limit Theorem} %\sectionvideohref{youtube-lsCc_pS3O28&list=PLkIselvEzpM7Pjo94m1e7J5jkIZkbQAl4}}
\label{cltSection}

\index{Central Limit Theorem|(}

The normal probability model for the sample mean tends to be very good when the sample consists of at least 30 independent observations and the population data are not strongly skewed. The Central Limit Theorem provides the theory that supports this assumption.

\begin{termBox}{\tBoxTitle{Central Limit Theorem, informal definition}
The distribution of $\overline{x}$ is approximately normal. The approximation can be poor if the sample size is small, but it improves with larger sample sizes.}
\end{termBox}

%%% revisions to here, 7/5/16, 13:48

This section examines the accuracy of the normal model for the sample mean for three possible quantitative continuous population distributions:  a \emph{uniform} distribution, one an \emph{exponential} distribution, and a \emph{log-normal} distribution. These density functions for these distributions are shown in the top panels of Figure~\ref{cltSimulations}. The uniform distribution is symmetric, the exponential distribution has moderate skew -- its right tail is relatively short (few outliers), and the log-normal distribution is strongly skewed and will tend to produce more apparent outliers.\index{skew!example: moderate}\index{skew!example: strong}

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/cltSimulations/cltSimulations}
   \caption{Sampling distributions for the mean at different sample sizes and for three different distributions. The dashed red lines show normal distributions.}
   \label{cltSimulations}
\end{figure}

The left panel in the $n=2$ row shows the theoretical sampling distribution of $\overline{x}$ if it a sample mean of two observations from the uniform distribution. The dashed line represents the closest approximation of the normal distribution. Similarly, the center and right panels of the $n=2$ row show the respective distributions of $\overline{x}$ for data from exponential and log-normal distributions.

\begin{exercise}
Examine the distributions in each row of Figure~\ref{cltSimulations}. What do you notice about the normal approximation for each sampling distribution as the sample size becomes larger?\footnote{The normal approximation becomes better as larger samples are used.}
\end{exercise}

\begin{example}{Would the normal approximation be good in all applications for a sample size of 30?}
Not necessarily. For example, the normal approximation for the log-normal example is questionable for a sample size of 30. Generally, the more skewed a population distribution or the more common the frequency of outliers, the larger the sample required to guarantee the distribution of the sample mean is nearly normal.
\end{example}

As discussed in Section~\ref{seOfTheMean}, the sample standard deviation, $s$, is often used  used as a substitute for the population standard deviation, $\sigma$, when computing the standard error. This estimate tends to be reasonable when $n\geq30$. Alternative approaches for smaller sample sizes are discussed in in Chapters~\ref{inferenceForNumericalData} and~\ref{inferenceForCategoricalData}.

%DH:  don't like this example.  Not biological, is only hypothetical and does not explain how the independence assumption is checked.  Does not provide practical guidance to a student, and should be replaced.

\begin{example}{Figure~\ref{pokerProfitsCanApplyNormalToSampMean} shows a histogram of 50  hypothetical observations --  winnings and losses from 50 consecutive days of a professional poker player. Can the normal approximation be applied to the sample mean, 90.69?}

\begin{itemize}
\setlength{\itemsep}{0mm}
\item[(1)] These are referred to as \term{time series data}, because the data arrived in a particular sequence. If the player wins on one day, it may influence how she plays the next. To make the assumption of independence we should perform careful checks on such data. While the supporting analysis is not shown, no evidence was found to indicate the observations are not independent.
\item[(2)] The sample size is 50, satisfying the sample size condition.
\item[(3)] There are two outliers, one very extreme, which suggests the data are very strongly skewed or very distant outliers may be common for this type of data. Outliers can play an important role and affect the distribution of the sample mean and the estimate of the standard error.
\end{itemize}
Since we should be skeptical of the independence of observations and the very extreme upper outlier poses a challenge, we should not use the normal model for the sample mean of these 50 observations. If we can obtain a much larger sample, perhaps several hundred observations, then the concerns about skew and outliers would no longer apply.
\end{example}

\begin{figure}[ht]
   \centering
   \includegraphics[height=58mm]{ch_inference_foundations_oi_biostat/figures/pokerProfitsCanApplyNormalToSampMean/pokerProfitsCanApplyNormalToSampMean}
   \caption{Sample distribution of poker winnings. These data include some very clear outliers. These are problematic when considering the normality of the sample mean. For example, outliers are often an indicator of very strong skew\index{skew!example: very strong}.}
   \label{pokerProfitsCanApplyNormalToSampMean}
\end{figure}

\begin{caution}
{Examine data structure when considering independence}
{Some data sets are collected in such a way that they have a natural underlying structure between observations, e.g. when observations occur consecutively. Be especially cautious about independence assumptions regarding such data sets.}
\end{caution}

\begin{caution}{Watch out for strong skew and outliers}
{Strong skew is often identified by the presence of clear outliers. If a data set has prominent outliers, or such observations are somewhat common for the type of data under study, then it is useful to collect a sample with many more than 30 observations if the normal model will be used for $\bar{x}$.}
\index{Central Limit Theorem|)}
\end{caution}

\index{skew!strongly skewed guideline}
\index{significance level|)}
\index{hypothesis testing!significance level|)}
\index{hypothesis testing|)}

\section[Summary]{Summary}
\label{ch4Summary}

Confidence intervals and hypothesis tests are two of the central ideas and concepts in inference for a population based on a sample, and both will be used frequently in later chapters.  A confidence interval provides a plausible interval estimate for a population parameter along with a confidence coefficient, while a hypothesis test is a tool for making a qualitative yes/no decision about the parameter. Both have value -- the confidence interval shows a range of population parameter values consistent with the data and the width of the interval reflects both the confidence coefficient and the inherent uncertainty or randomness in the sample. It is often used to design additional studies.  Hypothesis testing is useful when policy decisions will be made about an intervention (a new drug, for instance) or an association (the association of genotype with a phenotype).  The pre-specified value of $\alpha$ is a reliable measure of the likelihood of rejecting a null hypothesis incorrectly.  Most progress in science is incremental and controlling the type 1 error probability $\alpha$ keeps the chance of a false positive acceptability low in the face of the inherent variability in human and biological measurements.

The calculation of tests and confidence intervals is relatively straightforward -- once the hypotheses, $\alpha$, and the confidence coefficient have been specified.  For most students, however, (and indeed may experienced investigators) the steps that do not rely on calculation are the more difficult part of the problem.  Specific null and alternative hypotheses and conclusions from a test are entirely context dependent -- driven by the scientific setting of the investigation.  The confidence coefficient reflects a study team's judgement on the balance of precision (the width of the interval) and chance of possible error (1 minus the confidence coefficient). The normal distribution used to calculate probabilities for the $t$ statistic is an approximation and in small to moderate sample sizes ($30 \leq n \leq 50$) it may not be clear that the normal model is adequate.  These choices, often based on judgement, may be the largest distinction between a statistics problem and a purely mathematical problem.

These nuanced issues cannot be adequately covered in any introduction to statistics. Urging students to use their judgement is not realistic (or fair) and prescribing choices of $\alpha$ and other aspects of inference may misleads student into thinking the choices are always clear. It seems best to offer some guidance on reasonable choices to get started:

\begin{itemize}
	
	\item Unless it is clear in the context of a problem that a specific direction of change from a null hypothesis is the one of interest, alternative hypotheses should be two-sided.
	
	\item In science, the usual choice for $\alpha$ is 0.05; that should be the default value unless otherwise specified in a problem.
	
	\item Similarly, the default value of the confidence coefficient for a confidence interval should 95\%, since it corresponds to a 5\% chance of error for the interval.
	
	\item The use of a standard normal distribution to calculate probabilities for a $t$-statistic is reasonable for sample sizes of 30 or more if the distribution of the data are not strongly skewed and there are no large outliers.  Later chapters will provide methods that can be used with small samples.
	
	\item Sample sizes of 50 or more are usually sufficient in the presence of skewing or a few large outliers.
		
\end{itemize}

The next chapters will cover methods of inference in specific scientific settings -- comparing two groups or fitting lines to data, for instance. These settings will provide opportunities to calculate tests and intervals, read problems for context and check some of the underlying assumptions.





\end{spacing}
 


