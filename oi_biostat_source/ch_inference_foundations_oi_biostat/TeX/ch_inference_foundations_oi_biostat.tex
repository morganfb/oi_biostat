\chapter{Foundations for inference} 
\label{foundationsForInference}

Imagine the United States Center for Disease Control and Prevention (CDC) influencing policy makers in curbing the national obesity problem. The members of the CDC's Division of Nutrition, Physical Activity, and Obesity (DNPAO) "focus on policy and environmental strategies to make healthy eating and active living accessible and affordable for everyone." \footnote{\url{http://www.cdc.gov/obesity/}} Before they give policy suggestions, however, the DNPAO must first diagnose this problem of obesity in the United States. One metric that these scientists would consider could be a person's Body Mass Index, also known as BMI. A person's BMI has been a helpful tool to capture both a person's height and weight within one measurement. BMI can also be used as a measure of body fat with a high BMI as an indicator for high body fat. In medicine, BMI is used to categorize a person as "underweight," "overweight" or "obese." 

These policy makers are interested in a couple of questions. What is the average population BMI for all adults in the United States? Instead of providing a single number, what is a plausible range values for the average BMI in the United States? Finally using the categorization of BMI values and ranges that the World Health Organization (WHO) provides, noting that it disregards muscularity, is the average BMI in the United States considered an average healthy BMI? Below is a categorization of BMI values and ranges
\footnote{\url{http://apps.who.int/bmi/index.jsp?introPage=intro_3.html}}

\begin{center}
\begin{tabular}{|c|c|}
\hline 
Category & BMI range\tabularnewline
\hline 
\hline 
Underweight & $<18.50$\tabularnewline
\hline 
Normal (healthy weight) & 18.5-24.99\tabularnewline
\hline 
Overweight & $\geq 25$\tabularnewline
\hline 
Obese & $\geq30$\tabularnewline
\hline 
\end{tabular}
\end{center}

These questions encompass the broader idea of statistical inference covered in Chapter \ref{foundationsForInference}. Inference is a set of tools used to estimate properties or parameters about a population after observing a sample from the population. Inference also allows for different levels of quality or confidence of these parameter estimates. Once the parameter estimate is calculated, the average BMI in the United States for example, scientists can ask how confident they are that this estimate is representative of the greater population of the US. For example, a classic inferential question is, ``How likely is it that the estimated mean, $\bar{x}$, is near the population mean, $\mu$?'' Statistical inference includes asking these questions but also determining which estimates to use.

Chapter \ref {foundationsForInference} provides the groundwork for inference on a larger population from observing one sample, and later chapters will cover inference comparing two or more distinct populations. While the equations and details change depending on the setting, the foundations and general procedures for inference are the same throughout statistics. Understanding the foundation with point estimates in this chapter will provide familiarity for upcoming chapters. 

After looking at the data in Section \ref{brfssData} that the CDC could use in making these inferences, section \ref{variabilityInEstimates} will provide an introduction to point estimates, the sampling distribution that these estimates are drawn from, and the variability of these estimates. Section \ref{confidenceIntervals} will present tools to incorporate this variability. Rather than a single value, these policy makers can provide, instead, a range of values that they believe are likely estimates. Researchers could also want to compare the values they observe to some benchmark they believe in. Hypothesis testing in Section \ref{hypothesisTesting} allows the researchers use point estimates for comparisons while still encompassing the variability that confidence intervals provide. The hypothesis testing framework gives us structure to do so and uses the same moving parts as a confidence interval. 

\section{BRFSS Data}
\label{brfssData}
The Behavioral Risk Factor Surveillance System (BRFSS), organized by the CDC, was started in 1984 and is the world's largest on-going telephone health survey system. This survey is nationwide and aims to "monitor state-level prevalence of major behavioral risks among adults associated with premature morbidity and mortality." \footnote{\url{http://www.cdc.gov/brfss/about/about_brfss.htm}} Topics like smoking, alcohol use, diet and exercise are included in this questionnaire. The annual survey data from 2000, \data{BRFSS}, includes records on 289 variables and could be used to estimate the average BMI of the US population. The variables that we are particularly interested in with calculating BMI are listed in Table  ~\ref{brfssBMIVariables}. 
\begin{comment} http://www.cdc.gov/brfss/annual_data/annual_2000.htm#information\end{comment}
\begin{table}[h]
\centering\small
\begin{tabular}{l p{65mm}}
\hline
{\bf variable} & {\bf description} \\
\hline
\var{sex} & Male or Female where 1 is Male and 2 is Female\\
\var{age} & In years \\
\var{height} & In feet and inches where, for example, 5' 5" is listed as 505 \\
\var{weight} & In pounds\\
\end{tabular}
\caption{Variables of interest and their descriptions for the \data{BRFSS} data set.}
\label{brfssBMIVariables}
\end{table}

The calculation of a BMI index from weight and height using both Metric and Imperial is \[BMI=\frac{\mathrm{weight_{kg}}}{\mathrm{height_{m}}^2}=\frac{\mathrm{weight_{lb}}}{\mathrm{height_{in}}^2}\cdot 703\]
where $\mathrm{weight_{kg}}$ and $\mathrm{height_{m}}$ is the weight and height measured in kilograms and meters respectively while $\mathrm{weight_{lb}}$ and $\mathrm{height_{in}}$ is the weight and height in pounds and inches respectively. 

The heights and weights from the \data{BRFSS} data can be used to calculate BMI values for each person in the sample. Thus, each person's BMI value will be considered a single observation. The \data{BRFSS} data comprises of 170,000 observations, and the CDC is hoping to infer the characteristics of adults in the United States, the target population. A target population is the group that the statistician is interested in and wants to draw conclusions about. 

A simple random sample of 40,000 adults from the \data{BRFSS} data is taken to be used as the observed sample. \footnote{The \data{BRFSS} data without including weights was not a simple random sample. Rather the CDC,after collecting the data, edited, processed and weighted the data to make the values act like a simple random sample from the US population. Thus noncoverage and nonresponse become equal among all groupings of the population. The edited and weighted data is the \data{BRFSS} data that the CDC provides. The weighting formulae can be found \url{http://www.cdc.gov/brfss/annual_data/2010/pdf/overview_10.pdf} under the Data Processing section.} This random sample of 40,000 adults will be referred to as \data{BRFSS BMI} from now on. Part of this dataset with the BMI calculation is shown in Table ~\ref{brfssBMIData}. 

% latex table generated in R 3.1.1 by xtable 1.7-4 package
% Sun Jul 19 13:57:10 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & sex & age & height & weight & bmi \\ 
  \hline
1 &   2 &  60 & 508 & 200 & 30.41 \\ 
  2 &   2 &  25 & 506 & 145 & 23.40 \\ 
  3 &   1 &  40 & 511 & 180 & 25.10 \\ 
  4 &   1 &  53 & 511 & 210 & 29.29 \\ 
  5 &   2 &  80 & 504 & 170 & 29.18 \\ 
  6 &   2 &  71 & 501 & 108 & 20.40 \\ 
   \hline
\end{tabular}
\caption{Six observations from the BRFSS BMI dataset} 
\label{brfssBMIData}
\end{table}

This simple random sample from \data{BRFSS}, \data{BRFSS BMI}, will be used to draw conclusions about the target population of US adults. This is the practice of statistical inference in the broadest sense. Before estimating the average BMI in the US, Figure ~\ref{exploreBMI} provides a graphical exploration using the tools from Chapter \ref{introductionToData}. The histogram and box plot both show that the BMI values are skewed right suggesting the median of these BMIs is lower than the mean. 

\begin{figure}
\centering
\includegraphics[width =  \textwidth]{ch_inference_foundations_oi_biostat/figures/brfssBMIsampHistograms/brfssBMIsampHistograms}
\caption{Histogram and boxplot of BMI for the \data{BRFSS BMI} data. The data is skewed right by both the histogram and the box plot.}
\label{exploreBMI}
\end{figure}

The data from \data{BRFSS BMI} is special because in order to do statistical inference, the dataset needs to be representative of the population of interest. In this case, because the size of \data{BRFSS BMI} is so large and drawn randomly from \data{BRFSS}, the data can be assumed to be representative and that the estimates will be close to the population parameters. \footnote{In 2000, the US population was 282.2 million, \footnote{\url{http://www.census.gov/prod/2002pubs/c2kprof00-us.pdf}} 7,000 times the sample size of 40,000.} The foundations of statistical inference follows naturally after the data has been examined and explored, 

\section{Variability in estimates}
\label{variabilityInEstimates}

\index{point estimate|(}

If members at the CDC, after observing the sample of 40,000 BMI values, were asked to give their best guess for the average BMI in the US, what would it be? Here, they would employ point estimation. A \term{point estimate} is a single value derived from sample data that serves as the "best guess" for that population parameter. Section ~\ref{variabilityInEstimates} will touch upon point estimates as well as the variability inherent in using this single number as the best guess.\footnote{While this chapter focuses on the the mean of BMIs, questions regarding variation are often just as important in practice. For instance, potential action regarding obesity could change if the standard deviation of a person's BMI was 5 versus if it was 15.} 

\subsection{Point estimates}
\label{pointEstimates}

A likely choice to estimate the \term{population mean} from the \data{BRFSS BMI} sample is to simply take the \term{sample mean}. That is, use the average BMI of all 40,000 survey respondents in the sample as the estimate for the average BMI among US adults. 

For notation, use $\mathrm{bmi}_1, \mathrm{bmi}_2, \ldots, \mathrm{bmi}_{40,000}$  to represent the BMI for each survey respondent in the \data{BRFSS BMI} sample. The sample mean for BMI using the 40,000 observations is 
\begin{eqnarray*}
\bar{\mathrm{bmi}} = \frac{30.41 + 23.40 + 25.10 + \dots}{40,000} = 26.356
\end{eqnarray*}
\index{point estimate!single mean|(}
and is the \term{point estimate} of the population mean\footnote{If \var{weight} is the variable of interest, instead, the sample mean of observations denoted $w_1,\ldots w_{40,000}$ would be $\bar{w}$}. 

Suppose from the original respondents in \data{BRFSS}, a new sample of 40,000 people is taken and the mean calculated; the answer would likely not be the same as the sample mean from the  \data{BRFSS BMI} data set. Estimates generally vary from one sample to another even with samples of the same sample size. Low \term{sampling variation} can suggest that the estimate may be close, but not exactly equal to the parameter. A larger sample size can ensure a closer estimate to the population parameter than an estimate taken from a smaller sized sample. 

What about generating point estimates of other \term{population parameters}, such as the population median or population standard deviation? Sample statistics can help estimate these parameters as well. For example, the population standard deviation for BMI can be estimated using the sample standard deviation, and the population median using the sample median. Table ~\ref{BMIEstimates} provides the point estimates from \data{BRFSS BMI} to other population parameters relating to BMI.

% latex table generated in R 3.1.1 by xtable 1.7-4 package
% Sun Jul 19 20:25:13 2015
\begin{table}[ht]
\centering
\begin{tabular}{lr}
  \hline
BMI & estimates \\ 
  \hline
mean & 26.356 \\ 
  median & 25.620 \\ 
  std. dev. & 5.288 \\ 
   \hline
\end{tabular}
\caption{Point estimates for the \var{bmi} variable} 
\label{BMIEstimates}
\end{table}

\begin{exercise} \label{pointEstimateOfDesiredWeights}
Suppose the CDC was interested in an estimate for the average age for men and women in the US as well as the difference in ages. If $\bar{\mathrm{age}}_{\mathrm{women}} = 47.35 $ years and $\bar{\mathrm{age}} _ {\mathrm{men}} = 45.67 $ years, what would be a good point estimate for the population age difference? \footnote{Take the difference of the two sample means: $\bar{\mathrm{age}}_{\mathrm{women}} - \bar{\mathrm{age}} _ {\mathrm{men}} = 47.35 - 45.67 =  1.69$. Women are on average older than men by 1.69 years.}
\end{exercise}
%men = brfss.sample[which(brfss.sample$sex == 1),]
%women = brfss.sample[which(brfss.sample$sex == 2),]
%mean(women$age) - mean(men$age)

\begin{exercise}
Provide a point estimate of the population IQR for the BMI of participants using a sample.\footnote{To obtain a point estimate of the IQR for the population, use the IQR of a random sample from the population.}

\index{point estimate!single mean|)}

\end{exercise}

The sample mean calculated from this \data{BRFSS BMI} sample of 40,000 will likely be different from the sample mean of a different set of 40,000 respondents from \data{BRFSS} data. Using $R$, take another random sample from the \data{BRFSS} data of 40,000 and note that that the new sample mean for the BMI is 26.344. Estimates will differ across samples through sampling variation but the accuracy of the point estimate will improve once more data becomes available and sample sizes increase.

Consider a running mean from the \data{BRFSS BMI} data to explore increases in sample size and increase in accuracy. A \term{running mean} is a sequence of means, where each mean uses one more observation in its calculation than the mean directly before it in the sequence. In this case, the second mean is the average of the first two observations, $\mathrm{bmi}_1, \mathrm{bmi}_2$. The third number in the running mean sequence is the average of $\mathrm{bmi}_1, \mathrm{bmi}_2,$ and $\mathrm{bmi}_3$. 

The running mean for \data{BMI} in the \data{BRFSS BMI} dataset is shown in Figure~\ref{BMIRunningMean} for 300 and 5,000 observations. As more values get included, the running mean converges closer to the sample mean of 26.36. Similarly if the sample size increases from 40,000 to 100,000, the sample mean from 100,000 observations will be closer to the average US population BMI than the sample mean of 40,000 observations. 

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/brfssBMIRunningMean/brfssBMIRunningMean}
   \caption{The running means from the \var{BRFSS BMI} sample of 40,000 observations. The mean stabilizes and approaches the mean of the entire sample $\bar{\mathrm{bmi}} = 26.36$ as the number of observations increases}
      \label{BMIRunningMean}
\end{figure}

Sampling variation, however, is across samples of the same size. Figure ~\ref{runningSamplingVariation} displays the running means of two samples from \data{BRFSS}. With a small sample size, the path of the running means are not the same. There exists some sample variation. But as the number of observations increase, the running means for these two samples converge to the same value. The variation across the different running means decreases as the number of observations increases. Figure ~\ref{runningSamplingVariation20} illustrates this more clearly by calculating the running means of 20 independent samples of 40,000 observations. At sample sizes of less than 150, the sample variation is extremely large, but as the number of observations get larger, the sampling variation decreases. We will explore this concept more in Section ~\ref{accuracyAndPrecision}. 

Figure ~\ref{runningSamplingVariation20} 
\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/brfssBMISampVar/brfssBMISampVar}
   \caption{The running mean of two samples drawn from \data{BRFSS}. The exists high sampling variation at the beginning but as the sample sizes increase, the running mean values converge.}
	\label{runningSamplingVariation}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/brfssBMISampVar/brfssBMISampVar20}
   \caption{Many running means from different samples of 40,000 observations from \data{BRFSS}. The sampling variation decreases as the number of observations gets larger.}
	\label{runningSamplingVariation20}
\end{figure}


\subsection{Accuracy and Precision of Point Estimates}
\label{accuracyAndPrecision}

Accuracy and precision have colloquially become interchangeable. In science, however, they both have very distinct meanings. Accuracy is a characteristic of how close the measurements are to their true values. Precision is the characteristic of how close the measurements are to themselves. 

Within inference and point estimates, the sample mean is always accurate. The sample mean does not contain any systematic error. The sample mean is not equal to the population mean only because of random sampling error. While the sample mean may not always be equal to the population mean, in expectation, the sample mean and population parameter are equivalent. We observe the accuracy of the sample mean in practice with 20 running means from Figure ~\ref{runningSamplingVariation20}. The center of these running means is, in expectation, the population average BMI. 

While the sample mean is consistently accurate, it may not always be precise. As the sample size, $n$, increases, the randomness and variability in the sample mean decreases. Sampling variation at different sample sizes serves as evidence. Figure ~\ref{sampleMeanPrecision} shows two histograms of sample means. The left histogram has sample means with a sample size of 5 and the right has a sample size of 50. All of these samples are randomly drawn from the \data{BRFSS} data, the sample mean calculated and plotted as an observation on the histogram. The histogram with $n=50$ has noticeably smaller variance than the histogram with $n=5$. The histogram with $n=5$ is also slightly skewed. As such, with larger sample sizes, the sample variation decreases, making the sample mean more precise across samples. Section ~\ref{seOfTheMean} provides guidance on how to quantify and measure sampling variation as more data becomes available. 

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/brfssBMISampleMeanPrecision/brfssBMISampleMeanPrecision}
   \caption{Sample means of size $n=5$ and $n=50$. The histogram with $n=50$ has a smaller variance.}
   \label{sampleMeanPrecision}
\end{figure}
 
\subsection{Sampling Distribution and the Standard Error of the Mean}
\label{seOfTheMean}

From the random sample represented in \data{BRFSS BMI}, average BMI of an adult in the United States is estimated to be the sample mean, 26.356. Suppose another random sample of 40,000 individuals is taken and its mean calculated. In Section ~\ref{pointEstimates}, the value was 26.344. Suppose this happens again (26.350) and again (26.349), and continue to do this many many times -- which can only happen with access to the larger \data{BRFSS} dataset. \footnote{The sampling distribution depends on the underlying distribution of the target population. In this case, while \data{BRFSS} is not quite the target population of all US adults, it is large enough to illustrate the concept of sampling distribution and acts as a representative substitute to the US population. If the CDC had complete data from the target population, there would be no need to take a sample mean measurement. In practice, people generally are not even capable of taking another sample of 40,000 from \data{BRFSS}!} From these values, a \term{sampling distribution} for the sample mean can be constructed when the sample size is 40,000, shown in Figure~\ref{brfssBMISamplingDistribution}.

\begin{termBox}{\tBoxTitle{Sampling distribution}
The \term{sampling distribution} of a point estimate represents the distribution of the point estimate based on samples of a fixed size from a certain population. There is a unique sampling distribution that exists that is inherent to the point estimator that is being measured. Every time that the point estimate is calculated from a particular sample of said size, the point estimate is one sample from the sampling distribution. Understanding the concept of a sampling distribution is central to understanding statistical inference.}
\end{termBox}

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/brfssBMISamplingDistribution/brfssBMISamplingDistribution}
   \caption{A histogram of 100,000 sample means for BMI, where the samples are of size $n=40,000$. }
      \label{brfssBMISamplingDistribution}
\end{figure}

Figure~\ref{brfssBMISamplingDistribution} is an approximation of the sampling distribution. To truly get the sampling distribution, one would need to sample every possible unique combination of 40,000 respondents from the entire US adult population (and not just the \data{BRFSS} data set). However just as the running mean becomes a better approximation of the population average as more data becomes available, the approximation of the sampling distribution also resembles more closely the sampling distribution as more and more samples are drawn. An approximation of the sampling distribution of the sampling mean can be created with the following pseudocode \footnote{Refer to the appendix for R code}: 

\begin{verbatim}
(1) Have a place to store all the sample means that we will calculate
(2) Take a sample from the BRFSS dataset of 40,000
(3) Calculate the sample mean from this specific sample and store it in (1)
(4) Repeat (2) and (3) many many times 
(5) Plot all the sample means you have stored in (1) as a histogram
\end{verbatim}
  
The sampling distribution, in this case, is unimodal and approximately symmetric. The sampling distribution is also centered exactly at the target population's mean. Intuitively, this makes sense. The sample means should tend to ``fall around'' the mean of the population that we are drawing from.

From the sampling distribution, we also see that the point estimator will have some variability, and from the concept of sampling distribution introduced in Section ~\ref{accuracyAndPrecision}, the precision increases as the sample size gets larger. Point estimates, however, still vary sample by sample. There needs to be some metric to quantify the variability of the sample mean around the population mean.

\begin{tipBox}{\tipBoxTitle{More data means less variability}
In sampling, the larger the sample size the better. The precision of the sample mean increases as more data is observed in the sample.}
\end{tipBox}

Standard deviation is the most obvious method to quantify variability. The standard deviation of the sampling distribution denoted  $\sigma_{\bar{x}}$ or $s$ in some contexts is used to measure sampling variation of the sample mean. \footnote{Caution: The standard deviation of the sample mean is not equivalent to the estimate for the standard deviation of the population. Those are measuring two separate quantities.}

Just as with the definition of standard deviation in Chapter ~\ref{introductionToData}, the standard deviation of the sample mean indicates how far the typical estimate is away from the population mean. It also is a very good metric for the typical \term{error} of the point estimate, and for this reason, this version of standard deviation is called the \term{standard error (SE)} \index{SE}\marginpar[\raggedright\vspace{-4mm} $SE$\\\footnotesize standard\\error]{\raggedright\vspace{-4mm} $SE$\\\footnotesize standard\\error} of the estimate. 

\begin{termBox}{\tBoxTitle{Standard error of an estimate}
The standard deviation associated with an estimate is called the \emph{standard error}. It describes the typical error or uncertainty associated with the estimate.}
\end{termBox}

\begin{tipBox}{\tipBoxTitle{"Standard Deviation" $\neq$ "Standard Error"}
Remember, the term "standard error" is not interchangeable with "standard deviation". Standard deviation describes the spread of values within the sample. Using means, the standard error is the standard deviation of the sample mean and describes how accurate the sample mean is to the population mean.}
\end{tipBox}

\begin{exercise}
(a) Would you rather use a small sample or a large sample when estimating a parameter? Why? (b) Using reasoning from (a), would you expect a point estimate based on a small sample to have smaller or larger standard error than a point estimate based on a larger sample?\footnote{(a) Prefer a large sample. Consider two random samples: one of size 10 and one of size 1000. Individual observations in the small sample are highly influential on the estimate while in larger samples these individual observations would more often average each other out. The larger sample would tend to provide a more accurate estimate. (b) If we think an estimate is better, we probably mean it typically has less error. Based on (a), intuition suggests that a larger sample size corresponds to a smaller standard error. In general the standard error for an estimate gets smaller as the samples get larger.}
\end{exercise}

The standard error could be calculated if statisticians knew the exact sampling distribution. It would simply be the standard deviation of that sampling distribution.  However when considering the case of the point estimate $\bar{x}$, there is one problem: most often, scientists only observe one sample, and there is no obvious way to estimate the standard error from a single sample. Computation methods and statistical theory, instead, provide helpful tools to address this issue. 

Instead of only observing one sample from \data{BRFSS}, imagine the possibility of repeatedly sampling from \data{BRFSS} \footnote{just like creating the approximation to the sampling distribution}. While scientists do not have the exact sampling distribution,they do have the ability to calculate an approximate standard error. After many iterations, the standard deviation of the many sample means calculated becomes a fairly reasonable estimate of the standard error of the sample mean. In pseudocode: 
\begin{verbatim}
(1) Have a place to store all the sample means that we will calculate
(2) Take a sample from the BRFSS dataset of 40,000
(3) Calculate the sample mean from this specific sample and store it in (1)
(4) Repeat (2) and (3) many many times 
(5) Calculate the standard deviation of the values in (1). This is an 
	estimation of the standard error of the sample mean.
\end{verbatim}

The algorithm above can be implemented in $R$. The $R$ code first creates an approximate sampling distribution and then uses the standard deviation from the approximation to be the estimate for the standard error. The $R$ code is as follows:\footnote{This computing experience samples without replacement to simulate experimenting in the real world. Theory states however that individual BMI values need to be independent. A reliable method to ensure independence is to do a simple random sample from the population that is less than 10\% of the population. This 10\% rule, however, is used as a rule of thumb. By sampling without replacement within a finite population,\var{sd(sample.means)} is not exactly the theoretical standard error but quite close.}
\begin{verbatim}
> sample.means<-matrix(data=NA,nrow=1000, ncol=1) #to store the sample means
> for(i in 1:1000){ #do 1,000 iterations
+   sample<-sample(x=brfss.df$bmi, size=40000, replace=FALSE)
+   sample.means[i]<-mean(sample)
+ }
> sd(sample.means)
[1] 0.02359317 
\end{verbatim}

This method, however, has one main issue. Practitioners rarely are able to repeatedly sample from the larger population. Instead the standard error of the sample mean can be calculated through the following equation: 

\begin{termBox}{\tBoxTitle{Computing SE for the sample mean}
Given $n$ independent observations from a population with standard deviation $\sigma$, the standard error of the sample mean is equal to \vspace{-1mm}
\begin{eqnarray}
SE_{\text{ sample mean}} = \frac{\sigma}{\sqrt{n}}
\label{seOfXBar}
\end{eqnarray}\vspace{-3mm}%

A reliable method to ensure sample observations are independent is to guarantee that the sample from the population is a simple random sample with a size that is less than 10\% of the population.\index{standard error!single mean}
}
\end{termBox}

There is one subtle issue of Equation~(\ref{seOfXBar}): the population standard deviation is typically unknown. To resolve this problem, Section \ref{pointEstimates} suggests that the "best guess" for the population standard deviation can be used as a substitute. The point estimate of the population standard deviation can be used from an observed sample as a replacement instead. This estimate tends to be sufficiently good when the sample size is at least 30 and the population distribution is not strongly skewed. Thus, the sample standard deviation denoted $s$ is generally used more instead of the population standard deviation ~$\sigma$ in Equation \ref{seOfXBar}. When the sample size is smaller than 30, an additional method to account for extra uncertainty in the standard error needs to be used. If the skew condition is not met, a larger sample is needed to compensate for the extra skew. These topics are further discussed in Section~\ref{cltSection}. 

With the \data{BRFSS BMI} sample of 40,000, the standard error of the sample mean is calculated as

\begin{eqnarray*}
SE_{\bar{x}} = \frac{s}{\sqrt{n}} = \frac{5.288}{\sqrt{40000}} =  0.026
\end{eqnarray*}
where $s$ is the standard deviation of the sample and $n$ is the number of observations in the sample. We see that the standard error calculated $(0.026)$ is similar to the empirical standard deviation of the sampling distribution that was calculated above $(0.024)$. 

\begin{exercise}
In another sample of 40,000 US adults, the standard deviation of BMI is $s_\mathrm{bmi} = 5.34$. Because the sample is a simple random sample and consists of less than 10\% of the United States population, the observations are independent. (a)~What is the standard error of the sample mean, $\bar{bmi}=26.36$? (b)~Would you be surprised if someone told you the average BMI of all US adults was actually 30? What about 26? \footnote{(a) Use Equation~(\ref{seOfXBar}) with the sample standard deviation to compute the standard error: $SE_{\bar{\mathrm{bmi}}} = 5.34/\sqrt{40000} =  0.0267$. (b) It would be surprising if the average BMI of the US population was 30. A BMI of 30 is many many standard deviations away from the sample mean of 26.36. In other words, a BMI of 30 seems implausible given that our sample mean (26.36) is far from the population mean of 30 using the standard error of 0.0267 to identify what is close and what is not close. Even a BMI of 26 in this situation would be surprising given that the sample mean is more than one standard deviation (standard error of 0.0267) away from the population mean.}
\end{exercise}


\begin{exercise}
(a) As review, should you be more trusting of a sample that has 100 observations or 400 observations? (b) We want to show mathematically that estimates tends to be better when the sample size is larger. If the standard deviation of the individual observations is 10, what is the estimate of the standard error when the sample size is 100? What about when it is 400? (c) Explain how your answer to (b) mathematically justifies your intuition in part~(a).\footnote{(a) Look back to Section ~\ref{accuracyAndPrecision} on accuracy and precision. Extra observations are usually helpful in understanding the population, so a point estimate with 400 observations seems more trustworthy. (b) The standard error when the sample size is 100 is given by $SE_{100} = 10/\sqrt{100} = 1$. For 400: $SE_{400} = 10/\sqrt{400} = 0.5$. The larger sample has a smaller standard error. Therefore the sample mean with the large sample size will generally be closer to the population sample mean than the sample mean with $n=100$. (c) The standard error of the sample with 400 observations is lower than that of the sample with 100 observations. The standard error describes the typical error, and since it is lower for the larger sample, this mathematically shows the estimate from the larger sample tends to be more trustworthy (smaller standard error) -- though it does not guarantee that every large sample will provide a better estimate than a particular small sample.}
\end{exercise}

\subsection{Basic properties of point estimates}

Three goals were achieved in this section. First, point estimates from a sample were may be used to estimate population parameters. These point estimates also are not exact, and there exists some sampling variation. The sample mean is an example of a point estimate that is always accurate but not necessarily always precise. The precision of a point estimate can be represented through sampling variation and visualized through a sampling distribution. The point estimate that is observe is a single observation in the estimate's sampling distribution. Lastly, sampling variation and the uncertainty of the sample mean can be quantified using the standard error. The standard error of the sample mean can be mathematically represented in Equation~\eqref{seOfXBar} or approximately calculated through computation in $R$. While the standard error for other estimates -- such as the median, standard deviation, or any other number of statistics -- can also be quantified, these extensions will be postponed until later chapters and courses.

\index{point estimate)}

%__________________
\section{Confidence intervals}
\label{confidenceIntervals}

\index{confidence interval|(}

A point estimate from Section~\ref{pointEstimates} provides a single plausible value for a parameter. However, a point estimate is rarely perfect and exact; usually there is some error in the estimate. Section~\ref{pointEstimates} shows there exists sampling variation but a single point estimate does not convey how large this sampling variation is without including the point estimate's standard error as well. Instead of supplying a point estimate and standard error separately, the next logical step would be to provide a plausible \emph{range of values} to estimate the true value of the parameter.

This section and Section~\ref{hypothesisTesting} will emphasize the special case where the point estimate is a sample mean and the parameter of interest is the population mean. Section~\ref{aFrameworkForInference} generalizes these methods for a variety of point estimates and population parameters that will be encounter in Chapter~\ref{inferenceForNumericalData} and beyond.

\subsection{Capturing the population parameter}

A plausible range of values for the population parameter is called a \term{confidence interval}. The width of an interval provides a gauge of how large the sampling variation is. For the same confidence level, the larger the interval indicates the larger sampling variation and standard error.

Using only a point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing in the same lake with a net. Throwing a spear where fishermen see fish, they will probably miss. On the other hand, if fishermen toss a net in that area, they have a good chance of catching the fish.

Similarly if a point estimate is reported, the exact population parameter will probably not be hit. There is likely to be error associated with this estimate. On the other hand, if a range of plausible values is reported -- a confidence interval -- it becomes more likely that the parameter is captured within the range. As with fishing, the goal of the confidence interval is to include the population parameter within it. 

\begin{exercise}
If we want to be very certain we capture the population parameter, should we use a wider interval or a smaller interval?\footnote{If we want to be more certain when capturing a fish, we might use a wider net. Likewise, we use a wider confidence interval if we want to be more certain that we capture the parameter. The more values included in the range, the more likely it is that this range contains the true value. The interval contains simply \emph{more} values. However capturing the parameter is not always the only goal when constructing a confidence interval. The parameter will always be captured with the widest interval going from $-\infty$ to $+\infty$ but this range does not increase understanding of the population parameter. } 
\end{exercise}
\begin{exercise}
Suppose a confidence interval is 10 units wide and we are 50\% confident that the range encompasses the population parameter. If another interval was instead 5 units wide centered at the same value as the original interval, are we now more or less confident than 50\% that the range will include the population parameter? \footnote{We are less than 50\% confident that the smaller interval includes the population parameter. Using a smaller net with fewer values, we are less confident that the population parameter is captured.}
\end{exercise}

\subsection{Confidence levels}
\label{confidenceLevels}

The size of a fishing net depends on how confident fishermen want to be in catching a fish. Similarly, the size or width of a confidence intervals depends on how confident scientists want to be in estimating the true value of the parameter in question. Before jumping into the calculation of the confidence interval itself, the term "confident" needs to be first understood. 

Before scientists embark on most inference processes, they first much choose a confidence level. For a confidence interval, the confidence level is a percent that affects how wide the interval is. Confidence levels, in general, are associated with a level of uncertainty and how much a confidence interval or hypothesis test is allowed to commit a Type I Error or a false positive. Section~\ref{DecisionErrors} goes into more depth on Type 1 and Type 2 Errors. 

For example if the CDC says that they are 75\% confident that the population mean BMI is between two values, 75\% would be the measure of uncertainty and 25\% would be the probability of committing a Type 1 Error. In the context of confidence intervals, there is a 25\% chance that the confidence interval does not include the population parameter when in fact, it should. The Type I error is also known as $\alpha$.  

\begin{example}{Consider extreme confidence levels. What are the implications of a 100\% confidence level confidence interval? How about a 0.001\% confidence level?} \label{extremeConfidenceLevels}
A 100\% confidence level is equivalent to $\alpha = 0\%$. The confidence interval created will \emph{always} capture the population parameter. Therefore in order to guarantee this, the confidence interval will be $[-\infty, \infty]$ . Consider a confidence level of 0.001\%. This extremely low confidence level results in a very high Type 1 Error. In many cases when building a 0.001\% confidence interval, the confidence interval will not capture the population parameter. Therefore this interval is expected to be extremely narrow. 
\end{example}

Statisticians generally use a confidence level of 95\% per tradition but Section ~\ref{significanceLevel} demonstrates that any confidence level is allowed keeping inference goals in mind. But what does "95\% confident" truly mean? Suppose many samples were drawn and confidence intervals were built based on each sample. Then to be 95\% confident, approximately 95\% of those intervals would contain the actual mean, the population parameter, $\mu$. In the case with BMI, if the CDC took 100 independent samples and built 100 confidence intervals, 95 of these confidence intervals would contain the average US BMI and 5 of these would not.

Figure~\ref{95PercentConfidenceInterval} shows this process with 25 samples randomly drawn, where 24 of the resulting confidence intervals contain the average BMI for the population and one does not. 

\begin{figure}[hht]
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/95PercentConfidenceInterval/95PercentConfidenceInterval}
   \caption{Twenty-five samples of size $n=100$ were taken from the \data{BRFSS} data set. For each sample, a confidence interval was created to try to capture the average BMI for the population. Only~1 of these~25 intervals did not capture the true mean.}
   \label{95PercentConfidenceInterval}
\end{figure}

\subsection{Confidence intervals through computation}
\label{confidenceIntervalsComputation}

Figure~\ref{95PercentConfidenceInterval} should give inspiration on how to achieve an estimate of a 95\% confidence interval through computation. The goal, again, is to find a range of values that hopefully contains the population parameter. The natural method to estimate a confidence interval is through the sampling distribution from Figure~\ref{brfssBMISamplingDistribution} if independently resampling from the population is feasible. The mean of the sampling distribution was shown to be extremely close to the mean of the population, and thus a reasonable estimation for a 95\% confidence interval would be to take the middle 95\% of the sampling distribution. Below is pseudocode that implements this procedure using samples of size 40,000 \footnote{This highly resembles the pseudocode for approximating a sampling distribution from Section ~\ref{seOfTheMean}}
\begin{verbatim}
(1) Have a place to store all the sample means that we will calculate
(2) Take a sample from the BRFSS dataset of 40,000
(3) Calculate the sample mean from this specific sample and store it in (1)
(4) Repeat (2) and (3) many many times 
(5) Use the middle 95% of values as the 95% confidence interval. 
\end{verbatim}

Instead of plotting the values of the sample means (stored as \var{sample.means} within the $R$ code) to create the sampling distribution, the 95\% confidence interval is just the middle 95\% of all sample means calculated. 

The confidence interval itself is the BMI value  $c_1$ such that 2.5\% of the \var{sample.mean} values is below $c_1$ and another BMI value $c_2$ such that 2.5\% of the distribution values is greater than $c_2$. In order to find these values, recall from the distributions unit ~\ref{distributions} that we need to use the \var{quantile()} function in $R$. Particularly use the \var{quantil()} function on \var{sample.means}, the vector that that stores the sample means. 
\begin{verbatim}
confidence.interval<-quantile(x=sample.means,c(0.025,0.975))
> confidence.interval
    2.5%    97.5% 
26.30493 26.39697 
\end{verbatim}
The interval (26.30,26.40) is an estimation for a 95\% confidence interval using the sampling distribution of sample means. Scientists at the CDC can say that they are 95\% confident that the population mean BMI is between 26.30 and 26.40. Similarly after calculating many confidence intervals from many different observed samples, 95\% of all the confidence intervals that were calculated will contain the population mean. 

\begin{exercise}
Say the CDC was interested in creating a 90\% confidence interval and a 50\% confidence interval. (a) how do the widths of the confidence intervals would compare? (b) How would the \var{quantile()} function be used to find the 90\% and 50\% confidence intervals from using the array \var{sample.means}?
 \footnote{(a) The 50\% confidence interval would be expected to have the smaller width. In general the more confident people are, the larger the confidence interval width will be.(b) Remember to grab the middle percent of observed sample means. Therefore using \var{sample.means}, the $R$ code for calculating a 90\% confidence interval  would be \var{quantile(x=sample.means,c(0.05,0.95))}. For a 50\% confidence interval, the code would be \var{quantile(x=sample.means,c(0.25,0.75))}}
\end{exercise}

\subsection{Calculating an approximate 95\% confidence interval}
\label{calculate95confidence}

Computing the confidence interval from resampling is straightforward and serves as a great estimate for any confidence interval for the population. However, is this realistic beyond simulation? In general do scientists have the ability to resample the US population independently 100,000 times? Generally no. Most times people can't observe 100,000 sample means of BMI values let alone 100,000 complete samples from the US population. More often than not, researchers only view 1 sample and thus calculate only 1 sample mean. How then are confidence intervals calculated from only observing one sample mean? 

The confidence interval can be completely defined by two attributes: its center and its width. A confidence interval from previous computation is derived from a sampling distribution and ideally should be centered at the population parameter. However only observing one sample, it becomes intuitive to build the confidence interval around the observed sample mean. 
  
The width of the interval should encompass the confidence level, in this case 95\%, as well as the uncertainty associated with the point estimate. Recall from Section \ref{seOfTheMean} the definition standard error, and thus it becomes a natural measurement of uncertainty for building the interval.  

Roughly 95\% of the time, the estimate that is observed from resampling will be within approximately 2 standard errors\footnote{1.96 to be more precise if the sampling distribution resembles a Normal Distribution. Details coming up in Section ~\ref{sampdistmean}} of the population parameter. Therefore the 95\% confidence interval is 2 standard errors wide on either side of the sample mean. Equation ~\ref{95PercentConfidenceIntervalFormula} allows scientists to be roughly \term{95\% confident} that the confidence interval has captured the population parameter from the sample observed:
\begin{eqnarray}
\text{point estimate}\ \pm\ 2 \times SE \footnote{95\% confidence corresponds closer to 1.96 standard errors away} 
\label{95PercentConfidenceIntervalFormula}
\end{eqnarray}
Using the BMI sample from ~\ref{seOfTheMean} with an observed sample mean of 26.36 and sample standard deviation of 5.29, the 95\% confidence interval is calculated. 
\begin{align*}
\text{point estimate}\ &\pm\ 2 \times SE\\
26.36 &\pm 2\times \frac{5.29}{\sqrt{40000}}\\
26.36 &\pm  0.053\\
(26.31 &, 26.41)
\end{align*}
While not exact, the confidence interval simulated through $R$  achieves a very similar confidence interval as the one through calculation. The difference is due to randomness within the samples observed.
\begin{exercise}
In Figure~\ref{95PercentConfidenceInterval}, one interval does not contain a BMI value of 26.36. Does this imply that the average population BMI cannot be 26.36? \footnote{Just as some observations occur more than 2 standard deviations from the mean, some point estimates will be more than 2 standard errors from the parameter. A confidence interval only provides a plausible range of values for a parameter. While other values are implausible based on the data, this does not mean they are absolutely impossible.}
\end{exercise}

The statement "about 95\% of observations are within 2 standard deviations of the mean" is only approximately true. This rule of thumb holds very well for the normal distribution. As Section ~\ref{cltSection} soon shows, the sample mean tends to be normally distributed when the sample size is sufficiently large. 

\begin{example}{The CDC is also curious about how the average heights of men and women differ and create 95\% confidence intervals for the average male height and the average female height. The \data{BRFSS BMI} data can be divided using the \var{sex} variable. Among the 40,000 individuals within the \data{BRFSS BMI}, there are 16,843 men and 23,157 women. The average male height is 70.22 inches and the average female height is 64.38. The sample standard deviations for males and females  are 3.00 and 2.80 respectively. What are the 95\% confidence intervals for the average male and female height in the US? }\label{CIforGenderHeight}
Both 95\% confidence intervals are calculated using the formula \[\text{point estimate}\ \pm\ 2 \times SE\] and the information given above: 
\begin{align*}
\text{men: point estimate}\ &\pm\ 2 \times SE\\
70.22 &\pm 2\times \frac{3.00}{\sqrt{16843}}\\
70.22 &\pm  0.05 \\
(70.17 &, 70.27)
\end{align*}
\begin{align*}
\text{women: point estimate}\ &\pm\ 2 \times SE\\
64.38 &\pm 2\times \frac{2.80}{\sqrt{23157}}\\
64.38 &\pm  0.04\\
(64.34 &, 64.42)
\end{align*}
With different centers and different widths, the confidence intervals for average height by gender are different.  
\end{example}

The creation of the 95\% confidence interval depends on the center and the standard error. In Section ~\ref{changingTheConfidenceLevelSection}, we will see how the multiplier changes beyond 2 standard errors as confidence levels change. 

\begin{exercise} \label{95CIExerciseForBRFSSAge}
The sample data \data{BRFSS BMI} suggests the average adult's age is about 46.64 years with a standard error of 0.09 years (estimated using the sample standard deviation, 17.35). What is an approximate 95\% confidence interval for the average age of US adults?\footnote{Again apply Equation~(\ref{95PercentConfidenceIntervalFormula}): $46.64 \ \pm \ 2\times 0.09 \rightarrow (46.46, 46.82)$. The interpretation of the interval is, "We are about 95\% confident the average age of US adults is between 46.46 and 46.82 years." Looking at the entire \data{BRFSS} dataset that the \data{BRFSS BMI} sample is drawn from (normally we do not have this luxury!), the average age is 46.72 which is indeed within the confidence interval just calculated.}
\end{exercise}

\subsection{The sample size for a sampling distribution}
\label{sampdistmean}

In Section~\ref{seOfTheMean}, a sampling distribution for $\bar{\mathrm{bmi}}$, the average BMI value for samples of size 5 and 50, was introduced in Figure~\ref{brfssBMISamplingDistribution}. With larger sample sizes like $n=40,000$, the sampling variation decreases significantly than with $n=5$ or $n=50$. In Figure ~\ref{sampleMeanPrecision}, the sampling distribution for $n=5$ was slightly skewed but the sampling distribution for $n=50$ looks more symmetric. Figure \~ref{sampDistNormal} is a histogram of the sample means for 100,000 different random samples of size $n=50$ with a normal probability plot of those sample means. 

\begin{figure}[hht]
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/sampDistNormal/sampDistNormal}
   \caption{The left panel shows a histogram of the sample means for 100,000 different random samples of size $n=100$. The right panel shows a normal probability plot of those sample means.}
   \label{sampDistNormal}
\end{figure}

Does this distribution look familiar (think back to Chapter ~\ref{modeling} of probability distributions)? Hopefully so! The distribution of sample means closely resembles the normal distribution (see Section~\ref{normalDist}). A normal probability plot of these sample means is shown in the right panel of Figure~\ref{sampDistNormal}. The distribution of sample means is nearly normal because all of the points closely fall around a straight line. This result can be explained by the Central Limit Theorem\footnote{A more formal definition coming soon in ~\ref{cltSection}}.

\begin{termBox}{\tBoxTitle{Central Limit Theorem, informal description}
If a sample consists of at least 30 independent observations and the data are not strongly skewed, then the distribution of the sample mean is well approximated by a~normal model.\index{Central Limit Theorem}}
\end{termBox}

\subsubsection{Why 30?}
\label{why30}

The Central Limit Theorem uses a cutoff at 30 in this text but can very from book to book. As a quick exercise in statistical exploration and algorithmic thinking, think about how one would visually test if 30 independent observations is a sufficient number of observations to approximate the distribution to a normal model.  The \data{BRFSS} data can be used to sample from like before. 

To determine if 30 observations is a sufficient number, other values below 30 should also be considered to see if the cutoff is truly around 30 observations. Creating a sampling distribution of the sample mean for sample sizes of $n=5,10,20,30$ and overlaying a normal approximation on the histogram is a great guide to see if the distribution is approximated by a normal model. \footnote{Use the same code for creating a sampling distribution but vary the sample size. Then use the code: 
\var{hist(sample.means, freq=FALSE ) \\
curve(dnorm(x,mean=mean(sample.means), sd=sqrt(var(sample.means))), add = TRUE)} where the function \var{curve()} adds the normal curve on top of the histogram.}

Figure ~\ref{cltThirty} displays the sampling distributions of the sample mean for sample sizes of 5, 10, 20 and 30. The curve on top is a normal density curve with the normal distribution $\mathcal{N}(\mu, \sigma^2)$ where $\mu$ is the mean of the sample means and $\sigma$ is the standard deviation of the sample means.

\begin{figure}[hht] 
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/clt30/clt30}
   \label{cltThirty}
   \caption{The sampling distribution of sample means with different sample sizes $n=5, 10, 20, 30$. With a normal density curve on top, a normal model is begins to become a fitting approximation for  $n=30$, confirming the Central Limit Theorem rule of thumb.}
\end{figure}

This section will consider this informal version of the Central Limit Theorem for now, and Section ~\ref{cltSection} will discuss further details. 

The choice of using 2 standard errors in Equation~(\ref{95PercentConfidenceIntervalFormula}) was based on the general guideline that roughly 95\% of the time, observations are within two standard deviations of the mean. Under the normal model, with a sufficient number of samples ($n\geq 30$), the equation can be more accurate by using 1.96 in place of 2.
\begin{eqnarray}
\text{point estimate}\ \pm\ 1.96\times SE
\label{95PercentCIWhenUsingNormalModel}
\end{eqnarray}
Use Equation ~\ref{95PercentCIWhenUsingNormalModel} to create a 95\% confidence interval if a point estimate, such as $\bar{x}$, is associated with a normal model with standard error $SE$.

\subsection{Changing the confidence level}
\label{changingTheConfidenceLevelSection}

\index{confidence interval!confidence level|(}
Section ~\ref{confidenceLevels} foreshadowed confidence intervals using confidence levels beyond 95\% confident. Just as fishermen can fish with many different sizes of nets, scientists can create many different confidence intervals from the same sample by simply changing the confidence level. The CDC measuring the average BMI for US adults is a fairly low risk problem if the population parameter were measured incorrectly. Imagine instead the US Food and Drug Administration (FDA) was studying a drug's effects on children. The FDA might require evidence at the 99\% confidence level instead of simply the 95\% confidence level. Parameters that have life threatening implications or problems that are higher risk \footnote{In many cases generally, these are not the only factors that determine a confidence level. Factors like funding, logistics, the number of observations all are considered when choosing a confidence level} call for an increase in confidence level for both calculating confidence intervals and performing hypothesis tests. 

Think back to the analogy about trying to catch a fish: if fishermen want to be more sure that they will catch the fish, they should use a wider net. To create a 99\% confidence level, scientists must also widen the 95\% interval. On the other hand, if they want an interval with lower confidence, such as 90\%, they could make the original 95\% interval slightly slimmer.

The 95\% confidence interval structure provides guidance on creating intervals with confidence levels beyond 95\%. Below is a general 95\% confidence interval for a point estimate where the point estimate follows a nearly normal distribution.
\begin{eqnarray}
\text{point estimate}\ \pm\ 1.96\times SE
\end{eqnarray}

Recall from Section \label{calculate95confidence} that a confidence interval is defined by its center and width. Changing the confidence level will not change the center of the interval since the point estimate is independent of confidence level. The width, however, of a 95\% confidence interval, $1.96\times SE$, represents the width required to "capture 95\%" of the sampling distribution as seen in Figure ~\ref{choosingZForCI} for data that is approximately normally distributed. Therefore changing the confidence level will not affect the center but will definitely affect the width.


\begin{exercise} \label{leadInForMakingA99PercentCIExercise}
If $X$ is a normally distributed random variable, how often will $X$ be within 2.58 standard deviations of the mean?\footnote{This is equivalent to asking how often the $Z$ score will be larger than -2.58 but less than 2.58. (For a visualization, see Figure~\ref{choosingZForCI}.) To determine this probability, look up -2.58 and 2.58 in the normal probability table (0.0049 and 0.9951). There is a $0.9951-0.0049 \approx 0.99$ probability that the unobserved random variable $X$ will be within 2.58 standard deviations of $\mu$.}
\end{exercise}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/choosingZForCI/choosingZForCI}
\caption{If the confidence level is 99\%, choose $z^{\star}$ such that 99\% of the normal curve is between -$z^{\star}$ and $z^{\star}$, which corresponds to 0.5\% in the lower tail and 0.5\% in the upper tail: $z^{\star}=2.58$.}
\label{choosingZForCI}
\index{confidence interval!confidence level|)}
\end{figure}


How does the confidence level affect the width then? The standard error, calculated from sample, remains constant regardless of confidence level. The multiplier, the "1.96", instead changes and affects the confidence interval width. To have a 99\% confidence interval, change 1.96 in the 95\% confidence interval formula to be $2.58$ for a 99\% confidence interval. Exercise~\ref{leadInForMakingA99PercentCIExercise} highlights that 99\% of the time a normal random variable will be within 2.58 standard deviations of the mean. This approach -- using the Z scores in the normal model to compute confidence levels -- is appropriate when $\bar{x}$ is associated with a normal distribution with mean $\mu$ and standard deviation $SE_{\bar{x}}$. The formula for a 99\% confidence interval of the mean is
\begin{eqnarray}
\bar{x}\ \pm\ 2.58\times SE_{\bar{x}}
\label{99PercCIForMean}
\end{eqnarray}

Equation ~\ref{99PercCIForMean} is not population parameter specific. In fact, it can be generalized for any parameter to be \[\text{point estimate} \pm 2.58\times SE\]

The normal approximation is crucial to the precision of these confidence intervals. Section~\ref{cltSection} provides a more detailed discussion about when the normal model can safely be applied based on the Central Limit Theorem. When the normal model is not a good fit, alternative distributions can be used that better characterize the sampling distribution. Below however is a good checklist to determine whether or not the Central Limit Theorem can be informally applied to the distribution of sample mean.

\begin{termBox}{\tBoxTitle{Conditions for $\bar{x}$ being nearly normal and $SE$ being accurate\label{terBoxOfCondForXBarBeingNearlyNormalAndSEBeingAccurate}}
Important conditions to help ensure the sampling distribution of $\bar{x}$ is nearly normal and the estimate of SE sufficiently accurate:
\begin{itemize}
\setlength{\itemsep}{0mm}
\item The sample observations are independent.
\item The sample size is large: $n\geq30$ is a good rule of thumb.
\item The population distribution is not strongly skewed. (Check this using the sample distribution as an estimate of the population distribution.)
\end{itemize}
Additionally, the larger the sample size, the more lenient scientists are with the sample's skew.}
\end{termBox}

These three conditions help ensure that $\bar{x}$ is both distributed normally and representative of the target population. If the distribution of $\bar{x}$ is nearly normal, choosing a precise "1.96" or "2.58" becomes much easier for calculating confidence intervals. More importantly, however, the representativeness of the sample is imperative in the ability to infer about the target population. Randomness, independence and a large sample size safeguard against an extreme observation from skewing the conclusions from a sample. These conditions ensure the ability to accurately infer and generalize to the population of interest.

Verifying independence is often the most difficult of the conditions to check, and the way to check for independence varies from one situation to another. However, randomness is almost always included for independence. 

\begin{tipBox}{\tipBoxTitle{How to verify sample observations are independent}
Observations in a simple random sample consisting of less than 10\% of the population are independent.}
\end{tipBox}

\begin{caution}
{Independence for random processes and experiments}
{If a sample is from a random process or experiment, it is important to verify the observations from the process or subjects in the experiment are nearly independent and maintain their independence throughout the process or experiment. Usually subjects are considered independent if they undergo random assignment in an experiment or are selected randomly for some process.}
\end{caution}

\begin{exercise} \label{find99CIForBRFSSWeightExercise}
Create a 99\% confidence interval for the average weight of men from the \data{BRFSS BMI} sample. The point estimate is $\bar{w_{\mathrm{men}}} = 189.4$ pounds and the standard error is $SE_{\bar{w_{\mathrm{men}}}} = 0.28$ pounds. Refer to Figure ~\ref{brfssMenWeight} for guidance on skewness.\footnote{The observations are independent (simple random sample, $<10\%$ of the population), the sample size is at least 30 ($n=16,843$), and the distribution is only slightly skewed (Figure~\ref{brfssMenWeight}); the normal approximation and estimate of SE should be reasonable.  Apply the 99\% confidence interval formula: $\bar{w_{\mathrm{men}}}\ \pm\ 2.58 \times  SE_{\bar{w_{\mathrm{men}}}} \rightarrow (188.69, 190.12)$. We are 99\% confident that the average weight of all males is between 188.69 and 190.12 pounds.}
\end{exercise}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/brfssMenWeight/brfssMenWeight.pdf}
\caption{The histogram of men's weights in the \data{BRFSS BMI} sample. There are 16,843 men in \data{BRFSS BMI}, and the weights are slightly skewed. With such a large sample size, the sample mean can be considered nearly normal.}
\label{brfssMenWeight}
\index{confidence interval!confidence level|)}
\end{figure}

The calculation of a confidence interval for any confidence level can be generalized from the 95\% and 99\% confidence intervals. Remember that while it has become tradition to use the 95\% confidence level, any confidence level is allowed and varies by statistician and by goal. 

\begin{termBox}{\tBoxTitle{Confidence interval for any confidence level (nearly normal model)}
If the point estimate follows the normal model with standard error $SE$, then a confidence interval for the population parameter is
\begin{eqnarray*}
\text{point estimate}\ \pm\ z^{\star} SE
\end{eqnarray*}
where the value of $z^{\star}$ corresponds to the confidence level selected. The coefficient on the standard error, $z^{\star}$, is also known as the critical value. $z^{\star}$ is only used when the point estimate resembles a normal model \footnote{$z^{\star}$ is also used when the population standard deviation is known. However this is rarely ever the case in practice as previously mentioned and thus this situation is disregarded completely.}}
\end{termBox}
\begin{termBox}{\tBoxTitle{Margin of error}
\label{marginOfErrorTermBox}In a confidence interval, $z^{\star}\times SE$ is called the \term{margin of error} and is half the width of the confidence interval.}
\end{termBox}

Figure ~\ref{choosingZForCI} provides a picture of how to identify $z^{\star}$ based on a confidence level. Select $z^{\star}$ so that the area between -$z^{\star}$ and $z^{\star}$ in the normal model corresponds to the confidence level. Because $z^{\star}$ follows a $\mathcal{N}(0,1)$, scientists either use $R$ or a Z-table \footnote{also known as a Normal table} (\textbf{FOUND IN THE BACK OF THE BOOK HERE}) to find the critical value. The \var{qnorm()} function $R$ takes in a probability $p$ and outputs the quantile value $z$ such that $P(Z\leq z)=p$. For a 95\% confidence interval, $p=0.025$ since the confidence interval is the \emph{middle} 95\% of values. To find $z^{\star}$ in $R$\begin{verbatim}
> qnorm(0.025)
[1] -1.959964
\end{verbatim}
$z^{\star}=1.96$ is the critical value for 95\% \footnote{It does not matter if $z^{\star}$ is positive or negative since the half widths are on both sides of the point estimate.}. 
\begin{exercise} 
What is the critical value associated with (a) 90\%, (b) 75\% and (c) 50\%? \footnote{Remember to only consider the \emph{middle} values, and for any given confidence level $C$, type \var{qnorm($0.5\cdot (1-C)$)} into $R$.(a) \var{qnorm(0.05)= -1.644854} so $z^{\star}=1.65$ for a 90\% confidence level (b) 1.15 (c) 0.67}
\end{exercise}

\begin{exercise} \label{find90CIForBRFSSWeightExercise}
Use the data in Exercise~\ref{find99CIForBRFSSWeightExercise} to create a 90\% confidence interval for the average weight of men in the United States.\footnote{First find $z^{\star}$ such that 90\% of the distribution falls between -$z^{\star}$ and $z^{\star}$ in the standard normal model, $N(\mu=0, \sigma=1)$.  Find $z^{\star}$ in the normal probability table by looking for a lower tail of 5\% (the other 5\% is in the upper tail). $z^{\star}=1.65$. The 90\% confidence interval can then be computed as $\bar{w_\mathrm{men}}\ \pm\ 1.65\times SE_{\bar{w_\mathrm{men}}} \to (188.95, 189.86)$.  (We had already verified conditions for normality and the standard error in Exercise ~\ref{find99CIForBRFSSWeightExercise}.) That is, we are 90\% confident the average weight of males is between 188.95 and 189.86 pounds. Also note that because we are at a 90\% confidence level, our confidence interval width is smaller than the confidence interval in Exercise ~\ref{find99CIForBRFSSWeightExercise}.}
\end{exercise}

\subsection{Interpreting confidence intervals}
\label{interpretingCIs}

\index{confidence interval!interpretation|(}

A careful eye might have observed the somewhat awkward language used to describe confidence intervals. Correct interpretation:
\begin{quote}
We are XX\% confident that the population parameter is between...
\end{quote}

Looking back to ~\ref{confidenceIntervalsComputation}, confidence means that if a random sample from the population was drawn 100 times independently and a confidence interval was calculated around the point estimate each time, 95 confidence intervals would contain the true population parameter. 

It is interesting to note, however, that researchers in practice would almost never be able to resample 100 times and generate 100 confidence intervals. The meaning of being "95\% confident" has traditionally been one grounded in theory and less in practice. "Confidence" relates more to the reliability of the process of creating such a range and less so in the probability that the value is within the range. 

\emph{Incorrect} language might try to describe the confidence interval as capturing the population parameter with a certain probability. This is one of the most common errors: while it might be useful to think of it as a probability, the confidence level only quantifies how plausible it is that the parameter is in the interval. 

Another especially important consideration of confidence intervals is that confidence intervals \emph{only try to capture the population parameter}. Intervals say nothing about the confidence of capturing individual observations, a proportion of the observations, a percent of all the data or just the sampled data. A confidence interval also says nothing about capturing point estimates since the confidence interval is always centered at the observed point estimate. Confidence intervals only attempt to capture population parameters as statistical inference's goal is to infer on such population parameters.

Some incorrect interpretations of a 95\% confidence interval include: 
\begin{quote}
95\% of the observed data is between ...\\
95\% of the population distribution is contained in the confidence interval.\\
\end{quote}

Remember, a confidence interval is not a range of plausible values for the sample mean. The sample mean is already known. It is already observed. The confidence interval may be understood as an estimate of plausible values for the population parameter.

While the differences in correct and incorrect interpretations are extremely nuanced, the goal of this book is to provide the tools and mechanisms of calculating and computing a confidence interval from data and less so about the wording which, in practice, has become almost meaningless and obsolete. 

\index{confidence interval!interpretation|)}
\index{confidence interval|)}

\subsection[Nearly normal population with known SD (special topic)]{Nearly normal population with known SD (special topic)}
\label{nearlyNormalPopWithKnownSD}

\index{Central Limit Theorem!normal data|(}

In rare circumstances important characteristics of a population are known. For instance, scientists might already know that a population is nearly normal and they do not need to rely on a rule of thumb of $n\geq 30$. Scientists might also know certain parameter values. Even so, a random sample from a population might still be drawn to study other characteristics of the population. Consider the conditions required for modeling a sample mean using the normal distribution:
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item[(1)] The observations are independent.
\item[(2)] The sample size $n$ is at least 30.
\item[(3)] The data distribution is not strongly skewed.
\end{enumerate}
These conditions are required to ensure the distribution of sample means is nearly normal. However, if the population is known to be nearly normal, we know that the sample mean is always nearly normal (this is a special case of the Central Limit Theorem). If the standard deviation for the population is also known, then conditions (2) and (3) are not necessary for those data. 
\begin{example}{The heights of male seniors in high school closely follow a normal distribution $N(\mu=70.43, \sigma=2.73)$, where the units are inches.\footnote{These values were computed using the USDA Food Commodity Intake Database.} If the heights of five male seniors are randomly sampled, what distribution should the sample mean follow?}\label{simpleSampleOfFiveMaleSeniors}
The population is nearly normal, the population standard deviation is known, and the heights represent a random sample from a much larger population, satisfying the independence condition. Therefore the sample mean of the heights will follow a nearly normal distribution with mean $\mu=70.43$ inches and standard error $SE=\sigma/\sqrt{n} = 2.73/\sqrt{5}=1.22$ inches.
\end{example}

\begin{termBox}{\tBoxTitle{Alternative conditions for applying the normal distribution to model the sample mean}
If the population of cases is known to be nearly normal and the population standard deviation $\sigma$ is known, then the sample mean $\bar{x}$ will follow a nearly normal distribution $N(\mu, \sigma/\sqrt{n})$ if the sampled observations are also independent. With a known population standard deviation, the confidence interval does not need to include a standard error calculation and just replaces the population standard deviation for the standard deviation estimate from the observed sample.}
\end{termBox}

\begin{tipBox}{\tipBoxTitle{Relaxing the nearly normal condition}
As the sample size becomes larger, it is reasonable to \emph{slowly} relax the nearly normal assumption on the data for dealing with small samples. By the time the sample size reaches 30, the data must show strong skew to continue to have concerns about the normality of the sampling distribution.}
\index{Central Limit Theorem!normal data|)}
\end{tipBox}

In practice, the population standard deviation is rarely know, but the Central Limit Theorem allows the distribution of the sampling distribution to be described more specifically. 

%__________________
\section{Hypothesis testing}
\label{hypothesisTesting}

\index{hypothesis testing|(}

The WHO categorizes BMI values as underweight, normal, overweight and obese. Assuming that a BMI of 22 is considered healthy weight, the CDC might be curious to see if the average US BMI is equal to a BMI of 22. A rural college is interested in showing that students at the college sleep longer than 7 hours per night, the national college average, using a sample of its students. 

Section \textbf{SOME SECTION} will test if the average US BMI is equal to 22. Section \textbf{SOME OTHER SECTION} will investigate the sleeping habits of state college students. Many questions like these, given the correct data, can be answered through Hypothesis Testing. \term{Hypothesis testing} is a method in statistics that evaluates whether or not a population parameter takes on some hypothesized value or not with an associated probability of error. It is, most obviously, determining the probability that a given hypothesis is true or not.

Hypotheses are often simple questions that have a yes or no answer. Consider some hypotheses below: \begin{quote}
Is the mean body temperature really $98.6^\circ$ F? \\
Has consumption of soda changed across the US overtime? \\
Do MCAT classes improve MCAT scores? 
\end{quote}

The hypothesis testing process consists of generally 5 steps. Going through the \term{hypothesis testing framework} allows scientists to answer these yes/no questions with a certain degree of confidence after observing a related sample. This section begins by testing a hypothesis about a population mean from observing one sample. Remember, hypothesis testing, just as with confidence intervals can concern any population parameter. It can be the population mean, population standard deviation or even the population IQR if desired. 

\subsection{Hypothesis testing framework}
\label{hypothesisFramework}

The average BMI of \data{BRFSS BMI} is 26.36. The CDC is interested if this sample provides enough evidence that adults are, on average, healthy versus the alternative that they are not. This question can be simplified into two \termsub{hypotheses}: 
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[$H_0$:] US adults are on average healthy with an average BMI of 22. 
\item[$H_A$:] The average adult's BMI is not 22 lbs i.e. Average adults are on average not healthy.
\end{itemize}

\subsubsection{Step 1: Formulating Hypotheses}

The first step within the hypothesis testing framework is setting up the hypotheses. As shown above, there are  generally two hypotheses, a null and an alternative. 

$H_0$ is called\marginpar[\raggedright\vspace{6mm}

$H_0$\\\footnotesize null hypothesis\vspace{3mm}\\\normalsize $H_A$\\\footnotesize alternative\\ hypothesis]{\raggedright\vspace{6mm}

$H_0$\\\footnotesize null hypothesis\vspace{3mm}\\\normalsize $H_A$\\\footnotesize alternative\\ hypothesis} the null hypothesis and $H_A$ the alternative hypothesis.

\begin{termBox}{\tBoxTitle{Null and alternative hypotheses}
{\small The \term{null hypothesis ($H_0$)} often represents either a skeptical perspective or a claim to be tested. The \term{alternative hypothesis ($H_A$)} represents an alternative claim under consideration and is often represented by a range of possible parameter values.}}
\end{termBox}

The null hypothesis often represents a skeptical position or a common view on something. The null hypothesis is generally denoted as "no difference" or what one would observe if there is no change.  The alternative hypothesis often represents a new perspective, such as the possibility that there has been a change or a new discovery. If the null hypothesis is true, any difference between the observed sample is due only to chance variation. 

\begin{tipBox}{\tipBoxTitle{Hypothesis testing framework}
The logic of hypothesis testing is that the null hypothesis ($H_0$) will not be rejected, unless the evidence in favor of the alternative hypothesis ($H_A$) is so strong that $H_0$ must be rejected in favor of $H_A$.}
\end{tipBox}

The first step within the hypothesis testing framework is a very general tool, and is often used without a second thought. If a person makes a somewhat unbelievable claim, people are initially skeptical. The nulll hypothesis $H_0$ is generally believed. However, if there is sufficient observed evidence that supports the claim, skepticism is set aside and the null hypothesis is rejected in favor of the alternative. 

\begin{exercise} \label{hypTestStudyExample}
A new study would like to be published in a scientific journal. The board that determines the validity of the study considers two possible claims about this study: either the study is valid or pseudoscience. Set these claims up in a hypothesis framework. Which would be the null hypothesis and which the alternative? \footnote{The board considers whether the study's evidence, results and reproducibility is so convincing (strong) that the study must be valid. In this case, if the study is legitimate, the board rejects the null hypothesis (the study is pseudoscience) and concludes that the study is valid an should be published (alternative hypothesis).}
\end{exercise}

The scientists who sit on the board of publication journals look at the study and previous literature to see whether the science is convincingly valid. Even if these scientists leave unconvinced that the study is publishable, these board members do not necessarily believe the study is a complete fabrication. This mindset is also consistent with hypothesis testing: \emph{the null hypothesis cannot be accepted as true even if the null hypothesis is not rejected}. Failing to find strong evidence for the alternative hypothesis is not equivalent to accepting the null hypothesis.

\begin{tipBox}{\tipBoxTitle{Double negatives can sometimes be used in statistics}
In many statistical explanations, double negatives can be used. For instance, scientists might say that the null hypothesis is \emph{not implausible} or they \emph{failed to reject} the null hypothesis. Double negatives are used to communicate that while a position is not being rejected, it is also not correct.}
\end{tipBox}

The null hypothesis represents that the average US BMI is equal to 22 in the \data{BRFSS BMI} example. The alternative hypothesis represents something new or more interesting: the average US adult does not have a healthy BMI. These hypotheses can be described in mathematical notation using $\mu_{\mathrm{bmi}}$ as the average BMI for US adults.
\[H_0:\mu_{\mathrm{bmi}} = 22 \hspace{0.5in} H_A: \mu_{\mathrm{bmi}} \neq 22\]

where 22 represents the BMI of a healthy US adult. Using this mathematical notation, the hypotheses can now be evaluated using statistical tools. In this context, 22 is referred to as the \term{null value} since it represents the value of the parameter if the null hypothesis is true. The \data{BRFSS BMI} sample will be used to evaluate these hypotheses. 

Note it is important to remember that the CDC is not testing whether or not the average BMI observed from \data{BRFFS BMI} is 22 or not. They don't need to test that since the CDC observes all the values within \data{BRFSS BMI} and can simply calculate the average BMI. Rather when asking these hypotheses, they are testing if the \emph{population parameter} of all US adults is 22 or not. 

\begin{tipBox}{\tipBoxTitle{Null and Alternative Hypothesis Setup}
The null hypothesis is generally written as $H_0: \mu=\mu_0$ where $\mu$ is the population mean and $\mu_0$ is the hypothesized value that is believed to be true.The alternative hypothesis, on the other hand, can be many things.\\ 

If there exists no prior belief to influence the alternative hypothesis and the researchers are interested in showing any difference --an increase or decrease-- then the safest alternative hypothesis choice would be $\mu\neq \mu_0$, a two-sided alternative. If there does exist a prior belief of how $\mu$ and $\mu_0$ compare or researchers are interested in only showing an increase or decrease, not both, a one-sided alternative, $\mu\geq \mu_0$ or $\mu \leq \mu_0$ is available. Section ~\ref{pValue} will go into more detail on one-side versus two sided alternatives.}
\end{tipBox}

\subsubsection{Step 2: Specifying a Significance Level $\alpha$}
Step 1 is completed when the researchers have stated a null and alternative hypothesis. The researchers then need to specify a \term{significance level}. The significance level $\alpha$ is the acceptable error probability of the test. In hypothesis testing, the error probability is the probability of concluding the alternative hypothesis is true when it is not true. This error is called a Type I error, and $\alpha$ is the probability of a Type I error. Section \ref{DecisionErrors} will go into more detail on error types. 

Typically, $\alpha$ is taken to be 0.05, 0.01, or some other small value. $\alpha$ plays the same role as the error probability in confidence intervals, and is a measure of uncertainty. If $\alpha=0.05$, the hypothesis test will be at a 95\% confidence level. Section ~\ref{utilizingOurCI} will provide a clearer connection between hypothesis testing and confidence intervals.  

\subsubsection{Step 3: Calculating the Test Statistic}
The third step is calculating a test statistic from the observed data. The conclusions of the hypothesis test in Step 4 and 5 will be based upon this test statistic. This statistic measures the difference between the observed data and what is expected if the null hypothesis is true. It answers the question: "How many standard deviations from the hypothesized value is the observed sample value?" Thinking back to \textbf{THIS SECTION} ~\ref{some section in chapter 2 or 3} of standardizing a normal, the test statistic follows a similar construction. When testing hypotheses about a mean, the test statistic will always be 
\begin{eqnarray}T=\frac{\bar{x}-\mu_0}{s/\sqrt{n}}\end{eqnarray} 
where $\bar{x}$ is the sample mean, $s$ is the sample standard deviation and $n$ is the number of observations in the sample. This equation only applies to testing after observing only one sample. 
\emph{Note:} In general test statistics follow the form $\frac{\mathrm{observed-hypothesized}}{\mathrm{standard error}}$ to calculate the number of standard deviations the observed value is from the hypothesized value. This T-statistic, $T$, follows a $t$-distribution \textbf{INCLUDE CHAPTER 3} \footnote{ from \textbf{Chapter 3}} and will have $n-1$ degrees of freedom. 

\begin{termBox}{\tBoxTitle{Test statistic}
A \emph{test statistic} is a special summary statistic that is particularly useful for evaluating a hypothesis test or identifying the p-value. The test-statistic is a particular data summary that summarizes how many standard deviations the hypothesized null value is from the observed sample value. In general the T-statistic follows a $t$-distribution with $n-1$ degrees of freedom. The $t$-distribution will be covered in Section ~\ref{SOMETHING} \textbf{SOMETHING} \footnote{When a point estimate is nearly normal, the Z score of the point estimate can be used as the test statistic. Later chapters will encounter situations where other test statistics are helpful.}}
\index{hypothesis testing!using normal model|)}
\end{termBox}

\subsubsection{Step 4: Calculating the p-value}
Consider a T-statistic of 10. This means that the observed sample mean is 10 standard deviations away from the hypothesized value under the null hypothesis. According to intuition, observing the sample mean is such a rare event since it is so far away from the hypothesized value, but how rare is it? 10\% likely? 1\% likely? 0.001\%?

The \term{p-value} is the probability of observing the sample or a more extreme sample assuming the null hypothesis is true by chance. The p-value is the number that lets researchers make a conclusion about the T-statistic. Formally the p-value is a conditional probability.

\begin{termBox}{\tBoxTitle{p-value}
The \term{p-value}\index{hypothesis testing!p-value|textbf} is the probability of observing data at least as favorable to the alternative hypothesis as the current data set, if the null hypothesis is true. A summary statistic of the data is typically used to help compute the p-value and evaluate the hypotheses.}
\end{termBox}

How is this probability calculated? Section ~\ref{pValue} will go into more detail from using $R$, and the Z and t- tables. 

\subsubsection{Step 5: Making your conclusion}
The final step within the hypothesis testing framework is to make a conclusion from the p-value calculated in Step 4. Using the definition of p-value, if something extreme is observed, the probability associated with this observation will be small. Thus if the observation is rare, the T-statistic will be large and the p-value will provide evidence that the hypothesized value is unlikely. Therefore a low p-value should result in a rejection of the null hypothesis. The smaller the p-value, the stronger the evidence against the null hypothesis. 

How small is small? This is where Step 2 and the significance level, $\alpha$, come in. If the p-value is small or smaller than the pre-specified $\alpha$ level (usually 0.01 or 0.05),  the null hypothesis is rejected and conclusion that the result observed from the sample is statistically significant at the $\alpha$ level. 

If the p-value is $\alpha$ or greater, there is not enough evidence to reject the null hypothesis. The subtle but important point is that not rejecting $H_0$ is not equivalent to accepting $H_0$ (refer back to Example \ref{hypTestStudyExample}). In practice, however, not rejecting $H_0$ is equivalent to accepting $H_0$ when making decisions and acting on these conclusions. Most importantly, it is key that researchers state the conclusion in the context of the original problem, using the language and units of that problem. Most people forget this but is absolutely necessary in both theory and practice. 


\subsection{Calculating p-values}
\label{pValue}

\index{hypothesis testing!p-value|(}

Calculating p-values can be the most difficult part of the hypothesis testing framework. The p-value depends on many moving parts, including the T-statistic, the sample size and the alternative hypothesis but always remember, if the p-value is small (based on the $\alpha$ threshold), then the sample indicates that something rare just occurred, so rare that the null hypothesis should be rejected as true. Figure~\ref{pValueOneSidedSleepStudyExplained} shows the distribution of the sample mean where the p-value is the shaded area for a one sided alternative $\mu > \mu_0$. 

\begin{figure}[ht]
   \centering
   \includegraphics[width=0.9\textwidth]{ch_inference_foundations_oi_biostat/figures/pValueOneSidedSleepStudyExplained/pValueOneSidedSleepStudyExplained}
   \caption{To identify the p-value, the distribution of the sample mean is considered as if the null hypothesis was true. In this graph, the p-value is defined as the probability of observing the $\bar{x}$ or a sample mean even more extreme. The p-value implies whether or not it is favorable to follow $H_A$ under this distribution.} 
   \label{pValueOneSidedSleepStudyExplained}
\end{figure}

If the alternative hypothesis is one sided and has the form $\mu > \mu_0$, then the p-value would be represented by the upper tail (Figure~\ref{pValueOneSidedSleepStudyExplained}). If the alternative is one sided but has the form $\mu < \mu_0$, then the p-value would be the shaded area in the lower tail, left of the observed $\bar{x}$. In a two-sided test, \emph{two tails are shaded} since evidence in either direction is favorable to $H_A$ (Figure ~\ref{2ndSchSleepHTExample}). 

\begin{figure}
   \centering
   \includegraphics[width=0.9\textwidth]{ch_inference_foundations_oi_biostat/figures/2ndSchSleepHTExample/2ndSchSleepHTExample}
   \caption{$H_A$ is two-sided, so \emph{both} tails must be counted for the p-value.}
   \label{2ndSchSleepHTExample}
\end{figure}

After knowing the intuition behind the p-value, how do scientists turn the shaded area into a probability? Here is where the T-statistic comes into play. First, consider the \data{BRFSS BMI} example once again.

Recall that the CDC researchers are interested if US adults are maintaining a healthy weight and have the following null and alternative hypotheses where $\mu_{\mathrm{bmi}}$ denotes the average BMI of a US adult:

 \begin{itemize}
\setlength{\itemsep}{0mm}
\item[$H_0$:] $\mu_{\mathrm{bmi}}=22$
\item[$H_A$:] $\mu_{\mathrm{bmi}} \neq 22$ 
\end{itemize}

Instead of 40,000 within the \data{BRFSS BMI} sample, let's consider a sample of 50 people for simplicity with a sample mean of 24. The sample standard deviation is 5. Given this information the T-statistic is calculated as\footnote{calculating the T-statistic using actual data is an exercise in the book}. \[T=\frac{24-22}{5/\sqrt{50}}= 2.83\] The T-statistic can be thought of as a Z-score (standard score) that indicates how many standard deviations the observed sample mean is from the null value. This standardization becomes a great way to unify all the moving parts in order to calculate the p-value. 

Within the Hypothesis testing framework, Step 3 resulted in a T-statistic of 2.83. Step 4 begins by first knowing how the T-statistic is distributed. Section ~\ref{hypothesisFramework} stated that in general, the T-statistic follows a $t$-distribution with $n-1$ degrees of freedom. In fact, the T-statistic can either follow a $t$-distribution or a normal distribution. The sample size determines which distribution to model. Similarly to confidence intervals, if $n \geq 30$, the T-statistic can be thought of coming from a normal distribution. If $n < 30$, use a $t$-distribution for the T-statistic instead.

The p-value also depends on the significance level chosen in Step 2. From this, researchers can either use a Z or t-table to calculate the p-value or in most cases, use $R$. Assuming $\alpha = 0.05$ and the researcher is using a t-table like the one found on \textbf{SOME PAGE}, the researcher first finds the row with the correct degrees of freedom (for a one sample test, $df = n-1$). Then, looking across that same row, the researcher finds the T-statistic value that is closest to the one calculated in Step 3. Note that the table will not have every single T-statistic value listed. Once an approximate T-statistic is found, the p-value from either a one sided or two sided (one tail or two tail) alternative is found at the top of the cell's column.
 
The normal table (Z-table) is very similar but be wary that the Z-table only lists the areas left of the Z-score. This simply means that these probabilities coincide with a one-sided alternative. But because the normal distribution is symmetric, finding the p-value for a two sided alternative is just the values from the table times two! 

If available, $R$ is also a handy tool. Use the \var{pt()} or the \var{pnorm()} function to calculate the area left of the T-statistic. However be careful that the area left of the T-statistic is not necessarily always the p-value. Refer to Figure ~\ref{pValueOneSidedSleepStudyExplained} as guidance \footnote{While Figure ~\ref{pValueOneSidedSleepStudyExplained} is the distribution of the sample mean, the T-statistic also has a very similar distribution. Recall that the T-statistic is just the sample mean normalized and thus, the T-statistic distribution is centered at 0 (where the null value was). The observed $\bar{x}$ is instead the T-statistic. Therefore the shaded area remains the same whether researchers use Figure ~\ref{pValueOneSidedSleepStudyExplained} or the distribution of the T-statistic.} for when the complement of the $R$ output needs to be used. If students have the ability to use $R$, the $n\geq 30$ threshold does not need to be enforced since using a $t$-distribution in $R$ becomes as easy and more accurate than following a normal distribution. The $n\geq 30$ threshold, however, should be used with the Z or t-tables. However once $n\geq 30$, both distributions become almost equal. More evidence to this will appear in Section ~\ref{INTRO TO T} \textbf{INCLUDE LINK LATER}.  

Returning back to the example, $n=50$ and so the T-statistic of 2.83 will be modeled after a normal distribution. 
Using a normal table the calculate the p-value for a T-statistic of 2.83 corresponds to a shaded area of 0.9977. Therefore in the two-sided tail:
\begin{align*}
p &=Pr(T\leq -2.83) + Pr(T\geq 2.83)\\
&= Pr(|T| \geq 2.83)\\
&= 2Pr(T\geq 2.83)\\
&= 2 \cdot (1-0.9977)\\
&= 0.0046
\end{align*}
Using $R$, \var{pnorm()} and \var{pt()} are both used to check. \begin{verbatim}
> 2*(1-pnorm(2.83))
[1] 0.0046548
> 2*(1-pt(2.83, 49))
[1] 0.006730122
\end{verbatim}

Both output p-values from $R$ are extremely similar and agree with the p-value from the normal table. Step 4 resulted in a p-value of approximately 0.004. Step 5 is formulating the conclusion with the p-value and a significance level of $\alpha=0.05$. Because this p-value $< \alpha=0.05$, $H_0$ can be rejected. To put it into context: from observing a sample mean of 24 for the average BMI in the US, a p-value of 0.0046 was observed. Therefore the average BMI from the observed sample suggests that the population average BMI is not 22. 

\begin{termBox}{\tBoxTitle{p-value as a tool in hypothesis testing}
The p-value quantifies how strongly the data favor $H_A$ over $H_0$. A small p-value (usually $<0.05$) corresponds to sufficient evidence to reject $H_0$ in favor of $H_A$.}
\index{hypothesis testing!p-value|)}
\end{termBox}

\begin{exercise}
If the null hypothesis is true, how often should the p-value be less than 0.05?\footnote{About 5\% of the time. If the null hypothesis is true, then the data only has a 5\% chance of being in the 5\% of data most favorable to $H_A$.}
\index{data!school sleep|)}
\end{exercise}

\begin{tipBox}{\tipBoxTitle{Concluding on Critical Values}
Conclusions are made from the p-value and the significance level but if $\alpha=0.05$ or some other common value, the critical value can offer a quick shortcut to the conclusion. Recall that critical value are the coefficient on the standard error to calculate the confidence interval. However the critical value is also the point on the normalized sampling distribution that can be compared to the T-statistic for hypothesis testing. If the absolute value of the T-statistic is greater than the critical value (more extreme), the p-value is less than $\alpha$ and the null hypothesis can be rejected.}
\end{tipBox}

\begin{caution}{Critical value $\neq$ test statistic}
{Many times people get confused between the critical value and the test statistic. The critical value is associated with some $\alpha$ and does not change. For a specific $\alpha$, there is only one critical value. The T-statistic can change depending on the sample that is observed. The T-statistic should be compared to the critical value using the critical value as a benchmark.} 
\end{caution}

\subsubsection{One sided Sleep Example}
\label{onesidedSleepExample}

Section ~\ref{pValue} provided an introductory example with a two-sided alternative. This section will walk through all 5 steps in the Hypothesis testing framework for a problem about sleep.

\begin{exercise} \label{skepticalPerspOfRuralSchoolSleepExercise}
A poll by the National Sleep Foundation found that college students average about 7 hours of sleep per night. Researchers at a rural school are interested in showing that students at their school sleep longer than seven hours on average, and they would like to demonstrate this using a sample of students. What would be an appropriate skeptical position for this research?\footnote{A skeptic would have no reason to believe that sleep patterns at this school are any different than the sleep patterns at another school.}
\end{exercise}

\index{data!school sleep|(}

Hypothesis testing begins with setting up the hypotheses. The null hypothesis for this test becomes a skeptical perspective: the students at this school average 7 hours of sleep per night. The alternative hypothesis takes a new form reflecting the interests of the research: the students average more than 7 hours of sleep. The hypotheses in mathematical terms are \[H_0: \mu = 7 \hspace{0.5in} H_A: \mu \geq 7\]
Using $\mu \geq 7$ as the alternative is an example of a \term{one-sided} hypothesis test introduced previously. In this investigation, there is no apparent interest in learning whether the average hours of sleep in this rural college is less than 7~hours.\footnote{This is entirely based on the interests of the researchers. Had they been only interested in the opposite case -- showing that their students were actually averaging fewer than seven hours of sleep but not interested in showing more than 7 hours -- then the setup would have set the alternative as $\mu \leq 7$.} A \term{two-sided} hypothesis where any clear difference, greater than or less than the null value would have been interesting is not the scope of this hypothesis test..

Note: Always use a two-sided test unless it was made clear prior to data collection that the test should be one-sided. Switching a two-sided test to a one-sided test after observing the data is dangerous because it can inflate the chance of an incorrect conclusion. Section ~\ref{twoSidedTestsWithPValue} explores the consequences of switching between different alternative hypotheses. 

\begin{tipBox}{\tipBoxTitle{One-sided and two-sided tests}
If the researchers are only interested in showing an increase or a decrease, but not both, use a one-sided test. If the researchers would be interested in any difference from the null value -- an increase or decrease -- then the test should be two-sided.\vspace{0.5mm}}
\end{tipBox}

\begin{tipBox}{\tipBoxTitle{Always write the null hypothesis as an equality}
Writing the null hypothesis as an equality (e.g. $\mu = 7$) makes hypothesis testing easier. The alternative then should either be written with an unequal or inequality sign (e.g. $\mu\neq7$, $\mu \geq7$, or $\mu \leq7$).}
\end{tipBox}

The researchers at the rural school conducted a simple random sample of $n=110$ students on campus. They found that these students averaged 7.42 hours of sleep and the standard deviation of the amount of sleep for the students was 1.75 hours. A histogram of the sample is shown in Figure~\ref{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours}.

\begin{figure}
\centering
\includegraphics[width = \textwidth]{ch_inference_foundations_oi_biostat/figures/histOfSleepForCollegeThatWasCheckingForMoreThan7Hours/histOfSleepForCollegeThatWasCheckingForMoreThan7Hours}
\caption{Distribution of a night of sleep for 110 college students. These data are moderately skewed.\index{skew!example: moderate}}
\label{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours}
\end{figure}

Before moving onto Step 2, conditions for normality must be verified. (1)~Because this is a simple random sample from less than 10\% of the student body, the observations can be considered independent. (2)~The sample size in the sleep study is sufficiently large since it is greater than 30. (3)~The data show moderate skew in Figure~\ref{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours} and the presence of a couple of outliers. This skew and the outliers (which are not too extreme) are acceptable for a sample size of $n=110$. With these conditions verified, the normal model can be safely applied to $\bar{\mathrm{sleep}}$ and the estimated standard error will be very accurate.

\begin{exercise} \label{findSEOfFirstSleepStudyCheckingGreaterThan7Hours}
What is the standard deviation associated with $\bar{\mathrm{sleep}}$? That is, estimate the standard error of $\bar{\mathrm{sleep}}$.\footnote{The standard error can be estimated from the sample standard deviation and the sample size: $SE_{\bar{x}} = \frac{s_\mathrm{sleep}}{\sqrt{n}} = \frac{1.75}{\sqrt{110}} = 0.17$.}
\end{exercise}

The hypothesis test will be evaluated using a significance level of $\alpha = 0.05$. The sampling distribution is assuming that the null hypothesis is true. In this case, the sample mean, 7.42 hours, was drawn from a distribution that is nearly normal with mean 7 and standard deviation of about 0.17. Such a distribution is shown in Figure~\ref{pValueOneSidedSleepStudy}. 

\begin{figure}[hht]
   \centering
   \includegraphics[width=0.73\textwidth]{ch_inference_foundations_oi_biostat/figures/pValueOneSidedSleepStudy/pValueOneSidedSleepStudy}
   \caption{If the null hypothesis is true, then the sample mean $\bar{\mathrm{sleep}}$ is from this nearly normal distribution. The right tail describes the probability of observing such a large sample mean if the null hypothesis is true. The p-value is the area that is shaded blue that is more extreme than $\bar{\mathrm{sleep}}$. All means larger than the sample mean are more favorable to the alternative hypothesis than the observed mean.}
   \label{pValueOneSidedSleepStudy}
\end{figure}

Step 3 is the calculation of the T-statistic for sample mean $\bar{\mathrm{sleep}} = 7.42$. 
\[ T = \frac{\bar{\mathrm{sleep}} - \text{null value}}{SE_{\bar{\mathrm{sleep}}}} = \frac{7.42 - 7}{0.17} = 2.47\]

Step 4 continues with a T-statistic of 2.47. As noted before, $n=110$ so the T-statistic can be considered to be distributed normally. Using the normal probability table, the lower unshaded area is found to be 0.993. Thus the shaded area from Figure ~\ref{pValueOneSidedSleepStudy} is $1-0.993 = 0.007$. Using $R$ to confirm,
 \begin{verbatim} 
 > 1-pnorm(2.47)
[1] 0.006755653 
\end{verbatim}
{\em If the null hypothesis is true, the probability of observing such a large sample mean of 7.42 hours for a sample of 110 students is only 0.007.}\index{p-value!interpretation example} That is, if the null hypothesis is true, such a large mean would not be observed in most instances. 

Step 5 compares the p-value to the significance level. Because the p-value is less than the significance level $\alpha$($0.007 < 0.05$), the null hypothesis is rejected.\footnote{Using critical values instead, for $\alpha=0.05$ and a one sided alternative, the critical value is 1.65. Since the T-statistic is greater than 1.65, $H_0$ is rejected without calculating the actual p-value} The researchers observed was so unusual with respect to the null hypothesis that it casts serious doubt on $H_0$ and provides strong evidence favoring $H_A$. Therefore in context, it is most likely that these rural college students do sleep more than 7 hours per night, more than the national average among college students. 

\begin{tipBox}{\tipBoxTitle{It is useful to first draw a picture to find the p-value}
It is useful to draw a picture of the distribution of $\bar{x}$ as though $H_0$ was true (i.e. $\mu$ equals the null value), and shade the region (or regions) of sample means that are at least as favorable to the alternative hypothesis. The sum of the shaded regions represent the p-value.}
\end{tipBox}

\begin{exercise}
Suppose the significance level was instead 0.01 in the sleep study. Would the evidence have been strong enough to reject the null hypothesis? (The p-value was 0.007.) What if the significance level was $\alpha = 0.001$? \footnote{The null hypothesis is rejected whenever p-value $< \alpha$. If $\alpha = 0.01$, the null hypothesis would still be rejected. If $\alpha = 0.001$, a p-value of 0.007 would fail to reject the null hypothesis.}
\end{exercise}

\subsection{Testing hypotheses using confidence intervals}
\label{utilizingOurCI}

While confidence intervals and hypothesis testing may seem disjointed -- one provides a range of values while the other results in a yes or no conclusion -- these two methods are two sides of the same coin and arrive at the same conclusions. 
	
Consider the CDC  samples 100 people from \data{BRFSS} to test if the average age of all US adults is 35.3 years \footnote{The 2000 Census \url{https://www.census.gov/prod/2001pubs/c2kbr01-12.pdf} listed the median age instead of mean age as 35.3 years. The Census also does not suggest age is distributed normally, but the difference in mean and median age will only be slight as the US population is so large.} The sample has an average age of 43.9 years and a sample standard deviation of 17.97 years.

Beginning with hypothesis testing, the two hypotheses would be $H_0: \mu_{\mathrm{age}}=35.3$ and $H_A: \mu_{\mathrm{age}} \neq 35.3$. The alternative hypothesis is two-sided because the CDC has no prior belief on the sidedness of this question. 

Continuing on, the CDC performs the only steps within the Hypothesis testing framework. Let $\alpha = 0.05$ and the average the T-statistic is \[t = \frac{43.9-35.3}{17.97/\sqrt{100}}= \frac{8.6}{1.797} = 4.79\] Because $n=100$ which is larger than 30, the T-statistic can be modeled after a normal distribution. Using $R$, the p-value is \begin{verbatim}
> 2*(1-pnorm(4.79))
[1] 1.667813e-06
\end{verbatim}
The p-value calculated for a two sided alternative is less than the $\alpha$ level of 0.05. Therefore the sample mean that the CDC observed can be considered a "rare event," and the null hypothesis can be rejected. To put it in context, the CDC calculated a T-statistic of 4.79 and can reject the hypothesis that the average age of all US adults is 35.3 years after observing a sample mean of 43.9 years. In other words, the average age of adults is not 35.3 years. 

Remembering the conclusion of hypothesis testing, the CDC then calculates a confidence interval as well. Because $\alpha=0.05$ under hypothesis testing, the CDC chooses to create a 95\% confidence interval. Using the same sample the confidence interval is
\begin{align*}
43.9 &\pm 1.96 * \frac{17.97}{\sqrt{100}}\\
43.9 &\pm 1.96 * 1.797\\
(40.38,& 47.42)\
\end{align*} 

The CDC is 95\% confident that the average age of all the US adults lies between 40.38 and 47.42 years after observing a sample mean of 43.9 years. The CDC can take it a step further and note that 35.3 years is not included in this interval. Therefore it is equivalent to say that the average age of the US is not 35.3 years. 

The connection between confidence intervals and hypothesis testing is this: rejecting a null hypothesis for a two-sided alternative hypothesis corresponds to the null value that does not fall within a confidence interval. The equivalence only is true if assumptions that the alternative hypothesis is two-sided and the significance levels for both are equal are both met. 

\begin{exercise} \label{htForHousingExpenseForCommunityCollege650} 
An investigator is studying the results of standardized IQ tests in adolescents who suffered from severe asthma during childhood. She claims that those who had childhood asthma perform worse. For the standardized test she will use, the population mean score is 100. What are the null and alternative hypotheses to test whether this claim is accurate? \footnote{$H_0$: The average score is 100, $\mu = 100$. \hspace{3.4mm} $H_A$: The average score is lower than 100, $\mu \leq 100$.}
\end{exercise}

\begin{example}{In her sample of 100 children, she found a sample mean $\bar{x} = 96.7$ and standard deviation $s = 10$. Construct a 95\% confidence interval for the population mean and evaluate the hypotheses of Exercise~\ref{htForHousingExpenseForCommunityCollege650}.}
$$ SE = \frac{s}{\sqrt{n}} = \frac{10}{\sqrt{100}} = 1 $$
The normal model may be applied to the sample mean because the conditions are met: The data are a simple random sample and we assume that there are more than 1,000 adolescents who have suffered from asthma. The observations are independent and the sample size is also sufficiently large (n=100). The sample size mitigates potential effects of outliers even though the distribution is not provided. This ensures a 95\% confidence interval may be accurately constructed:
$$\bar{x}\ \pm\ z^{\star} SE \quad\to\quad 96.7\ \pm\ 1.96 \times  1 \quad \to \quad (94.74, 98.66) $$
Because the null value 100 does not lie within the confidence interval, a population mean score of 100 from all adolescents who suffered from sever asthma during childhood is implausible, and the null hypothesis is rejected. The data provide statistically significant evidence that adolescents who suffered from severe asthma during childhood do perform worse on standardized IQ tests. 
\end{example}

\subsection{Decision errors}\label{DecisionErrors}

\index{hypothesis testing!decision errors|(}

Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, scientists can make wrong decisions in statistical hypothesis tests in the presence of sampling variation. With sample means and standard deviations varying sample to sample, confidence intervals and p-values can differ. This variation induces some imprecision and the potential to make the wrong decision. However, the difference is that, unlike the court system, there exists tools necessary to quantify how often such errors are made.

There are two competing hypotheses: the null and the alternative. Hypothesis testing results in scientists concluding which hypothesis might be true. However, these scientists might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized in Table~\ref{fourHTScenarios}.

\begin{table}[ht]
\centering
\begin{tabular}{l l c c}
& & \multicolumn{2}{c}{\textbf{Test conclusion}} \\
  \cline{3-4}
\vspace{-3.7mm} \\
& & do not reject $H_0$ &  reject $H_0$ in favor of $H_A$ \\
  \cline{2-4}
\vspace{-3.7mm} \\
& $H_0$ true & okay &  Type~1 Error \\
\raisebox{1.5ex}{\textbf{Truth}} & $H_A$ true & Type 2 Error & okay \\
  \cline{2-4}
\end{tabular}
\caption{Four different scenarios for hypothesis tests.}
\label{fourHTScenarios}
\end{table}

A \term{Type~1 Error} is rejecting the null hypothesis when $H_0$ is actually true. A \term{Type~2 Error} is failing to reject the null hypothesis when the alternative is actually true.

\begin{exercise} \label{whatAreTheErrorTypesInUSCourts}
In a US court, the defendant is either innocent ($H_0$) or  guilty ($H_A$). What does a Type~1 Error represent in this context? What does a Type 2 Error represent? Table~\ref{fourHTScenarios} may be useful.\footnote{If the court makes a Type~1 Error, this means the defendant is innocent ($H_0$ true) but wrongly convicted. A Type 2 Error means the court failed to reject $H_0$ (i.e. failed to convict the person) when the defendant was in fact guilty ($H_A$ true).}
\end{exercise}

\begin{exercise} \label{howToReduceType1ErrorsInUSCourts}
How could we reduce the Type~1 Error rate in US courts? What influence would this have on the Type 2 Error rate?\footnote{To lower the Type~1 Error rate (incorrect convictions), the standard conviction could be raised from ``beyond a reasonable doubt'' to ``beyond a conceivable doubt'' so fewer people would be wrongly convicted. However, there exists a tradeoff. By increasing the threshold, it would make it more difficult to convince people who are guilty. By reducing the Type~1 Error rate, the US courts would increase the Type 2 Error rate.}
\end{exercise}

\begin{exercise} \label{howToReduceType2ErrorsInUSCourts}
How could Type~2 Error rate be reduced in US courts? What influence would this have on the Type~1 Error rate?\footnote{Lowering the Type~2 Error rate means that the US courts will convict more guilty people. Instead of Exercise \label{howToReduceType1ErrorsInUSCourts} and raising the standard of conviction, the standard could be lowered to reduce Type~2 Error. Lowering the bar from ``beyond a reasonable doubt'' to ``beyond a little doubt'' for guilt will also result in more wrongful convictions, raising the Type~1 Error rate.}
\end{exercise}

\begin{exercise} \label{errorsinHIVTesting}
Consider a person getting tested for HIV. What do a Type~1 and Type~2 Error represent in this context? \footnote{Type~1 Error is if this person does not have HIV but was tested positive for HIV. Type~2 Error would be failing to detect HIV when the patient actually has HIV. }
\end{exercise}

\index{hypothesis testing!decision errors|)}

Exercises~\ref{whatAreTheErrorTypesInUSCourts}-\ref{howToReduceType2ErrorsInUSCourts} provide an important lesson: if one type of error is reduced, the other error type is generally increased. 

Hypothesis testing can occur in all industries ranging from manufacturing to medical testing, and the question of which error type to target remains in all industries. The real world application of the hypotheses will determine if reducing Type 1 Error is more desirable than reducing Type 2 Error. 


Hypothesis testing is built around rejecting or failing to reject the null hypothesis from strong evidence. But what precisely does \emph{strong evidence} mean? Section ~ref{confidenceLevels} first introduced the relationship between $\alpha$ and Type 1 Error. Scientists, in practice, will use $\alpha = 0.05$ universally. This value for $\alpha$, the \term{significance level}, \index{hypothesis testing!significance level}  \marginpar[\raggedright\vspace{-4mm}

$\alpha$\\\footnotesize significance\\level of a\\hypothesis test]{\raggedright\vspace{-4mm}

$\alpha$\\\footnotesize significance\\level of a\\hypothesis test} is interpreted for those cases where the null hypothesis is actually true, scientists do not want to incorrectly reject $H_0$ more than 5\% of the time. Scientists do not want to commit a Type 1 Error more than 5\% of the time. Different significance levels beyond $\alpha = 0.05$ will be discussed in Section~\ref{significanceLevel}. However the significance level chosen depends on the study's threshold for committing a Type 1 Error and allowing a Type 2 Error. 

\subsection{Two-sided versus One-sided hypothesis testing: Dos and Don'ts}
\label{twoSidedTestsWithPValues}

\index{data!school sleep|(}

Students and scientists alike have the urge to reject the null hypothesis. Scientists, after spending so much time and resources, aim to be published with a new discovery and make an important contribution to the field. Rejecting the null hypothesis, because it is generally stated as the status quo, achieves this scientific contribution. 

However, determining an alternative hypothesis can get tricky, and the choice between a one-sided and two sided test can be controversial. Most importantly, it is never okay to change two-sided tests to one-sided tests after observing the data. The following exercise considers the consequences of switching mid-process.

\begin{exercise} \label{2ndSchSleepHypSetupExercise}
The exists a second group of researchers who want to evaluate whether the students at their college differ from the norm of 7 hours. Write the null and alternative hypotheses for this investigation.\footnote{Because the researchers are interested in any difference, they should use a two-sided setup: $H_0: \mu = 7$, $H_A: \mu \neq 7$.}
\end{exercise}

\begin{example}{The second college randomly samples 72 students and finds a mean of $\bar{x} = 6.61$ hours and a standard deviation of $s=1.8$ hours. Does this provide strong evidence against $H_0$ in Exercise~\ref{2ndSchSleepHypSetupExercise}? Use a significance level of $\alpha=0.05$.}
First, assumptions must be verified. (1) A simple random sample of less than 10\% of the student body means the observations are independent. (2) The sample size is 72, which is greater than 30. (3) Based on the earlier distribution and what is already known about college student sleep habits, the distribution is likely not to be strongly~skewed.

Next compute the standard error ($SE_{\bar{x}} = \frac{s}{\sqrt{n}} = 0.21$) of the estimate and create a picture to represent the p-value, shown in Figure~\ref{2ndSchSleepHTExample}. Both tails are shaded since the alternative hypothesis provided in Exercise \ref{2ndSchSleepHypSetupExercise} is two-sided.

Calculate the tail areas by first finding the lower tail corresponding to $\bar{x}$:
\begin{eqnarray*}
T = \frac{6.61 - 7.00}{0.21} = -1.86 \quad\stackrel{\text{table or $R$}}{\rightarrow}\quad \text{left tail}=0.03
\end{eqnarray*}
Because the normal model is symmetric, the right tail will have the same area as the left tail. The p-value is found as the sum of the two shaded tails:
\begin{eqnarray*}
\text{p-value} = \text{left tail} + \text{right tail} = 2\times(\text{left tail}) = 0.06
\end{eqnarray*}
This p-value is is larger than $\alpha=0.05$, so $H_0$ should not be rejected. That is, if $H_0$ is true, it would not be very unusual to see a sample mean this far from 7 hours simply due to sampling variation. There is not have sufficient evidence to conclude that the mean is different than 7 hours.

However, consider if this second group of researchers switched to a one-sided alternative hypothesis before the T-statistic was calculated. Keeping the sample and all other values the same, the p-value under a one-sided alternative would be 0.03. This new p-value is less than $\alpha = 0.05$ and would result in $H_0$ being rejected, a different conclusion than with the two-sided alternative. 
\index{data!school sleep|)}
\end{example}

Exercise ~\ref{2ndSchSleepHypSetupExercise} introduces the dangers of switching alternative hypothesis after observing the data. The next example shows that freely switching from two-sided tests to one-sided tests will cause researchers to make twice as many Type~1 Errors as intended.

\begin{example}{Consider two cases: (1) The sample mean is larger than the null value and (2) the sample mean is smaller than the null value. 

(1) Suppose the sample mean was larger than the null value, $\mu_0$ (e.g. $\mu_0$ would represent~7 if $H_0$:~$\mu = 7$). Then the researchers could flipped to a one-sided test instead of a two-sided test, they would use $H_A$: $\mu > \mu_0$ after observing the data\footnote{They opt to minimize the Type~1 Errors}. Any T-statistic greater than 1.65 would result in rejecting $H_0$. If the null hypothesis is true, the large T-statistic would result in incorrectly rejecting the null hypothesis about 5\% of the time when the sample mean is above the null value, as shown in Figure~\ref{type1ErrorDoublingExampleFigure}.

(2) Suppose the sample mean was smaller than the null value. The researchers would choose $H_A$: $\mu < \mu_0$ if they could change to a one-sided test. If $\bar{x}$ had a T-statistic smaller than -1.65, $H_0$ would be rejected. If the null hypothesis is true, $H_0$ would be incorrectly rejected about 5\% of the time.}

Putting these two scenarios together, if the researchers were free to switch alternative hypotheses after observing the data, the Type~1 Error rate would be $5\%+5\%=10\%$ of the time instead of the specified $\alpha = 0.05$. This is twice the error rate we prescribed with our significance level of 0.05! 

\begin{figure}
   \centering
   \includegraphics[width=0.7\textwidth]{ch_inference_foundations_oi_biostat/figures/type1ErrorDoublingExampleFigure/type1ErrorDoublingExampleFigure}
   \caption{The shaded regions represent areas where we would reject $H_0$ under the bad practices considered in Example~\ref{swappingHypAfterDataDoublesType1ErrorRate} when $\alpha = 0.05$.}
   \label{type1ErrorDoublingExampleFigure}
\end{figure}

\end{example}

This text's examples and exercises will be obvious enough to decide on a correct alternative hypothesis. With real world data, however, it can be less straightforward. If the sidedness is uncertain, many scientists opt to use a two-sided alternative because it is more \emph{conservative}. What does conservative in this context mean? Exercise ~\ref{2ndSchSleepHypSetupExercise} and the previous example showed that the two-sided tests would fail to reject the null hypothesis more often. The p-value for a two-sided alternative is twice as large producing a "safer" and more conservative hypothesis test. 

\begin{caution}{One-sided hypotheses are allowed only \emph{before} seeing data}
{After observing data, it is tempting to turn a two-sided test into a one-sided test. Avoid this temptation. Remember, the direction of a one-sided test must be made a priori, not after peeking at the data since the results could be statistically significant with a one-sided test, but not significant with a two-sided test. Hypotheses must be set up \emph{before} observing the data. If~they are not, the test must be two-sided.}
\end{caution}


\subsection{Choosing a significance level}
\label{significanceLevel}

\index{hypothesis testing!significance level|(}
\index{significance level|(}

Choosing a significance level for a test is important in many contexts, and the traditional level is $\alpha=0.05$. However, it is often helpful to adjust the significance level based on the application and on the consequences of any conclusions reached from the test.

If making a Type~1 Error is dangerous or especially costly, a small significance level (e.g. smaller than 0.05) should be chosen. The study should demand very strong evidence favoring $H_A$ before $H_0$ is rejected. Many would use $\alpha=0.01$ in this situation. 

If a Type 2 Error is relatively more dangerous or much more costly than a Type~1 Error, a higher significance level (e.g. 0.10) should be chosen. Here hypothesis tests should be cautious about failing to reject $H_0$ when the null is actually false.  Section~\ref{sampleSizeAndPower} will discuss this case in more detail.

\begin{tipBox}{\tipBoxTitle[]{Significance levels should reflect consequences of errors}
The significance level selected for a test should reflect the consequences associated with Type~1 and Type 2 Errors.}
\end{tipBox}

\begin{example}{A medical machine manufacturer is considering a higher quality but more expensive supplier for parts in making an MRI. They sample a number of parts from their current supplier and also parts from the new supplier. They decide that if the high quality parts will last more than 12\% longer, it makes financial sense to switch to this more expensive supplier. Is there good reason to modify the significance level in such a hypothesis test?}
The null hypothesis is that the more expensive parts last no more than 12\% longer while the alternative is that they do last more than 12\% longer. This decision is just one of the many regular factors that have a marginal impact on the MRI and the company financial health. A significance level of 0.05 seems reasonable since neither a Type~1 or Type 2 error should be dangerous or (relatively) much more expensive since the machine's accuracy will be less affected.
\end{example}

\begin{example}{Now consider that the same MRI manufacturer is considering a slightly more expensive supplier for parts related to safety not longevity. If the durability of the machine's components is shown to be better than the current supplier, they will switch manufacturers. Is there good reason to modify the significance level in such an evaluation?}
The null hypothesis would be that the suppliers' parts are equally reliable and equally accurate in detection. Because safety is involved, the MRI machine company should be eager to switch to the slightly more expensive manufacturer (reject $H_0$) even if the evidence of increased safety and effectiveness is only moderately strong. A slightly larger significance level, such as $\alpha=0.10$, might be appropriate.
\end{example}

\begin{exercise}
A part inside of a machine is very expensive to replace. However, the machine usually functions properly even if this part is broken. The machine still detects the most common injuries at the same level with a broken part. The part is replaced only if the doctors are extremely certain it is broken based on a series of measurements. Identify appropriate hypotheses for this test (in plain language) and suggest an appropriate significance level.\footnote{Here the null hypothesis is that the part is not broken, and the alternative is that it is broken. If there is not sufficient evidence to reject $H_0$, the part will not be replaced. Failing to fix the part if it is broken ($H_0$ false, $H_A$ true) is not very problematic, and replacing the part is expensive. Thus, very strong evidence against $H_0$ should exist before we replace the part. Choose a small significance level, such as $\alpha=0.01$.}
\end{exercise}

\index{significance level|)}
\index{hypothesis testing!significance level|)}
\index{hypothesis testing|)}

%__________________

\section{A Primer on the $t$-distribution}
\label{tdistribution}

Section ~\ref{hypothesisTesting} examined the T-statistic as distributed according to a $t$-distribution. Section ~\ref{pValue} offered a rule of thumb that for $n>30$, the p-value of a $t$-distribution becomes almost indistinguishable from the p-value of a normal distribution. However, throughout the entire section, the characteristics of a $t$-distribution were not explained. The properties of the $t$-distribution are not needed to perform hypothesis testing. However this section will introduce the $t$-distribution at a more theoretical level because the T-statistic follows a $t$-distribution. 

\subsection{Introducing the $t$-distribution}
\label{introducingTheTDistribution}

\index{t-distribution|(}
\index{distribution!$t$|(}

In the cases where a small sample is used to calculate the standard error or a T-statistic is calculated, it will be useful to rely on a new distribution for inference calculations: the $t$-distribution. A $t$-distribution, shown as a solid line in Figure~\ref{tDistCompareToNormalDist}, has a bell shape. However, its tails are thicker than the normal model's. This means observations are more likely to fall beyond two standard deviations from the mean than under the normal distribution.\footnote{The standard deviation of the $t$-distribution is actually a little more than 1. However, it is useful to always think of the $t$-distribution as having a standard deviation of 1 in all of this text's applications.} While the estimate of the standard error will be a little less accurate when a small data set is analyzed, these extra thick tails of the $t$-distribution are exactly the correction needed to resolve the problem of a poorly estimated standard error.

\begin{figure}
\centering
\includegraphics[width = \textwidth]{ch_inference_foundations_oi_biostat/figures/tDistCompareToNormalDist/tDistCompareToNormalDist}
\caption{Comparison of a $t$-distribution (solid line) to a normal distribution (dotted line).}
\label{tDistCompareToNormalDist}
\end{figure}

The $t$-distribution, always centered at zero, has a single parameter: degrees of freedom. The \termsub{degrees of freedom (df)}{degrees of freedom (df)!$t$-distribution} describe the precise form of the bell-shaped $t$-distribution. Several $t$-distributions are shown in Figure~\ref{tDistConvergeToNormalDist}. When there are more degrees of freedom, the $t$-distribution looks very much like the standard normal distribution. Within hypothesis testing from observing 1 sample, the degrees of freedom is equal to $n-1$ where $n$ is the sample size.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/tDistConvergeToNormalDist/tDistConvergeToNormalDist}
\caption{The larger the degrees of freedom, the more closely the $t$-distribution resembles the standard normal model.}
\label{tDistConvergeToNormalDist}
\end{figure}

\begin{termBox}{\tBoxTitle{Degrees of freedom (df)}
The degrees of freedom describe the shape of the $t$-distribution. The larger the degrees of freedom, the more closely the distribution approximates the normal model.}
\end{termBox}

When the degrees of freedom is about 30 or more, the $t$-distribution is nearly indistinguishable from the normal distribution. 

It's very useful to become familiar with the $t$-distribution, because it allows greater flexibility than the normal distribution when analyzing numerical data. The \term{t-table}, partially shown in Table~\ref{tTableSample}, is used in place of the normal probability table. A larger $t$-table is in Appendix~\ref{tDistributionTable} on page~\pageref{tDistributionTable}. In~practice, it's more common to use statistical software like $R$ instead of a table. 
\begin{table}[hht]
\centering
\begin{tabular}{r | rrr rr}
one tail & \hspace{1.5mm}  0.100 & \hspace{1.5mm} 0.050 & \hspace{1.5mm} 0.025 & \hspace{1.5mm} 0.010 & \hspace{1.5mm} 0.005  \\
two tails & 0.200 & 0.100 & 0.050 & 0.020 & 0.010 \\
\hline
{$df$} \hfill 1  &  {\normalsize  3.08} & {\normalsize  6.31} & {\normalsize 12.71} & {\normalsize 31.82} & {\normalsize 63.66}  \\ 
2  &  {\normalsize  1.89} & {\normalsize  2.92} & {\normalsize  4.30} & {\normalsize  6.96} & {\normalsize  9.92}  \\ 
3  &  {\normalsize  1.64} & {\normalsize  2.35} & {\normalsize  3.18} & {\normalsize  4.54} & {\normalsize  5.84}  \\ 
$\vdots$ & $\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ & \\
17  &  {\normalsize  1.33} & {\normalsize  1.74} & {\normalsize  2.11} & {\normalsize  2.57} & {\normalsize  2.90}  \\ 
\highlightO{18}  &  \highlightO{\normalsize  1.33} & \highlightO{\normalsize  1.73} & \highlightO{\normalsize  2.10} & \highlightO{\normalsize  2.55} & \highlightO{\normalsize  2.88}  \\ 
19  &  {\normalsize  1.33} & {\normalsize  1.73} & {\normalsize  2.09} & {\normalsize  2.54} & {\normalsize  2.86}  \\ 
20  &  {\normalsize  1.33} & {\normalsize  1.72} & {\normalsize  2.09} & {\normalsize  2.53} & {\normalsize  2.85}  \\ 
$\vdots$ & $\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ & \\
400  &  {\normalsize  1.28} & {\normalsize  1.65} & {\normalsize  1.97} & {\normalsize  2.34} & {\normalsize  2.59}  \\ 
500  &  {\normalsize  1.28} & {\normalsize  1.65} & {\normalsize  1.96} & {\normalsize  2.33} & {\normalsize  2.59}  \\ 
$\infty$  &  {\normalsize  1.28} & {\normalsize  1.64} & {\normalsize  1.96} & {\normalsize  2.33} & {\normalsize  2.58}  \\ 
\end{tabular}
\caption{An abbreviated look at the $t$-table. Each row represents a different $t$-distribution. The columns describe the cutoffs for specific tail areas. The row with $df=18$ has been \highlightO{highlighted}.}
\label{tTableSample}
\end{table}

Each row in the $t$-table represents a $t$-distribution with different degrees of freedom. The columns correspond to tail probabilities. For instance, if $df=18$, researchers can examine row 18, which is highlighted in Table~\ref{tTableSample}. If researchers instead want the value in this row that identifies the cutoff for an upper tail of 10\%, they can look in the column where \emph{one tail} is 0.100. This cutoff is 1.33. If they had wanted the cutoff for the lower 10\%, they would use -1.33. Just like the normal distribution, all $t$-distributions are symmetric.

\begin{example}{What proportion of the $t$-distribution with 18 degrees of freedom falls below -2.10?}
Just like a normal probability problem, Figure~\ref{tDistDF18LeftTail2Point10} provides a picture with the shaded area below -2.10. To find this area, the appropriate row needs to be identified in the $t$-table: \mbox{$df=18$}. 
Then identify the column containing the absolute value of -2.10; it~is the third column. The shaded area is only one tale, and the top line of the table shows that a one tail area for a value in the third row corresponds to 0.025. About 2.5\% of the distribution falls below -2.10. The next example encounters a case where the exact $t$ value is not listed in the table where $R$ would provide more use.
\end{example}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{ch_inference_foundations_oi_biostat/figures/tDistDF18LeftTail2Point10/tDistDF18LeftTail2Point10}
\caption{The $t$-distribution with 18 degrees of freedom. The area below -2.10 has been shaded.}
\label{tDistDF18LeftTail2Point10}
\end{figure}

\begin{example}{A $t$-distribution with 20 degrees of freedom is shown in the left panel of Figure~\ref{tDistDF20RightTail1Point65}. Estimate the proportion of the distribution falling above 1.65.}
Looking at the $t$-table again, identify the row using the degrees of freedom: $df=20$. Then look for 1.65; it is not listed. It falls between the first and second columns. Since these values bound 1.65, their tail areas will bound the tail area corresponding to 1.65. One tail area of the first and second columns -- 0.050 and 0.10 --concluded that between 5\% and 10\% of the distribution is more than 1.65 standard deviations above the mean. However this is only an estimation. Statistical software like $R$ can provide more precision. \begin{verbatim}
> 1-pt(1.65,20)
[1] 0.05728041
\end{verbatim}
The precise area that is shaded is 0.0573. 
\end{example}

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{ch_inference_foundations_oi_biostat/figures/tDistDF20RightTail1Point65/tDistDF20RightTail1Point65}
\caption{Left: The $t$-distribution with 20 degrees of freedom, with the area above 1.65 shaded. Right: The $t$-distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded.}
\label{tDistDF20RightTail1Point65}
\end{figure}

\begin{example}{A $t$-distribution with 2 degrees of freedom is shown in the right panel of Figure~\ref{tDistDF20RightTail1Point65}. Estimate the proportion of the distribution falling more than 3 units from the mean (above or below).}
As before, first identify the appropriate row: $df=2$. Next, find the columns that capture 3; because $2.92 < 3 < 4.30$, use the second and third columns. Finally, 0.05 and 0.1 are the bounds for the tail areas under "two tails" since the $t$-distribution is symmetric. For more precision, $R$ outputs 
\begin{verbatim} > 2*pt(-3, 2)
[1] 0.09546597
\end{verbatim}
\end{example}

\begin{exercise}
What proportion of the $t$-distribution with 19 degrees of freedom falls above -1.79 units?\footnote{Shade area \emph{above} -1.79 on a symmetric distribution (the picture is left to you). The small left tail is between 0.025 and 0.05, so the larger upper region must have an area between 0.95 and 0.975 (1 - the small left tail area). Using $R$ again, > 1-pt(-1.79, 19) [1] 0.9552978}

\index{distribution!$t$|)}
\index{t-distribution|)}

\end{exercise}

\subsection{Conditions for using the $t$-distribution for inference on a sample mean}
\label{tDistSolutionToSEProblem}

To proceed with the $t$-distribution for inference about a single mean, two conditions need to be checked. They will be introduced more formally here.
\begin{description}
\item[Independence of observations.] This condition was verified before in Section ~\ref{changingTheConfidenceLevelSection} as a condition that $\bar{x}$ is nearly normal. The sample should be taken from less than 10\% of the population as a simple random sample. If the data are from an experiment or random process, the observations should be independent. 
\item[Observations come from a nearly normal distribution.] This second condition is difficult to verify with small data sets. To diagnose (i) take a look at a plot of the data for obvious departures from the normal model, and (ii) consider any previous experiences in which the data may not be nearly normal. \footnote{A large enough sample size can sometimes counteract concerns with skewness and make this assumption more lenient.}
\end{description}
When examining a sample mean and estimated standard error from a sample of $n$ independent and nearly normal observations, a $t$-distribution with $n-1$ degrees of freedom~($df$) can be used. For example, if the sample size was 19, then the $t$-distribution would have $df=19-1=18$ degrees of freedom and proceed exactly as in the rest of Chapter~\ref{foundationsForInference}, except that \emph{now the $t$-distribution is being used}.

\begin{tipBox}{\tipBoxTitle{When to use the $t$-distribution}
Use the $t$-distribution for inference of the sample mean when observations are independent and nearly normal. The nearly normal condition is relaxed as the sample size increases. For example, the data distribution may be moderately skewed when the sample size is at least~30.}
\end{tipBox}

\section{Examining the Central Limit Theorem Closer (Special Topic)}
\label{cltSection}

\index{Central Limit Theorem|(}

Looking back to ~\ref{why30}, the normal model for the sample mean tends to be very good when the sample consists of at least 30 independent observations and the population data are not strongly skewed. The Central Limit Theorem provides the theory that allows this assumption to be made.

\begin{termBox}{\tBoxTitle{Central Limit Theorem, informal definition}
The distribution of $\bar{x}$ is approximately normal. The approximation can be poor if the sample size is small, but it improves with larger sample sizes. As $n$ increases, the sample mean becomes more and more normally distributed.}
\end{termBox}

The Central Limit Theorem states that when the sample size is small, the normal approximation may not be very good. However, as the sample size becomes large, the normal approximation improves. Three theoretical cases will be considered to see roughly when the approximation is reasonable.

Consider three data sets: one from a \emph{uniform} distribution, one from an \emph{exponential} distribution, and the other from a \emph{log-normal} distribution. Recall the properties of these distributions from Chapter ~\ref{modeling}. These distributions are shown in the top panels of Figure~\ref{cltSimulations}. The uniform distribution is symmetric, the exponential distribution may be considered as having moderate skew since its right tail is relatively short (few outliers), and the log-normal distribution is strongly skewed and will tend to produce more apparent outliers.\index{skew!example: moderate}\index{skew!example: strong}

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/cltSimulations/cltSimulations}
   \caption{Sampling distributions for the mean at different sample sizes and for three different distributions. The dashed red lines show normal distributions.}
   \label{cltSimulations}
\end{figure}

The left panel in the $n=2$ row represents the sampling distribution of $\bar{x}$ if it is the sample mean of two observations from the uniform distribution shown. The dashed line represents the closest approximation of the normal distribution. Similarly, the center and right panels of the $n=2$ row represent the respective distributions of $\bar{x}$ for data from exponential and log-normal distributions.

\begin{exercise}
Examine the distributions in each row of Figure~\ref{cltSimulations}. What do you notice about the normal approximation for each sampling distribution as the sample size becomes larger?\footnote{The normal approximation becomes better as larger samples are used.}
\end{exercise}

\begin{example}{Would the normal approximation be good in all applications where the sample size is at least 30?}
Not necessarily. For example, the normal approximation for the log-normal example is questionable for a sample size of 30. Generally, the more skewed a population distribution or the more common the frequency of outliers, the larger the sample required to guarantee the distribution of the sample mean is nearly normal.
\end{example}

\begin{tipBox}{\tipBoxTitle{With larger $n$, the sampling distribution of $\bar{x}$ becomes more normal}
As the sample size increases, the normal model for $\bar{x}$ becomes more reasonable. The condition in skewness can be relaxed when the sample size is very large.}
\end{tipBox}

Section~\ref{seOfTheMean} proposed that the sample standard deviation, $s$, could be used as a substitute of the population standard deviation, $\sigma$, when computing the standard error. This estimate tends to be reasonable when $n\geq30$. Alternative distributions for smaller sample sizes will be considered in Chapters~\ref{inferenceForNumericalData} and~\ref{inferenceForCategoricalData}.

\begin{example}{Figure~\ref{pertussisTS} displays a histogram of 169 observations from a study\footnote{Magpantay FMG, Domenech de Cells M, Rohani P, King AA (2015) Pertussis immunity and epidemiology: mode and duration of vaccine-induced immunity. Parasitology, online in advance of print. \url{http://dx.doi.org/10.1017/S0031182015000979}} published in \emph{Parasitology} in 2015 on pertussis immunity \footnote{whooping cough}. This study investigated the nature and duration of vaccine protection in hopes to explain the resurgence of pertussis in countries with high vaccination coverage. The data, provided by the Italian Ministry of Health, is the number of monthly pertussis notification reports from 1996 to 2009 in selected regions in Italy. Figure ~\ref{pertussisTS} and figure ~\ref{pertussisHist} both use data from the Italian region Lombardia. Can the sample distribution of the sample mean, 23.07, be approximated to a normal distribution?}
Conditions first need to be verified. 
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[(1)] These are referred to as \term{time series data}, because the data arrived in a particular sequence. Time series data generally deals with, you guessed it, time! If there were many Italians who contracted pertussis in one month, it may influence how many people would get pertussis the next month (and thus the number of monthly reports). Similarly if many Italians were vaccinated one month before, fewer people would be at risk of pertussis the following month. To make the assumption of independence, careful checks should be performed on such data. This condition can be weakly satisfied since the population of Lombardia is large enough for human interactions and non-independence to be influential\footnote{The population of Lombardia is around 9,000,000.}. 
\item[(2)] The sample size is 169, satisfying the sample size condition.
\item[(3)] Figure ~\ref{pertussisHist} suggests that the data are very strongly skewed or very distant outliers may be common for this type of data. Outliers can play an important role and affect the distribution of the sample mean and the estimate of the standard error.
\end{itemize}
A normal distribution should not model the sample mean of these 169 observations because of the independence assumption. The very extreme skewness also poses a challenge. \end{example}

\begin{figure}[ht]
   \centering
   \includegraphics[width = \textwidth]{ch_inference_foundations_oi_biostat/figures/pertussisHist/pertussisHist}
   \caption{Histogram of the number of monthly reports every month from 1996 to 2009. These data include an obvious skewness. These are problematic when considering the normality of the sample mean. \index{skew!example: very strong}.}
   \label{pertussisHist}
\end{figure}

\begin{figure}[ht]
   \centering
   \includegraphics[width = \textwidth]{ch_inference_foundations_oi_biostat/figures/pertussisTS/pertussisTS}
   \caption{This is a plot depicting the time series, the number of monthly reports over time. Time series data is generally plotted like this because it becomes easy to see how values change over time.}
   \label{pertussisTS}
\end{figure}

\begin{caution}
{Examine data structure when considering independence}
{Some data sets are collected in such a way that they have a natural underlying structure between observations, e.g. when observations occur consecutively. Be especially cautious about independence assumptions regarding such data sets.}
\end{caution}

\begin{caution}
{Watch out for strong skew and outliers}
{Strong skew is often identified by the presence of clear outliers. If a data set has prominent outliers, or such observations are somewhat common for the type of data under study, then it is useful to collect a sample with many more than 30 observations if the normal model will be used for $\bar{x}$. There are no simple guidelines for what sample size is big enough for all situations, so proceed with caution when working in the presence of strong skew or more extreme outliers.}
\index{skew!strongly skewed guideline}
\index{Central Limit Theorem|)}
\end{caution}


%__________________
\section{Inference for other estimators}
\label{aFrameworkForInference}

The sample mean is not the only point estimate for which the sampling distribution is nearly normal. The procedures introduced in Chapter ~\ref{foundationsForInference} are not limited to just estimating the population mean. For example, the sampling distribution of sample proportions closely resembles the normal distribution when the sample size is sufficiently large. In this section, a number of examples will be introduced where the normal approximation is reasonable for the point estimate. Chapters~\ref{inferenceForNumericalData} and~\ref{inferenceForCategoricalData} will revisit each of the point estimates in this section along with some other new statistics.

These tools can be used for other point estimates. However these point estimates need to obey some characteristics and assumptions. Another important assumption needs to be made about each point estimate encountered in this section: the estimate is unbiased. A point estimate is \term{unbiased} if the sampling distribution of the estimate is centered at the parameter it estimates. A biased point estimate on the other hand consistently is too high or estimates are always always too low. That is, an unbiased estimate does not naturally over or underestimate the parameter. Rather, it tends to provide a ``good'' estimate. The sample mean is an example of an unbiased point estimate, as are each of the examples introduced in this section.

Finally, the general case where a point estimate may follow some distribution other than the normal distribution is discussed. Guidance will be provided about how to handle scenarios where the statistical techniques introduced thus far are insufficient for the problem at hand.

\subsection{Confidence intervals for nearly normal point estimates}

\index{confidence interval!using normal model|(}

In Section~\ref{confidenceIntervals}, the point estimate $\bar{x}$ with a standard error $SE_{\bar{x}}$ was used to create a 95\% confidence interval for the population mean:
\begin{align}
\bar{x}\ \pm\ 1.96 \times SE_{\bar{x}}
\label{95PercCIForMeanInGeneralizingSection}
\end{align}
This interval was constructed by noting that the sample mean is within 1.96 standard errors of the actual mean about 95\% of the time. This same logic generalizes to any unbiased point estimate that is nearly normal. This can be generalized for any confidence level by using a place-holder $z^{\star}$.

\begin{termBox}{\tBoxTitle{General confidence interval for the normal sampling distribution case}\label{generalConfidenceIntervalTermBox}%
For any unbiased point estimate, the confidence interval for a nearly normal point estimate is
\begin{eqnarray}
\text{point estimate}\ \pm\ z^{\star}SE
\label{95PercGeneralCIInGeneralizingSection}
\end{eqnarray}
This is of the same form as the generalized confidence interval for the sample mean where $z^{\star}$ is selected to correspond to the confidence level, and $SE$ represents the standard error. Remember from previously that the value $z^{\star}SE$ is called the \emph{margin of error}\index{margin of error}.}
\end{termBox}

Generally the standard error for a point estimate is estimated from the data and computed using a formula. For example, the standard error for the sample mean is
\begin{eqnarray*}
SE_{\bar{x}} = \frac{s}{\sqrt{n}}
\end{eqnarray*}
In this section, the computed standard error for each example and exercise is provided without detailing where the values came from. In future chapters, the formulae and other details for each scenario will elaborated upon.

\begin{example}{Using the \data{BRFSS BMI} data, the point estimate for the average difference in weights between men and women is $\bar{x}_\mathrm{{men}}-\bar{x}_\mathrm{{women}}= 36.61$ pounds. This point estimate is associated with a nearly normal distribution with SE = 0.35 pounds. What is a reasonable 95\% confidence interval for the difference in gender weights?}
\label{confIntervalForDifferenceOfRunTimeBetweenGenders}
The normal approximation is said to be valid, so apply Equation~\eqref{95PercGeneralCIInGeneralizingSection}:
\begin{eqnarray*}
\text{point estimate}\ \pm\ z^{\star} SE
	\quad\rightarrow\quad 36.61\ \pm\ 1.96\times 0.35
	\quad\rightarrow\quad (35.91, 37.31)
\end{eqnarray*}
Thus, scientists are 95\% confident that the men were, on average, between 35.91 to 37.31 pounds heavier than women. That is, the actual average difference is plausibly between 35.91 and 37.31 pounds with 95\% confidence.
\end{example}

\begin{example}{Does Example~\ref{confIntervalForDifferenceOfRunTimeBetweenGenders} guarantee that if a husband and wife both weighed themselves, the husband would weigh between 35.91 and 37.31 pounds more than the wife?}
The confidence interval above says absolutely nothing about individual observations. It \emph{only} makes a statement about a plausible range of values for the \emph{average} difference between all men and all women in the US. 
\end{example}

\begin{exercise}
The proportion of men in the \data{BRFSS BMI} sample is $\hat{p}=0.42$. This sample meets certain conditions that ensure $\hat{p}$ will be nearly normal, and the standard error of the estimate is $SE_{\hat{p}}=0.05$. Create a 90\% confidence interval for the proportion of participants in the \data{BRFSS} study that will resemble the proportion of men in the US. \footnote{Use $z^{\star}=1.65$, and apply the general confidence interval formula:
\begin{eqnarray*}
\hat{p}\ \pm\ z^{\star}SE_{\hat{p}}
	\quad\to\quad 0.42\ \pm\ 1.65\times 0.05
	\quad\to\quad (0.3375, 0.5025)
\end{eqnarray*}
Thus, the CDC is 90\% confident that between 34\% and 50\% are men.}
\index{confidence interval!using normal model|)}
\end{exercise}

\subsection{Hypothesis testing for nearly normal point estimates}
\index{hypothesis testing!using normal model|(}

Just as the confidence interval method works with many other point estimates, the obvious connection between confidence intervals and hypothesis testing should apply to these new point estimates that are unbiased. Remember the Hypothesis testing framework from ~\ref{hypothesisFramework}. The following examples will only use the p-value approach introduced in Section ~\ref{pValue} and forgo the critical value shortcut.  

\begin{termBox}{\tBoxTitle[]{Hypothesis testing framework using the normal model}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item First write the hypotheses in plain language, then set them up in mathematical notation using the appropriate point estimate and parameter of interest. 
\item State a significance level $\alpha$. Generally use $\alpha=0.05$. 
\item Compute the test-statistic using the point estimate and standard error estimate. 
\item Calculate the p-value by drawing a picture of the sampling distribution under $H_0$. Know which area is being shaded to represent the correct p-value. 
\item Use the p-value to evaluate the hypotheses. Write a conclusion within the context of the problem. 
\end{enumerate}
} For point estimates other than the sampling mean, conditions must be verified to ensure that the point estimate is nearly normal and unbiased so that the standard error estimate is also reasonable. This step can be done before computing the test-statistic.  
\end{termBox}

\begin{exercise} \label{fdaHypSetupForSulph}
A drug called sulphinpyrazone was under consideration for use in reducing the death rate in heart attack patients. To determine whether the drug was effective, a set of 1,475 patients were recruited into an experiment and randomly split into two groups: a control group that received a placebo and a treatment group that received the new drug. What would be an appropriate null hypothesis? And the alternative?\footnote{The skeptic's perspective is that the drug does not work at reducing deaths in heart attack patients ($H_0$), while the alternative is that the drug does work ($H_A$).}
\end{exercise}

Formalize the hypotheses from Exercise~\ref{fdaHypSetupForSulph} by letting $p_{control}$ and $p_{treatment}$ represent the proportion of patients who died in the control and treatment groups, respectively. Then the hypotheses can be written as
\begin{eqnarray*}
&&H_0: p_{control} = p_{treatment} \quad\text{(the drug doesn't work)} \quad \\
&&H_A: p_{control} > p_{treatment} \quad\text{(the drug works)}
\end{eqnarray*}
or equivalently,
\begin{eqnarray*}
&&H_0: p_{control} - p_{treatment} = 0 \quad\text{(the drug doesn't work)} \quad \\
&&H_A: p_{control} - p_{treatment} > 0 \quad\text{(the drug works)}
\end{eqnarray*}
Strong evidence against the null hypothesis and in favor of the alternative would correspond to an observed difference in death rates,
\begin{eqnarray*}
\text{point estimate} = \hat{p}_{control} - \hat{p}_{treatment}
\end{eqnarray*}
being larger than would expected from chance alone. This difference in sample proportions represents a point estimate that is useful in evaluating the hypotheses. 

\begin{example}{Evaluate the hypothesis setup from Exericse~\ref{fdaHypSetupForSulph} using data from the actual study.\footnote{Anturane Reinfarction Trial Research Group. 1980. Sulfinpyrazone in the prevention of sudden death after myocardial infarction. New England Journal of Medicine 302(5):250-256.} In the control group, 60 of 742 patients died. In the treatment group, 41 of 733 patients died. The sample difference in death rates can be summarized as
\begin{eqnarray*}
\text{point estimate} = \hat{p}_{control} - \hat{p}_{treatment} = \frac{60}{742} - \frac{41}{733} = 0.025
\end{eqnarray*}
This point estimate is nearly normal and is an unbiased estimate of the actual difference in death rates. The standard error of this sample difference is $SE = 0.013$. Evaluate the hypothesis test at a 5\% significance level: $\alpha=0.05$.}
Identify the p-value to evaluate the hypotheses. If the null hypothesis is true, then the point estimate would have come from a nearly normal distribution, like the one shown in Figure~\ref{sulphStudyFindPValueUsingNormalApprox}. The distribution is centered at zero since $p_{control}-p_{treatment}=0$ under the null hypothesis. Because a large positive difference provides evidence against the null hypothesis and in favor of the alternative, the upper tail has been shaded to represent the p-value. The lower tail does not need to be shaded since this is a one-sided test: an observation in the lower tail does not support the alternative hypothesis.

\begin{figure}[bt]
   \centering
   \includegraphics[height=37mm]{ch_inference_foundations_oi_biostat/figures/sulphStudyFindPValueUsingNormalApprox/sulphStudyFindPValueUsingNormalApprox}
   \caption{The distribution of the sample difference if the null hypothesis is true.}
   \label{sulphStudyFindPValueUsingNormalApprox}
\end{figure}

The p-value can be computed by using the Z score of the point estimate and the normal probability table.
\begin{eqnarray}
Z = \frac{\text{point estimate} - \text{null value}}{SE_{\text{point estimate}}}
	= \frac{0.025 - 0}{0.013} = 1.92
\label{zScoreOfPointEstimateForSulphinpyrazoneThisIsFirstTestStatReference}
\end{eqnarray}
Examining $Z$ in the normal probability table, the lower unshaded tail is about 0.973. Thus, the upper shaded tail representing the p-value is
\begin{eqnarray*}
\text{p-value} = 1-0.973 = 0.027
\end{eqnarray*}
Because the p-value is less than the significance level ($\alpha=0.05$), the null hypothesis is implausible. That is, the null hypothesis is rejected in favor of the alternative and conclude that the drug is effective at reducing deaths in heart attack patients.
\end{example}

\subsection{Non-normal point estimates}

The ideas of confidence intervals and hypothesis testing may be applied to cases where the point estimate or test statistic is not necessarily normal. There are many reasons why such a situation may arise:
\begin{itemize}
\setlength{\itemsep}{0mm}
\item the sample size is too small for the normal approximation to be valid;
\item the standard error estimate may be poor; or
\item the point estimate tends towards some distribution that is not the normal distribution.
\end{itemize}
For each case where the normal approximation is not valid, the first task for any scientist is always to understand and characterize the sampling distribution of the point estimate or test statistic. Next, apply the general frameworks for confidence intervals and hypothesis testing to these alternative distributions.


\subsection{When to retreat}
\label{whenToRetreat}

Statistical tools rely on conditions. When the conditions are not met, these tools are unreliable and drawing conclusions from them is treacherous. The conditions for these tools typically come in two forms.
\begin{itemize}
\setlength{\itemsep}{0mm}
\item \textbf{The individual observations must be independent.} A random sample from less than 10\% of the population ensures the observations are independent. In experiments, subjects are randomized into groups. If independence fails, then advanced techniques must be used, and in some such cases, inference may not be possible.
\item \textbf{Other conditions focus on sample size and skew.} For example, if the sample size is too small, the skew too strong, or extreme outliers are present, then the normal model for the sample mean will fail.
\end{itemize}
Verification of conditions for statistical tools is always necessary. Whenever conditions are not satisfied for a statistical technique, there are three options. The first is to learn new methods that are appropriate for the data. The second route is to consult a statistician.The third route is to ignore the failure of conditions. This last option effectively invalidates any analysis and may discredit novel and interesting findings.

Finally with caution, there may not exist any inference tools helpful when considering data that include unknown biases, such as convenience samples. For this reason, there are books, courses, and researchers devoted to the techniques of sampling and experimental design. See Sections~\ref{overviewOfDataCollectionPrinciples}-\ref{experimentsSection} for basic principles of data collection.


%__________________
\section{Sample size and power (special topic)}
\label{sampleSizeAndPower}

Before it infers the average population BMI, the CDC needs to establish its methodology on gathering data, how many resources it wants to spend collecting the data and how many people, $n$, to gather data from. Sampling and post-processing the data can be extremely costly. Sample size and a test's power\footnote{$P(\text{rejecting the null hypothesis when it is false})$} go hand in hand.  

The Type 2 Error rate and the magnitude of the error for a point estimate are controlled by the sample size \footnote{Remember the margin of error comes from the confidence interval (point estimate $\pm$ margin of error where the margin of error = $q^{\star} \cdot SE$ for a certain confidence level)}. Real differences from the null value, even large ones, may be difficult to detect with small samples. If a very large sample is taken, there might exist a statistically significant difference but the magnitude of the difference might be so small that it is of no practical value. This section will describe techniques for selecting an appropriate sample size based on these considerations.

\subsection{Finding a sample size for a certain margin of error}
\label{findingASampleSizeForACertainME}

\index{margin of error|(}

Many companies are concerned about rising healthcare costs. A company may estimate certain health characteristics of its employees, such as blood pressure, to project its future cost obligations. However, it might be too expensive to measure the blood pressure of every employee at a large company. The company may choose to take a sample instead.

\begin{example}{Blood pressure oscillates with the beating of the heart, and the systolic pressure is defined as the peak pressure when a person is at rest. The average systolic blood pressure for people in the U.S. is about 130 mmHg with a standard deviation of about 25 mmHg. How large of a sample is necessary to estimate the average systolic blood pressure with a margin of error of 4 mmHg using a 95\% confidence level?}
\label{sampleSizeComputationForSystolicBloodPressure}
First, recall that the margin of error is the part that is added and subtracted from the point estimate when computing a confidence interval. Here since the company is large, it is assumed that the company has more than 30 employees. Because $n>30$,  1.96 can be used as the critical value for this nearly normal point estimate \footnote{Other assumptions should be verified as well: independence etc.} The margin of error for a 95\% confidence interval estimating a mean can be written as
\begin{align*}
ME_{95\%} = 1.96\times SE = 1.96\times\frac{\sigma_{employee}}{\sqrt{n}}
\end{align*}
The challenge in this case is to find the sample size $n$ so that this margin of error is less than or equal to 4. This problem is written as an inequality:
\begin{align*}
1.96\times \frac{\sigma_{employee}}{\sqrt{n}} \leq 4
\end{align*}
The company needs to solve for the appropriate value of $n$, but they do not know the value of $\sigma_{employee}$ since they haven't yet collected any data. There is no direct estimate! The company, instead, can use the best estimate available. It considers using the approximate standard deviation for the U.S. population, 25 and proceeds to solve for $n$:
\begin{align*}
1.96\times \frac{\sigma_{employee}}{\sqrt{n}} \approx 1.96\times\frac{25}{\sqrt{n}}
	&\leq 4 \\
1.96\times\frac{25}{4} &\leq \sqrt{n} \\
\left(1.96\times\frac{25}{4}\right)^2 &\leq n \\
150.06 &\leq n
\end{align*}
This suggests that the sample size should be of at least 151 employees. The company rounds up because the sample size must be \emph{greater than or equal to 150.06} to ensure a margin of error of 4.
\end{example}

Potentially controversial in Example~\ref{sampleSizeComputationForSystolicBloodPressure} is the use of the U.S. standard deviation for the employee standard deviation. Usually the standard deviation for the sample is not known since the sample hasn't been taken just yet! In such cases, many practicing statisticians review scientific literature or market research to make an educated guess and a reasonable substitution for the standard deviation to calculate the standard error. \footnote{Substitutions and assumptions like these also induce random chance that the hypotheses will be rejected incorrectly. More on this follows Section ~\ref{powerType2}.}

\begin{termBox}{\tBoxTitle{Identify a sample size for a particular margin of error}
To estimate the necessary sample size for a maximum margin of error $m$, set up an equation to represent this relationship:
\begin{align*}
m \geq ME = q^{\star}\frac{\sigma}{\sqrt{n}}
\end{align*}
where $z^{\star}$ is chosen to correspond to the desired confidence level for a nearly normal point estimate, and $\sigma$ is the standard deviation associated with the population. Solve for the sample size,~$n$.\\
If the point estimate is believed not to be nearly normal, use $q^{\star}$ from the $t$-distribution instead. However in practice, a nearly normal point estimate is used more often than not.}
\end{termBox}

Sample size computations are helpful in planning data collection, and they require careful forethought. Type 2 Error rate is considered next, an important topic in planning data collection and setting a sample size.

\index{margin of error|)}


\subsection{Power and the Type 2 Error rate}
\label{powerType2}

Consider the following two hypotheses:
\begin{itemize}
\setlength{\itemsep}{0.5mm}
\item[$H_0$:] The average blood pressure of employees is the same as the national average, $\mu = 130$.
\item[$H_A$:] The average blood pressure of employees is different than the national average, $\mu \neq 130$.
\end{itemize}
Suppose the alternative hypothesis is actually true. Then the large company might like to know, what is the chance that they make a Type 2 Error? That is, what is the chance that it fails to reject the null hypothesis even though it should be rejected? The answer is not obvious! If the average blood pressure of the employees is 132 (just 2 mmHg from the null value), it might be very difficult to detect the difference unless a large sample is used. On the other hand, it would be easier to detect a difference if the real average of employees was 140.

\begin{example}{Suppose the actual employee average is 132 and the company takes a sample of 100 individuals. Then the true sampling distribution of $\bar{x}$ is approximately $N(132, 2.5)$ \footnote{under the Central Limit Theorem} (since $SE = \frac{25}{\sqrt{100}} = 2.5$). What is the probability of successfully rejecting the null hypothesis?}
\label{computePowerIfMuIs132AndMu0Is130}
This problem can be divided into two normal probability questions. First, identify what values of $\bar{x}$  that would represent sufficiently strong evidence to reject $H_0$. Second, use the hypothetical sampling distribution with center $\mu=132$ to find the probability of observing sample means in the areas found in the first step.

\textbf{Step 1.} The null distribution could be represented by $N(130, 2.5)$, the same standard deviation as the true distribution but with the null value as its center. The two tail areas can be found by identifying the T-statistic corresponding to the 2.5\% tails ($\pm 1.96$), and solving for $x$ in the T-statistic equation:
\begin{align*}
-1.96 = T_1 &= \frac{x_1 - 130}{2.5}
	&+1.96 = T_2 &= \frac{x_2 - 130}{2.5} \\
x_1 &= 125.1
	&x_2 &= 134.9
\end{align*}
(An equally valid approach is to recognize that $x_1$ is $1.96\times SE$ below the mean and $x_2$ is $1.96\times SE$ above the mean to compute the values.) Figure~\ref{power132And141} shows the null distribution on the left with these two dotted cutoffs.

\textbf{Step 2.} Next,the probability of rejecting $H_0$ is computed if $\bar{x}$ actually came from $N(132, 2.5)$. This is the same as finding the two shaded tails for the second distribution in Figure~\ref{power132And141}. Use the T-statistic method again:
\begin{align*}
&T_{left} = \frac{125.1 - 132}{2.5} = -2.76
	&&T_{right} = \frac{134.9 - 132}{2.5} = 1.16 \\
&area_{left} =0.003
	&&area_{right} =0.123
\end{align*}
The probability of rejecting the null mean, if the true mean is 132, is the sum of these areas: $0.003 + 0.123 = 0.126$.
\end{example}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{ch_inference_foundations_oi_biostat/figures/power132And141/power132And141}
\caption{The sampling distribution of $\bar{x}$ under two scenarios. Left: $N(130, 2.5)$. Right: $N(132, 2.5)$, and the shaded areas in this distribution represent the power of the test.}
\label{power132And141}
\end{figure}

The probability of rejecting the null hypothesis is called the \term{power}. The power varies depending on what the supposed the truth might be. In Example~\ref{computePowerIfMuIs132AndMu0Is130}, the difference between the null value and the supposed true mean was relatively small, so the power was also small: only 0.126. However, when the truth is far from the null value, where the standard error is used as a measure of what is far, the power tends to increase.

\begin{exercise}
Suppose the true sampling distribution of $\bar{x}$ is centered at 140. That is, $\bar{x}$ comes from $N(140, 2.5)$. What would the power be under this scenario? It may be helpful to draw $N(140, 2.5)$ and shade the area representing power on Figure~\ref{power132And141}; use the same cutoff values identified in Example~\ref{computePowerIfMuIs132AndMu0Is130}.\footnote{Draw the distribution $N(140, 2.5)$, then find the area below 125.1 (about zero area) and above 134.9 (about 0.979). If the true mean is 140, the power is about 0.979.}
\end{exercise}

\begin{exercise}
If the power of a test is 0.979 for a particular mean, what is the Type 2 Error rate for this mean?\footnote{The Type 2 Error rate represents the probability of failing to reject the null hypothesis. Since the power is the probability that the null hypothesis is rejected, the Type 2 Error rate will be $1-0.979 = 0.021$.}
\end{exercise}

\begin{exercise}
Provide an intuitive explanation for why it is more likely to reject $H_0$ when the true mean is further from the null value.\footnote{Answers may vary a little. When the truth is far from the null value, the point estimate also tends to be far from the null value, making it easier to detect the difference and reject $H_0$.}
\end{exercise}

\subsection{Statistical significance versus practical significance}

When the sample size becomes larger, point estimates become more precise and any real differences in the mean and null value become easier to detect and recognize. Even a very small difference would likely be detected with a large enough sample. Sometimes researchers will decide to take such large samples that even the slightest difference is detected. While we still say that difference is \term{statistically significant}, it might not be \term{practically significant}.

Statistically significant differences are sometimes so minor that they are not practically relevant. This is especially important to research: if a study is conducted, the goal of the study is to find a meaningful result. Researchers don't want to spend lots of money finding results that hold no practical and applicable value.

The role of a statistician in conducting a study often includes planning the size of the study and determining the value of $\alpha$. Statisticians might first consult experts or scientific literature to learn what would be the smallest meaningful difference from the null value. They also would obtain some reasonable estimate for the standard deviation. With these important pieces of information, a sufficiently large sample size would be chosen so that the power for the meaningful difference is perhaps 80\% or 90\%. While larger sample sizes may still be used, statisticians in practice might advise against using them in some cases, especially in sensitive areas of research. While statistical rigor in hypothesis testing is absolutely important, many of these tests must also stand up to practical significance in the real world within applicable and relevance. 









%%%%%%%%%%%%%%HERE IS THE COMMENT%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%HERE IS THE COMMENT%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%HERE IS THE COMMENT%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%HERE IS THE COMMENT%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-----------------------------------------------------------------------------------------------------------------------------------------------------

%%% Use for an example/exercise
\begin{comment}

\subsection{One sample $t$-confidence intervals}
\label{oneSampleTConfidenceIntervals}

\index{data!dolphins and mercury|(}

Dolphins are at the top of the oceanic food chain, which causes dangerous substances such as mercury to concentrate in their organs and muscles. This is an important problem for both dolphins and other animals, like humans, who occasionally eat them. For instance, this is particularly relevant in Japan where school meals have included dolphin at times.
\setlength{\captionwidth}{86mm}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{ch_inference_foundations_oi_biostat/figures/rissosDolphin/rissosDolphin.jpg}  \\
\addvspace{2mm}
\begin{minipage}{\textwidth}
   \caption[rissosDolphinPic]{A Risso's dolphin.\vspace{-1mm} \\
   -----------------------------\vspace{-2mm}\\
   {\footnotesize Photo by Mike Baird (\oiRedirect{textbook-bairdphotos_com}{www.bairdphotos.com}). \oiRedirect{textbook-CC_BY_2}{CC~BY~2.0~license}.}\vspace{-8mm}}
   \label{rissosDolphin}
\end{minipage}
\vspace{3mm}
\end{figure}
\setlength{\captionwidth}{\mycaptionwidth}

Here we identify a confidence interval for the average mercury content in dolphin muscle using a sample of 19 Risso's dolphins from the Taiji area in Japan.\footnote{Taiji was featured in the movie \emph{The Cove}, and it is a significant source of dolphin and whale meat in Japan. Thousands of dolphins pass through the Taiji area annually, and we will assume these 19 dolphins represent a simple random sample from those dolphins. Data reference: Endo T and Haraguchi K. 2009. High mercury levels in hair samples from residents of Taiji, a Japanese whaling town. Marine Pollution Bulletin 60(5):743-747.} The data are summarized in Table~\ref{summaryStatsOfHgInMuscleOfRissosDolphins}. The minimum and maximum observed values can be used to evaluate whether or not there are obvious outliers or skew.

\begin{table}[h]
\centering
\begin{tabular}{ccc cc}
\hline
$n$ & $\bar{x}$ & $s$ & minimum & maximum \\
19   & 4.4	  & 2.3  & 1.7	       & 9.2 \\
\hline
\end{tabular}
\caption{Summary of mercury content in the muscle of 19 Risso's dolphins from the Taiji area. Measurements are in $\mu$g/wet g (micrograms of mercury per wet gram of muscle).}
\label{summaryStatsOfHgInMuscleOfRissosDolphins}
\end{table}

\begin{example}{Are the independence and normality conditions satisfied for this data~set?}
The observations are a simple random sample and consist of less than 10\% of the population, therefore independence is reasonable. The summary statistics in Table~\ref{summaryStatsOfHgInMuscleOfRissosDolphins} do not suggest any skew or outliers; all observations are within 2.5 standard deviations of the mean. Based on this evidence, the normality assumption seems reasonable.
\end{example}

In the normal model, we used $z^{\star}$ and the standard error to determine the width of a confidence interval. We revise the confidence interval formula slightly when using the $t$-distribution:
\begin{eqnarray*}
\bar{x} \ \pm\  t^{\star}_{df}SE
\end{eqnarray*}
\marginpar[\raggedright\vspace{-9mm}

$t^{\star}_{df}$\vspace{1mm}\\\footnotesize Multiplication\\factor for\\$t$ conf. interval]{\raggedright\vspace{-9mm}

$t^{\star}_{df}$\vspace{1mm}\\\footnotesize Multiplication\\factor for\\$t$ conf. interval}The sample mean and estimated standard error are computed just as before ($\bar{x} = 4.4$ and $SE = s/\sqrt{n} = 0.528$). The value $t^{\star}_{df}$ is a cutoff we obtain based on the confidence level and the $t$-distribution with $df$ degrees of freedom. Before determining this cutoff, we will first need the degrees of freedom.

\begin{termBox}{\tBoxTitle{Degrees of freedom for a single sample}
If the sample has $n$ observations and we are examining a single mean, then we use the $t$-distribution with $df=n-1$ degrees of freedom.}
\end{termBox}

In our current example, we should use the $t$-distribution with $df=19-1=18$ degrees of freedom. Then identifying $t_{18}^{\star}$ is similar to how we found $z^{\star}$. 
\begin{itemize}
\setlength{\itemsep}{0mm}
\item For a 95\% confidence interval, we want to find the cutoff $t^{\star}_{18}$ such that 95\% of the $t$-distribution is between -$t^{\star}_{18}$ and $t^{\star}_{18}$.
\item We look in the $t$-table on page~\pageref{tTableSample}, find the column with area totaling 0.05 in the two tails (third column), and then the row with 18 degrees of freedom: $t^{\star}_{18} = 2.10$.
\end{itemize}
Generally the value of $t^{\star}_{df}$ is slightly larger than what we would get under the normal model with~$z^{\star}$.

Finally, we can substitute all our values into the confidence interval equation to create the 95\% confidence interval for the average mercury content in muscles from Risso's dolphins that pass through the Taiji area:
\begin{eqnarray*}
\bar{x} \ \pm\  t^{\star}_{18}SE
	\quad \to \quad
4.4 \ \pm\  2.10 \times 0.528
	\quad \to \quad
(3.29, 5.51)
\end{eqnarray*}
We are 95\% confident the average mercury content of muscles in Risso's dolphins is between 3.29 and 5.51 $\mu$g/wet gram, which is considered extremely high.

\index{data!dolphins and mercury|)}

\begin{termBox}{\tBoxTitle{Finding a $t$-confidence interval for the mean}
Based on a sample of $n$ independent and nearly normal observations, a confidence interval for the population mean is
\begin{eqnarray*}
\bar{x} \ \pm\  t^{\star}_{df}SE
\end{eqnarray*}
where $\bar{x}$ is the sample mean, $t^{\star}_{df}$ corresponds to the confidence level and degrees of freedom, and $SE$ is the standard error as estimated by the sample.}
\end{termBox}

\textC{\pagebreak}

\begin{exercise} \label{croakerWhiteFishPacificExerConditions}
\index{data!white fish and mercury|(}
The FDA's webpage provides some data on mercury content of fish.\footnote{\oiRedirect{textbook-fda_mercury_in_fish_2010}{www.fda.gov/food/foodborneillnesscontaminants/metals/ucm115644.htm}} Based on a sample of 15 croaker white fish (Pacific), a sample mean and standard deviation were computed as 0.287 and 0.069 ppm (parts per million), respectively. The 15 observations ranged from 0.18 to 0.41 ppm. We will assume these observations are independent. Based on the summary statistics of the data, do you have any objections to the normality condition of the individual observations?\footnote{There are no obvious outliers; all observations are within 2 standard deviations of the mean. If there is skew, it is not evident. There are no red flags for the normal model based on this (limited) information, and we do not have reason to believe the mercury content is not nearly normal in this type of fish.}
\end{exercise}

\begin{example}{Estimate the standard error of $\bar{x}=0.287$ ppm using the data summaries in Guided Practice~\ref{croakerWhiteFishPacificExerConditions}. If we are to use the $t$-distribution to create a 90\% confidence interval for the actual mean of the mercury content, identify the degrees of freedom we should use and also find $t^{\star}_{df}$.}
\label{croakerWhiteFishPacificExerSEDFTStar}
The standard error: $SE = \frac{0.069}{\sqrt{15}} = 0.0178$. Degrees of freedom: $df = n - 1 = 14$.

Looking in the column where two tails is 0.100 (for a 90\% confidence interval) and row $df=14$, we identify $t^{\star}_{14} = 1.76$.
\end{example}

\begin{exercise}
Using the results of Guided Practice~\ref{croakerWhiteFishPacificExerConditions} and Example~\ref{croakerWhiteFishPacificExerSEDFTStar}, compute a 90\% confidence interval for the average mercury content of croaker white fish (Pacific).\footnote{$\bar{x} \ \pm\ t^{\star}_{14} SE \ \to\  0.287 \ \pm\  1.76\times 0.0178\ \to\ (0.256, 0.318)$. We are 90\% confident that the average mercury content of croaker white fish (Pacific) is between 0.256 and 0.318 ppm.}

\index{data!white fish and mercury|)}

\end{exercise}

\end{comment} 
%%%